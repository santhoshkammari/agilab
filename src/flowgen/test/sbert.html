

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Losses &mdash; Sentence Transformers  documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../../../_static/tabs.css?v=4c969af8" />
      <link rel="stylesheet" type="text/css" href="../../../_static/css/custom.css?v=36be663e" />


    <link rel="shortcut icon" href="../../../_static/favicon.ico"/>
    <link rel="canonical" href="https://www.sbert.netdocs/package_reference/sentence_transformer/losses.html"/>
      <script src="../../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../../_static/documentation_options.js?v=5929fcd5"></script>
      <script src="../../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../../../_static/copybutton.js?v=f281be69"></script>
      <script src="../../../_static/tabs.js?v=3ee01567"></script>
      <script src="../../../_static/js/custom.js?v=681dfd9f"></script>
    <script src="../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Samplers" href="sampler.html" />
    <link rel="prev" title="Training Arguments" href="training_args.html" />
</head>

<body class="wy-body-for-nav">
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >



          <a href="../../../index.html">

              <img src="../../../_static/logo.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../installation.html">Installation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../installation.html#install-with-pip">Install with pip</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../installation.html#install-with-conda">Install with Conda</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../installation.html#install-from-source">Install from Source</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../installation.html#editable-install">Editable Install</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../installation.html#install-pytorch-with-cuda-support">Install PyTorch with CUDA support</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../quickstart.html">Quickstart</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../quickstart.html#sentence-transformer">Sentence Transformer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../quickstart.html#cross-encoder">Cross Encoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../quickstart.html#sparse-encoder">Sparse Encoder</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../quickstart.html#next-steps">Next Steps</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../migration_guide.html">Migration Guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../migration_guide.html#migrating-from-v4-x-to-v5-x">Migrating from v4.x to v5.x</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../migration_guide.html#migration-for-model-encode">Migration for model.encode</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../migration_guide.html#migration-for-asym-to-router">Migration for Asym to Router</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../migration_guide.html#migration-of-advanced-usage">Migration of advanced usage</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../migration_guide.html#migrating-from-v3-x-to-v4-x">Migrating from v3.x to v4.x</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../migration_guide.html#migration-for-parameters-on-crossencoder-initialization-and-methods">Migration for parameters on <code class="docutils literal notranslate"><span class="pre">CrossEncoder</span></code> initialization and methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../migration_guide.html#migration-for-specific-parameters-from-crossencoder-fit">Migration for specific parameters from <code class="docutils literal notranslate"><span class="pre">CrossEncoder.fit</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../migration_guide.html#migration-for-crossencoder-evaluators">Migration for CrossEncoder evaluators</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../migration_guide.html#migrating-from-v2-x-to-v3-x">Migrating from v2.x to v3.x</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../migration_guide.html#migration-for-specific-parameters-from-sentencetransformer-fit">Migration for specific parameters from <code class="docutils literal notranslate"><span class="pre">SentenceTransformer.fit</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../migration_guide.html#migration-for-custom-datasets-and-dataloaders-used-in-sentencetransformer-fit">Migration for custom Datasets and DataLoaders used in <code class="docutils literal notranslate"><span class="pre">SentenceTransformer.fit</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Sentence Transformer</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../sentence_transformer/usage/usage.html">Usage</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../examples/sentence_transformer/applications/computing-embeddings/README.html">Computing Embeddings</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/applications/computing-embeddings/README.html#initializing-a-sentence-transformer-model">Initializing a Sentence Transformer Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/applications/computing-embeddings/README.html#calculating-embeddings">Calculating Embeddings</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/applications/computing-embeddings/README.html#prompt-templates">Prompt Templates</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/applications/computing-embeddings/README.html#input-sequence-length">Input Sequence Length</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/applications/computing-embeddings/README.html#multi-process-multi-gpu-encoding">Multi-Process / Multi-GPU Encoding</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../sentence_transformer/usage/semantic_textual_similarity.html">Semantic Textual Similarity</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../sentence_transformer/usage/semantic_textual_similarity.html#similarity-calculation">Similarity Calculation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../examples/sentence_transformer/applications/semantic-search/README.html">Semantic Search</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/applications/semantic-search/README.html#background">Background</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/applications/semantic-search/README.html#symmetric-vs-asymmetric-semantic-search">Symmetric vs. Asymmetric Semantic Search</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/applications/semantic-search/README.html#manual-implementation">Manual Implementation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/applications/semantic-search/README.html#optimized-implementation">Optimized Implementation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/applications/semantic-search/README.html#speed-optimization">Speed Optimization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/applications/semantic-search/README.html#elasticsearch">Elasticsearch</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/applications/semantic-search/README.html#opensearch">OpenSearch</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/applications/semantic-search/README.html#approximate-nearest-neighbor">Approximate Nearest Neighbor</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/applications/semantic-search/README.html#retrieve-re-rank">Retrieve &amp; Re-Rank</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/applications/semantic-search/README.html#examples">Examples</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../examples/sentence_transformer/applications/retrieve_rerank/README.html">Retrieve &amp; Re-Rank</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/applications/retrieve_rerank/README.html#retrieve-re-rank-pipeline">Retrieve &amp; Re-Rank Pipeline</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/applications/retrieve_rerank/README.html#retrieval-bi-encoder">Retrieval: Bi-Encoder</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/applications/retrieve_rerank/README.html#re-ranker-cross-encoder">Re-Ranker: Cross-Encoder</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/applications/retrieve_rerank/README.html#example-scripts">Example Scripts</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/applications/retrieve_rerank/README.html#pre-trained-bi-encoders-retrieval">Pre-trained Bi-Encoders (Retrieval)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/applications/retrieve_rerank/README.html#pre-trained-cross-encoders-re-ranker">Pre-trained Cross-Encoders (Re-Ranker)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../examples/sentence_transformer/applications/clustering/README.html">Clustering</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/applications/clustering/README.html#k-means">k-Means</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/applications/clustering/README.html#agglomerative-clustering">Agglomerative Clustering</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/applications/clustering/README.html#fast-clustering">Fast Clustering</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/applications/clustering/README.html#topic-modeling">Topic Modeling</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../examples/sentence_transformer/applications/paraphrase-mining/README.html">Paraphrase Mining</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/applications/paraphrase-mining/README.html#sentence_transformers.util.paraphrase_mining"><code class="docutils literal notranslate"><span class="pre">paraphrase_mining()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../examples/sentence_transformer/applications/parallel-sentence-mining/README.html">Translated Sentence Mining</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/applications/parallel-sentence-mining/README.html#margin-based-mining">Margin Based Mining</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/applications/parallel-sentence-mining/README.html#examples">Examples</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../examples/sentence_transformer/applications/image-search/README.html">Image Search</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/applications/image-search/README.html#installation">Installation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/applications/image-search/README.html#usage">Usage</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/applications/image-search/README.html#examples">Examples</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../examples/sentence_transformer/applications/embedding-quantization/README.html">Embedding Quantization</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/applications/embedding-quantization/README.html#binary-quantization">Binary Quantization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/applications/embedding-quantization/README.html#scalar-int8-quantization">Scalar (int8) Quantization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/applications/embedding-quantization/README.html#additional-extensions">Additional extensions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/applications/embedding-quantization/README.html#demo">Demo</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/applications/embedding-quantization/README.html#try-it-yourself">Try it yourself</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../sentence_transformer/usage/custom_models.html">Creating Custom Models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../sentence_transformer/usage/custom_models.html#structure-of-sentence-transformer-models">Structure of Sentence Transformer Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../sentence_transformer/usage/custom_models.html#sentence-transformer-model-from-a-transformers-model">Sentence Transformer Model from a Transformers Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../sentence_transformer/usage/custom_models.html#advanced-custom-modules">Advanced: Custom Modules</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../sentence_transformer/usage/efficiency.html">Speeding up Inference</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../sentence_transformer/usage/efficiency.html#pytorch">PyTorch</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../sentence_transformer/usage/efficiency.html#onnx">ONNX</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../sentence_transformer/usage/efficiency.html#openvino">OpenVINO</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../sentence_transformer/usage/efficiency.html#benchmarks">Benchmarks</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../sentence_transformer/pretrained_models.html">Pretrained Models</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../sentence_transformer/pretrained_models.html#original-models">Original Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../sentence_transformer/pretrained_models.html#semantic-search-models">Semantic Search Models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../sentence_transformer/pretrained_models.html#multi-qa-models">Multi-QA Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../sentence_transformer/pretrained_models.html#msmarco-passage-models">MSMARCO Passage Models</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../sentence_transformer/pretrained_models.html#multilingual-models">Multilingual Models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../sentence_transformer/pretrained_models.html#semantic-similarity-models">Semantic Similarity Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../sentence_transformer/pretrained_models.html#bitext-mining">Bitext Mining</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../sentence_transformer/pretrained_models.html#image-text-models">Image &amp; Text-Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../sentence_transformer/pretrained_models.html#instructor-models">INSTRUCTOR models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../sentence_transformer/pretrained_models.html#scientific-similarity-models">Scientific Similarity Models</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../sentence_transformer/training_overview.html">Training Overview</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../sentence_transformer/training_overview.html#why-finetune">Why Finetune?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../sentence_transformer/training_overview.html#training-components">Training Components</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../sentence_transformer/training_overview.html#model">Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../sentence_transformer/training_overview.html#dataset">Dataset</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../sentence_transformer/training_overview.html#dataset-format">Dataset Format</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../sentence_transformer/training_overview.html#loss-function">Loss Function</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../sentence_transformer/training_overview.html#training-arguments">Training Arguments</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../sentence_transformer/training_overview.html#evaluator">Evaluator</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../sentence_transformer/training_overview.html#trainer">Trainer</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../sentence_transformer/training_overview.html#callbacks">Callbacks</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../sentence_transformer/training_overview.html#multi-dataset-training">Multi-Dataset Training</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../sentence_transformer/training_overview.html#deprecated-training">Deprecated Training</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../sentence_transformer/training_overview.html#best-base-embedding-models">Best Base Embedding Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../sentence_transformer/training_overview.html#comparisons-with-crossencoder-training">Comparisons with CrossEncoder Training</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../sentence_transformer/dataset_overview.html">Dataset Overview</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../sentence_transformer/dataset_overview.html#datasets-on-the-hugging-face-hub">Datasets on the Hugging Face Hub</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../sentence_transformer/dataset_overview.html#pre-existing-datasets">Pre-existing Datasets</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../sentence_transformer/loss_overview.html">Loss Overview</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../sentence_transformer/loss_overview.html#loss-table">Loss Table</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../sentence_transformer/loss_overview.html#loss-modifiers">Loss modifiers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../sentence_transformer/loss_overview.html#distillation">Distillation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../sentence_transformer/loss_overview.html#commonly-used-loss-functions">Commonly used Loss Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../sentence_transformer/loss_overview.html#custom-loss-functions">Custom Loss Functions</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../sentence_transformer/training/examples.html">Training Examples</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../examples/sentence_transformer/training/sts/README.html">Semantic Textual Similarity</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/training/sts/README.html#training-data">Training data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/training/sts/README.html#loss-function">Loss Function</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../examples/sentence_transformer/training/nli/README.html">Natural Language Inference</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/training/nli/README.html#data">Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/training/nli/README.html#softmaxloss">SoftmaxLoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/training/nli/README.html#multiplenegativesrankingloss">MultipleNegativesRankingLoss</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../examples/sentence_transformer/training/paraphrases/README.html">Paraphrase Data</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/training/paraphrases/README.html#pre-trained-models">Pre-Trained Models</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../examples/sentence_transformer/training/quora_duplicate_questions/README.html">Quora Duplicate Questions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/training/quora_duplicate_questions/README.html#training">Training</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/training/quora_duplicate_questions/README.html#multiplenegativesrankingloss">MultipleNegativesRankingLoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/training/quora_duplicate_questions/README.html#pretrained-models">Pretrained Models</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../examples/sentence_transformer/training/ms_marco/README.html">MS MARCO</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/training/ms_marco/README.html#bi-encoder">Bi-Encoder</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../examples/sentence_transformer/training/matryoshka/README.html">Matryoshka Embeddings</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/training/matryoshka/README.html#use-cases">Use Cases</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/training/matryoshka/README.html#results">Results</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/training/matryoshka/README.html#training">Training</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/training/matryoshka/README.html#inference">Inference</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/training/matryoshka/README.html#code-examples">Code Examples</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../examples/sentence_transformer/training/adaptive_layer/README.html">Adaptive Layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/training/adaptive_layer/README.html#use-cases">Use Cases</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/training/adaptive_layer/README.html#results">Results</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/training/adaptive_layer/README.html#training">Training</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/training/adaptive_layer/README.html#inference">Inference</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/training/adaptive_layer/README.html#code-examples">Code Examples</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../examples/sentence_transformer/training/multilingual/README.html">Multilingual Models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/training/multilingual/README.html#extend-your-own-models">Extend your own models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/training/multilingual/README.html#training">Training</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/training/multilingual/README.html#datasets">Datasets</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/training/multilingual/README.html#sources-for-training-data">Sources for Training Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/training/multilingual/README.html#evaluation">Evaluation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/training/multilingual/README.html#available-pre-trained-models">Available Pre-trained Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/training/multilingual/README.html#usage">Usage</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/training/multilingual/README.html#performance">Performance</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/training/multilingual/README.html#citation">Citation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../examples/sentence_transformer/training/distillation/README.html">Model Distillation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/training/distillation/README.html#knowledge-distillation">Knowledge Distillation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/training/distillation/README.html#speed-performance-trade-off">Speed - Performance Trade-Off</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/training/distillation/README.html#dimensionality-reduction">Dimensionality Reduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/training/distillation/README.html#quantization">Quantization</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../examples/sentence_transformer/training/data_augmentation/README.html">Augmented SBERT</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/training/data_augmentation/README.html#motivation">Motivation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/training/data_augmentation/README.html#extend-to-your-own-datasets">Extend to your own datasets</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/training/data_augmentation/README.html#methodology">Methodology</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/training/data_augmentation/README.html#scenario-1-limited-or-small-annotated-datasets-few-labeled-sentence-pairs">Scenario 1: Limited or small annotated datasets (few labeled sentence-pairs)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/training/data_augmentation/README.html#scenario-2-no-annotated-datasets-only-unlabeled-sentence-pairs">Scenario 2: No annotated datasets (Only unlabeled sentence-pairs)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/training/data_augmentation/README.html#training">Training</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/training/data_augmentation/README.html#citation">Citation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../examples/sentence_transformer/training/prompts/README.html">Training with Prompts</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/training/prompts/README.html#what-are-prompts">What are Prompts?</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/training/prompts/README.html#why-would-we-train-with-prompts">Why would we train with Prompts?</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/training/prompts/README.html#how-do-we-train-with-prompts">How do we train with Prompts?</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../examples/sentence_transformer/training/peft/README.html">Training with PEFT Adapters</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/training/peft/README.html#compatibility-methods">Compatibility Methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/training/peft/README.html#adding-a-new-adapter">Adding a New Adapter</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/training/peft/README.html#loading-a-pretrained-adapter">Loading a Pretrained Adapter</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/training/peft/README.html#training-script">Training Script</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../examples/sentence_transformer/unsupervised_learning/README.html">Unsupervised Learning</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/unsupervised_learning/README.html#tsdae">TSDAE</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/unsupervised_learning/README.html#simcse">SimCSE</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/unsupervised_learning/README.html#ct">CT</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/unsupervised_learning/README.html#ct-in-batch-negative-sampling">CT (In-Batch Negative Sampling)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/unsupervised_learning/README.html#masked-language-model-mlm">Masked Language Model (MLM)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/unsupervised_learning/README.html#genq">GenQ</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/unsupervised_learning/README.html#gpl">GPL</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/unsupervised_learning/README.html#performance-comparison">Performance Comparison</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../examples/sentence_transformer/domain_adaptation/README.html">Domain Adaptation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/domain_adaptation/README.html#domain-adaptation-vs-unsupervised-learning">Domain Adaptation vs. Unsupervised Learning</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/domain_adaptation/README.html#adaptive-pre-training">Adaptive Pre-Training</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/domain_adaptation/README.html#gpl-generative-pseudo-labeling">GPL: Generative Pseudo-Labeling</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../examples/sentence_transformer/training/hpo/README.html">Hyperparameter Optimization</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/training/hpo/README.html#hpo-components">HPO Components</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/training/hpo/README.html#putting-it-all-together">Putting It All Together</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/training/hpo/README.html#example-scripts">Example Scripts</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../sentence_transformer/training/distributed.html">Distributed Training</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../sentence_transformer/training/distributed.html#comparison">Comparison</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../sentence_transformer/training/distributed.html#fsdp">FSDP</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Cross Encoder</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../cross_encoder/usage/usage.html">Usage</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../examples/cross_encoder/applications/README.html">Cross-Encoder vs Bi-Encoder</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/cross_encoder/applications/README.html#cross-encoder-vs-bi-encoder">Cross-Encoder vs. Bi-Encoder</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/cross_encoder/applications/README.html#when-to-use-cross-bi-encoders">When to use Cross- / Bi-Encoders?</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/cross_encoder/applications/README.html#cross-encoders-usage">Cross-Encoders Usage</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/cross_encoder/applications/README.html#combining-bi-and-cross-encoders">Combining Bi- and Cross-Encoders</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/cross_encoder/applications/README.html#training-cross-encoders">Training Cross-Encoders</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../examples/sentence_transformer/applications/retrieve_rerank/README.html">Retrieve &amp; Re-Rank</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/applications/retrieve_rerank/README.html#retrieve-re-rank-pipeline">Retrieve &amp; Re-Rank Pipeline</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/applications/retrieve_rerank/README.html#retrieval-bi-encoder">Retrieval: Bi-Encoder</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/applications/retrieve_rerank/README.html#re-ranker-cross-encoder">Re-Ranker: Cross-Encoder</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/applications/retrieve_rerank/README.html#example-scripts">Example Scripts</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/applications/retrieve_rerank/README.html#pre-trained-bi-encoders-retrieval">Pre-trained Bi-Encoders (Retrieval)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sentence_transformer/applications/retrieve_rerank/README.html#pre-trained-cross-encoders-re-ranker">Pre-trained Cross-Encoders (Re-Ranker)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../cross_encoder/usage/efficiency.html">Speeding up Inference</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../cross_encoder/usage/efficiency.html#pytorch">PyTorch</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../cross_encoder/usage/efficiency.html#onnx">ONNX</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../cross_encoder/usage/efficiency.html#openvino">OpenVINO</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../cross_encoder/usage/efficiency.html#benchmarks">Benchmarks</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../cross_encoder/pretrained_models.html">Pretrained Models</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../cross_encoder/pretrained_models.html#ms-marco">MS MARCO</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../cross_encoder/pretrained_models.html#squad-qnli">SQuAD (QNLI)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../cross_encoder/pretrained_models.html#stsbenchmark">STSbenchmark</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../cross_encoder/pretrained_models.html#quora-duplicate-questions">Quora Duplicate Questions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../cross_encoder/pretrained_models.html#nli">NLI</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../cross_encoder/pretrained_models.html#community-models">Community Models</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../cross_encoder/training_overview.html">Training Overview</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../cross_encoder/training_overview.html#why-finetune">Why Finetune?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../cross_encoder/training_overview.html#training-components">Training Components</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../cross_encoder/training_overview.html#model">Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../cross_encoder/training_overview.html#dataset">Dataset</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../cross_encoder/training_overview.html#dataset-format">Dataset Format</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../cross_encoder/training_overview.html#hard-negatives-mining">Hard Negatives Mining</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../cross_encoder/training_overview.html#loss-function">Loss Function</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../cross_encoder/training_overview.html#training-arguments">Training Arguments</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../cross_encoder/training_overview.html#evaluator">Evaluator</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../cross_encoder/training_overview.html#trainer">Trainer</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../cross_encoder/training_overview.html#callbacks">Callbacks</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../cross_encoder/training_overview.html#multi-dataset-training">Multi-Dataset Training</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../cross_encoder/training_overview.html#training-tips">Training Tips</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../cross_encoder/training_overview.html#deprecated-training">Deprecated Training</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../cross_encoder/training_overview.html#comparisons-with-sentencetransformer-training">Comparisons with SentenceTransformer Training</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../cross_encoder/loss_overview.html">Loss Overview</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../cross_encoder/loss_overview.html#loss-table">Loss Table</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../cross_encoder/loss_overview.html#distillation">Distillation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../cross_encoder/loss_overview.html#commonly-used-loss-functions">Commonly used Loss Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../cross_encoder/loss_overview.html#custom-loss-functions">Custom Loss Functions</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../cross_encoder/training/examples.html">Training Examples</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../examples/cross_encoder/training/sts/README.html">Semantic Textual Similarity</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/cross_encoder/training/sts/README.html#training-data">Training data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/cross_encoder/training/sts/README.html#loss-function">Loss Function</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/cross_encoder/training/sts/README.html#inference">Inference</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../examples/cross_encoder/training/nli/README.html">Natural Language Inference</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/cross_encoder/training/nli/README.html#data">Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/cross_encoder/training/nli/README.html#crossentropyloss">CrossEntropyLoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/cross_encoder/training/nli/README.html#inference">Inference</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../examples/cross_encoder/training/quora_duplicate_questions/README.html">Quora Duplicate Questions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/cross_encoder/training/quora_duplicate_questions/README.html#training">Training</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/cross_encoder/training/quora_duplicate_questions/README.html#inference">Inference</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../examples/cross_encoder/training/ms_marco/README.html">MS MARCO</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/cross_encoder/training/ms_marco/README.html#cross-encoder">Cross Encoder</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/cross_encoder/training/ms_marco/README.html#training-scripts">Training Scripts</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/cross_encoder/training/ms_marco/README.html#inference">Inference</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../examples/cross_encoder/training/rerankers/README.html">Rerankers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/cross_encoder/training/rerankers/README.html#binarycrossentropyloss">BinaryCrossEntropyLoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/cross_encoder/training/rerankers/README.html#cachedmultiplenegativesrankingloss">CachedMultipleNegativesRankingLoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/cross_encoder/training/rerankers/README.html#inference">Inference</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../examples/cross_encoder/training/distillation/README.html">Model Distillation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/cross_encoder/training/distillation/README.html#cross-encoder-knowledge-distillation">Cross Encoder Knowledge Distillation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/cross_encoder/training/distillation/README.html#inference">Inference</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../sentence_transformer/training/distributed.html">Distributed Training</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../sentence_transformer/training/distributed.html#comparison">Comparison</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../sentence_transformer/training/distributed.html#fsdp">FSDP</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Sparse Encoder</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../sparse_encoder/usage/usage.html">Usage</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../examples/sparse_encoder/applications/computing_embeddings/README.html">Computing Sparse Embeddings</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sparse_encoder/applications/computing_embeddings/README.html#initializing-a-sparse-encoder-model">Initializing a Sparse Encoder Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sparse_encoder/applications/computing_embeddings/README.html#calculating-embeddings">Calculating Embeddings</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sparse_encoder/applications/computing_embeddings/README.html#input-sequence-length">Input Sequence Length</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sparse_encoder/applications/computing_embeddings/README.html#controlling-sparsity">Controlling Sparsity</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sparse_encoder/applications/computing_embeddings/README.html#interpretability-with-splade-models">Interpretability with SPLADE Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sparse_encoder/applications/computing_embeddings/README.html#multi-process-multi-gpu-encoding">Multi-Process / Multi-GPU Encoding</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../examples/sparse_encoder/applications/semantic_textual_similarity/README.html">Semantic Textual Similarity</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sparse_encoder/applications/semantic_textual_similarity/README.html#similarity-calculation">Similarity Calculation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../examples/sparse_encoder/applications/semantic_search/README.html">Semantic Search</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sparse_encoder/applications/semantic_search/README.html#manual-search">Manual Search</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sparse_encoder/applications/semantic_search/README.html#vector-database-search">Vector Database Search</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sparse_encoder/applications/semantic_search/README.html#qdrant-integration">Qdrant Integration</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sparse_encoder/applications/semantic_search/README.html#opensearch-integration">OpenSearch Integration</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sparse_encoder/applications/semantic_search/README.html#seismic-integration">Seismic Integration</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sparse_encoder/applications/semantic_search/README.html#elasticsearch-integration">Elasticsearch Integration</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../examples/sparse_encoder/applications/retrieve_rerank/README.html">Retrieve &amp; Re-Rank</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sparse_encoder/applications/retrieve_rerank/README.html#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sparse_encoder/applications/retrieve_rerank/README.html#interactive-demo-simple-wikipedia-search">Interactive Demo: Simple Wikipedia Search</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sparse_encoder/applications/retrieve_rerank/README.html#comprehensive-evaluation-hybrid-search-pipeline">Comprehensive Evaluation: Hybrid Search Pipeline</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sparse_encoder/applications/retrieve_rerank/README.html#pre-trained-models">Pre-trained Models</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../examples/sparse_encoder/evaluation/README.html">Sparse Encoder Evaluation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sparse_encoder/evaluation/README.html#example-with-retrieval-evaluation">Example with Retrieval Evaluation:</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../sparse_encoder/pretrained_models.html">Pretrained Models</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../sparse_encoder/pretrained_models.html#core-splade-models">Core SPLADE Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../sparse_encoder/pretrained_models.html#inference-free-splade-models">Inference-Free SPLADE Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../sparse_encoder/pretrained_models.html#model-collections">Model Collections</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../sparse_encoder/training_overview.html">Training Overview</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../sparse_encoder/training_overview.html#why-finetune">Why Finetune?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../sparse_encoder/training_overview.html#training-components">Training Components</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../sparse_encoder/training_overview.html#model">Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../sparse_encoder/training_overview.html#dataset">Dataset</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../sparse_encoder/training_overview.html#dataset-format">Dataset Format</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../sparse_encoder/training_overview.html#loss-function">Loss Function</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../sparse_encoder/training_overview.html#training-arguments">Training Arguments</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../sparse_encoder/training_overview.html#evaluator">Evaluator</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../sparse_encoder/training_overview.html#trainer">Trainer</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../sparse_encoder/training_overview.html#callbacks">Callbacks</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../sparse_encoder/training_overview.html#multi-dataset-training">Multi-Dataset Training</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../sparse_encoder/training_overview.html#training-tips">Training Tips</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../sentence_transformer/dataset_overview.html">Dataset Overview</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../sentence_transformer/dataset_overview.html#datasets-on-the-hugging-face-hub">Datasets on the Hugging Face Hub</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../sentence_transformer/dataset_overview.html#pre-existing-datasets">Pre-existing Datasets</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../sparse_encoder/loss_overview.html">Loss Overview</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../sparse_encoder/loss_overview.html#sparse-specific-loss-functions">Sparse specific Loss Functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../sparse_encoder/loss_overview.html#splade-loss">SPLADE Loss</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../sparse_encoder/loss_overview.html#csr-loss">CSR Loss</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../sparse_encoder/loss_overview.html#loss-table">Loss Table</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../sparse_encoder/loss_overview.html#distillation">Distillation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../sparse_encoder/loss_overview.html#commonly-used-loss-functions">Commonly used Loss Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../sparse_encoder/loss_overview.html#custom-loss-functions">Custom Loss Functions</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../sparse_encoder/training/examples.html">Training Examples</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../examples/sparse_encoder/training/distillation/README.html">Model Distillation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sparse_encoder/training/distillation/README.html#marginmse">MarginMSE</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../examples/sparse_encoder/training/ms_marco/README.html">MS MARCO</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sparse_encoder/training/ms_marco/README.html#sparsemultiplenegativesrankingloss">SparseMultipleNegativesRankingLoss</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../examples/sparse_encoder/training/sts/README.html">Semantic Textual Similarity</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sparse_encoder/training/sts/README.html#training-data">Training data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sparse_encoder/training/sts/README.html#loss-function">Loss Function</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../examples/sparse_encoder/training/nli/README.html">Natural Language Inference</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sparse_encoder/training/nli/README.html#data">Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sparse_encoder/training/nli/README.html#spladeloss">SpladeLoss</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../examples/sparse_encoder/training/quora_duplicate_questions/README.html">Quora Duplicate Questions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sparse_encoder/training/quora_duplicate_questions/README.html#training">Training</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../examples/sparse_encoder/training/retrievers/README.html">Information Retrieval</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sparse_encoder/training/retrievers/README.html#sparsemultiplenegativesrankingloss-mnrl">SparseMultipleNegativesRankingLoss (MNRL)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../examples/sparse_encoder/training/retrievers/README.html#inference-evaluation">Inference &amp; Evaluation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../sentence_transformer/training/distributed.html">Distributed Training</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../sentence_transformer/training/distributed.html#comparison">Comparison</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../sentence_transformer/training/distributed.html#fsdp">FSDP</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Package Reference</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Sentence Transformer</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="SentenceTransformer.html">SentenceTransformer</a><ul>
<li class="toctree-l3"><a class="reference internal" href="SentenceTransformer.html#id1">SentenceTransformer</a></li>
<li class="toctree-l3"><a class="reference internal" href="SentenceTransformer.html#sentencetransformermodelcarddata">SentenceTransformerModelCardData</a></li>
<li class="toctree-l3"><a class="reference internal" href="SentenceTransformer.html#similarityfunction">SimilarityFunction</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="trainer.html">Trainer</a><ul>
<li class="toctree-l3"><a class="reference internal" href="trainer.html#sentencetransformertrainer">SentenceTransformerTrainer</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="training_args.html">Training Arguments</a><ul>
<li class="toctree-l3"><a class="reference internal" href="training_args.html#sentencetransformertrainingarguments">SentenceTransformerTrainingArguments</a></li>
</ul>
</li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Losses</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#batchalltripletloss">BatchAllTripletLoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="#batchhardsoftmargintripletloss">BatchHardSoftMarginTripletLoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="#batchhardtripletloss">BatchHardTripletLoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="#batchsemihardtripletloss">BatchSemiHardTripletLoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="#contrastiveloss">ContrastiveLoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="#onlinecontrastiveloss">OnlineContrastiveLoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="#contrastivetensionloss">ContrastiveTensionLoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="#contrastivetensionlossinbatchnegatives">ContrastiveTensionLossInBatchNegatives</a></li>
<li class="toctree-l3"><a class="reference internal" href="#cosentloss">CoSENTLoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="#angleloss">AnglELoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="#cosinesimilarityloss">CosineSimilarityLoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="#denoisingautoencoderloss">DenoisingAutoEncoderLoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="#gistembedloss">GISTEmbedLoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="#cachedgistembedloss">CachedGISTEmbedLoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="#mseloss">MSELoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="#marginmseloss">MarginMSELoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="#matryoshkaloss">MatryoshkaLoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="#matryoshka2dloss">Matryoshka2dLoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="#adaptivelayerloss">AdaptiveLayerLoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="#megabatchmarginloss">MegaBatchMarginLoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="#multiplenegativesrankingloss">MultipleNegativesRankingLoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="#cachedmultiplenegativesrankingloss">CachedMultipleNegativesRankingLoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="#multiplenegativessymmetricrankingloss">MultipleNegativesSymmetricRankingLoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="#cachedmultiplenegativessymmetricrankingloss">CachedMultipleNegativesSymmetricRankingLoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="#softmaxloss">SoftmaxLoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="#tripletloss">TripletLoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="#distillkldivloss">DistillKLDivLoss</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="sampler.html">Samplers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="sampler.html#batchsamplers">BatchSamplers</a></li>
<li class="toctree-l3"><a class="reference internal" href="sampler.html#multidatasetbatchsamplers">MultiDatasetBatchSamplers</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="evaluation.html">Evaluation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="evaluation.html#binaryclassificationevaluator">BinaryClassificationEvaluator</a></li>
<li class="toctree-l3"><a class="reference internal" href="evaluation.html#embeddingsimilarityevaluator">EmbeddingSimilarityEvaluator</a></li>
<li class="toctree-l3"><a class="reference internal" href="evaluation.html#informationretrievalevaluator">InformationRetrievalEvaluator</a></li>
<li class="toctree-l3"><a class="reference internal" href="evaluation.html#nanobeirevaluator">NanoBEIREvaluator</a></li>
<li class="toctree-l3"><a class="reference internal" href="evaluation.html#mseevaluator">MSEEvaluator</a></li>
<li class="toctree-l3"><a class="reference internal" href="evaluation.html#paraphraseminingevaluator">ParaphraseMiningEvaluator</a></li>
<li class="toctree-l3"><a class="reference internal" href="evaluation.html#rerankingevaluator">RerankingEvaluator</a></li>
<li class="toctree-l3"><a class="reference internal" href="evaluation.html#sentenceevaluator">SentenceEvaluator</a></li>
<li class="toctree-l3"><a class="reference internal" href="evaluation.html#sequentialevaluator">SequentialEvaluator</a></li>
<li class="toctree-l3"><a class="reference internal" href="evaluation.html#translationevaluator">TranslationEvaluator</a></li>
<li class="toctree-l3"><a class="reference internal" href="evaluation.html#tripletevaluator">TripletEvaluator</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="datasets.html">Datasets</a><ul>
<li class="toctree-l3"><a class="reference internal" href="datasets.html#parallelsentencesdataset">ParallelSentencesDataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="datasets.html#sentencelabeldataset">SentenceLabelDataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="datasets.html#denoisingautoencoderdataset">DenoisingAutoEncoderDataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="datasets.html#noduplicatesdataloader">NoDuplicatesDataLoader</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="models.html">Modules</a><ul>
<li class="toctree-l3"><a class="reference internal" href="models.html#main-modules">Main Modules</a></li>
<li class="toctree-l3"><a class="reference internal" href="models.html#further-modules">Further Modules</a></li>
<li class="toctree-l3"><a class="reference internal" href="models.html#base-modules">Base Modules</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="quantization.html">quantization</a><ul>
<li class="toctree-l3"><a class="reference internal" href="quantization.html#sentence_transformers.quantization.quantize_embeddings"><code class="docutils literal notranslate"><span class="pre">quantize_embeddings()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="quantization.html#sentence_transformers.quantization.semantic_search_faiss"><code class="docutils literal notranslate"><span class="pre">semantic_search_faiss()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="quantization.html#sentence_transformers.quantization.semantic_search_usearch"><code class="docutils literal notranslate"><span class="pre">semantic_search_usearch()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../cross_encoder/index.html">Cross Encoder</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../cross_encoder/cross_encoder.html">CrossEncoder</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../cross_encoder/cross_encoder.html#id1">CrossEncoder</a></li>
<li class="toctree-l3"><a class="reference internal" href="../cross_encoder/cross_encoder.html#crossencodermodelcarddata">CrossEncoderModelCardData</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../cross_encoder/trainer.html">Trainer</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../cross_encoder/trainer.html#crossencodertrainer">CrossEncoderTrainer</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../cross_encoder/training_args.html">Training Arguments</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../cross_encoder/training_args.html#crossencodertrainingarguments">CrossEncoderTrainingArguments</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../cross_encoder/losses.html">Losses</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../cross_encoder/losses.html#binarycrossentropyloss">BinaryCrossEntropyLoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="../cross_encoder/losses.html#crossentropyloss">CrossEntropyLoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="../cross_encoder/losses.html#lambdaloss">LambdaLoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="../cross_encoder/losses.html#listmleloss">ListMLELoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="../cross_encoder/losses.html#plistmleloss">PListMLELoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="../cross_encoder/losses.html#listnetloss">ListNetLoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="../cross_encoder/losses.html#multiplenegativesrankingloss">MultipleNegativesRankingLoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="../cross_encoder/losses.html#cachedmultiplenegativesrankingloss">CachedMultipleNegativesRankingLoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="../cross_encoder/losses.html#mseloss">MSELoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="../cross_encoder/losses.html#marginmseloss">MarginMSELoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="../cross_encoder/losses.html#ranknetloss">RankNetLoss</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../cross_encoder/evaluation.html">Evaluation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../cross_encoder/evaluation.html#crossencoderrerankingevaluator">CrossEncoderRerankingEvaluator</a></li>
<li class="toctree-l3"><a class="reference internal" href="../cross_encoder/evaluation.html#crossencodernanobeirevaluator">CrossEncoderNanoBEIREvaluator</a></li>
<li class="toctree-l3"><a class="reference internal" href="../cross_encoder/evaluation.html#crossencoderclassificationevaluator">CrossEncoderClassificationEvaluator</a></li>
<li class="toctree-l3"><a class="reference internal" href="../cross_encoder/evaluation.html#crossencodercorrelationevaluator">CrossEncoderCorrelationEvaluator</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../sparse_encoder/index.html">Sparse Encoder</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../sparse_encoder/SparseEncoder.html">SparseEncoder</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../sparse_encoder/SparseEncoder.html#id1">SparseEncoder</a></li>
<li class="toctree-l3"><a class="reference internal" href="../sparse_encoder/SparseEncoder.html#sparseencodermodelcarddata">SparseEncoderModelCardData</a></li>
<li class="toctree-l3"><a class="reference internal" href="../sparse_encoder/SparseEncoder.html#similarityfunction">SimilarityFunction</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../sparse_encoder/trainer.html">Trainer</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../sparse_encoder/trainer.html#sparseencodertrainer">SparseEncoderTrainer</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../sparse_encoder/training_args.html">Training Arguments</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../sparse_encoder/training_args.html#sparseencodertrainingarguments">SparseEncoderTrainingArguments</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../sparse_encoder/losses.html">Losses</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../sparse_encoder/losses.html#spladeloss">SpladeLoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="../sparse_encoder/losses.html#flopsloss">FlopsLoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="../sparse_encoder/losses.html#csrloss">CSRLoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="../sparse_encoder/losses.html#csrreconstructionloss">CSRReconstructionLoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="../sparse_encoder/losses.html#sparsemultiplenegativesrankingloss">SparseMultipleNegativesRankingLoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="../sparse_encoder/losses.html#sparsemarginmseloss">SparseMarginMSELoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="../sparse_encoder/losses.html#sparsedistillkldivloss">SparseDistillKLDivLoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="../sparse_encoder/losses.html#sparsetripletloss">SparseTripletLoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="../sparse_encoder/losses.html#sparsecosinesimilarityloss">SparseCosineSimilarityLoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="../sparse_encoder/losses.html#sparsecosentloss">SparseCoSENTLoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="../sparse_encoder/losses.html#sparseangleloss">SparseAnglELoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="../sparse_encoder/losses.html#sparsemseloss">SparseMSELoss</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="sampler.html">Samplers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="sampler.html#batchsamplers">BatchSamplers</a></li>
<li class="toctree-l3"><a class="reference internal" href="sampler.html#multidatasetbatchsamplers">MultiDatasetBatchSamplers</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../sparse_encoder/evaluation.html">Evaluation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../sparse_encoder/evaluation.html#sparseinformationretrievalevaluator">SparseInformationRetrievalEvaluator</a></li>
<li class="toctree-l3"><a class="reference internal" href="../sparse_encoder/evaluation.html#sparsenanobeirevaluator">SparseNanoBEIREvaluator</a></li>
<li class="toctree-l3"><a class="reference internal" href="../sparse_encoder/evaluation.html#sparseembeddingsimilarityevaluator">SparseEmbeddingSimilarityEvaluator</a></li>
<li class="toctree-l3"><a class="reference internal" href="../sparse_encoder/evaluation.html#sparsebinaryclassificationevaluator">SparseBinaryClassificationEvaluator</a></li>
<li class="toctree-l3"><a class="reference internal" href="../sparse_encoder/evaluation.html#sparsetripletevaluator">SparseTripletEvaluator</a></li>
<li class="toctree-l3"><a class="reference internal" href="../sparse_encoder/evaluation.html#sparsererankingevaluator">SparseRerankingEvaluator</a></li>
<li class="toctree-l3"><a class="reference internal" href="../sparse_encoder/evaluation.html#sparsetranslationevaluator">SparseTranslationEvaluator</a></li>
<li class="toctree-l3"><a class="reference internal" href="../sparse_encoder/evaluation.html#sparsemseevaluator">SparseMSEEvaluator</a></li>
<li class="toctree-l3"><a class="reference internal" href="../sparse_encoder/evaluation.html#reciprocalrankfusionevaluator">ReciprocalRankFusionEvaluator</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../sparse_encoder/models.html">Modules</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../sparse_encoder/models.html#splade-pooling">SPLADE Pooling</a></li>
<li class="toctree-l3"><a class="reference internal" href="../sparse_encoder/models.html#mlm-transformer">MLM Transformer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../sparse_encoder/models.html#sparseautoencoder">SparseAutoEncoder</a></li>
<li class="toctree-l3"><a class="reference internal" href="../sparse_encoder/models.html#sparsestaticembedding">SparseStaticEmbedding</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../sparse_encoder/callbacks.html">Callbacks</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../sparse_encoder/callbacks.html#spladeregularizerweightschedulercallback">SpladeRegularizerWeightSchedulerCallback</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../sparse_encoder/search_engines.html">Search Engines</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../sparse_encoder/search_engines.html#sentence_transformers.sparse_encoder.search_engines.semantic_search_elasticsearch"><code class="docutils literal notranslate"><span class="pre">semantic_search_elasticsearch()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../sparse_encoder/search_engines.html#sentence_transformers.sparse_encoder.search_engines.semantic_search_opensearch"><code class="docutils literal notranslate"><span class="pre">semantic_search_opensearch()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../sparse_encoder/search_engines.html#sentence_transformers.sparse_encoder.search_engines.semantic_search_qdrant"><code class="docutils literal notranslate"><span class="pre">semantic_search_qdrant()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../sparse_encoder/search_engines.html#sentence_transformers.sparse_encoder.search_engines.semantic_search_seismic"><code class="docutils literal notranslate"><span class="pre">semantic_search_seismic()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../util.html">util</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../util.html#module-sentence_transformers.util">Helper Functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../util.html#sentence_transformers.util.community_detection"><code class="docutils literal notranslate"><span class="pre">community_detection()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../util.html#sentence_transformers.util.http_get"><code class="docutils literal notranslate"><span class="pre">http_get()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../util.html#sentence_transformers.util.is_training_available"><code class="docutils literal notranslate"><span class="pre">is_training_available()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../util.html#sentence_transformers.util.mine_hard_negatives"><code class="docutils literal notranslate"><span class="pre">mine_hard_negatives()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../util.html#sentence_transformers.util.normalize_embeddings"><code class="docutils literal notranslate"><span class="pre">normalize_embeddings()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../util.html#sentence_transformers.util.paraphrase_mining"><code class="docutils literal notranslate"><span class="pre">paraphrase_mining()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../util.html#sentence_transformers.util.semantic_search"><code class="docutils literal notranslate"><span class="pre">semantic_search()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../util.html#sentence_transformers.util.truncate_embeddings"><code class="docutils literal notranslate"><span class="pre">truncate_embeddings()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../util.html#module-sentence_transformers.backend">Model Optimization</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../util.html#sentence_transformers.backend.export_dynamic_quantized_onnx_model"><code class="docutils literal notranslate"><span class="pre">export_dynamic_quantized_onnx_model()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../util.html#sentence_transformers.backend.export_optimized_onnx_model"><code class="docutils literal notranslate"><span class="pre">export_optimized_onnx_model()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../util.html#sentence_transformers.backend.export_static_quantized_openvino_model"><code class="docutils literal notranslate"><span class="pre">export_static_quantized_openvino_model()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../util.html#module-sentence_transformers.util">Similarity Metrics</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../util.html#sentence_transformers.util.cos_sim"><code class="docutils literal notranslate"><span class="pre">cos_sim()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../util.html#sentence_transformers.util.dot_score"><code class="docutils literal notranslate"><span class="pre">dot_score()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../util.html#sentence_transformers.util.euclidean_sim"><code class="docutils literal notranslate"><span class="pre">euclidean_sim()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../util.html#sentence_transformers.util.manhattan_sim"><code class="docutils literal notranslate"><span class="pre">manhattan_sim()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../util.html#sentence_transformers.util.pairwise_cos_sim"><code class="docutils literal notranslate"><span class="pre">pairwise_cos_sim()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../util.html#sentence_transformers.util.pairwise_dot_score"><code class="docutils literal notranslate"><span class="pre">pairwise_dot_score()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../util.html#sentence_transformers.util.pairwise_euclidean_sim"><code class="docutils literal notranslate"><span class="pre">pairwise_euclidean_sim()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../util.html#sentence_transformers.util.pairwise_manhattan_sim"><code class="docutils literal notranslate"><span class="pre">pairwise_manhattan_sim()</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">Sentence Transformers</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="index.html">Sentence Transformer</a></li>
      <li class="breadcrumb-item active">Losses</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/UKPLab/sentence-transformers/blob/master/docs/package_reference/sentence_transformer/losses.md" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">

  <section id="losses">
<h1>Losses<a class="headerlink" href="#losses" title="Link to this heading"></a></h1>
<p><code class="docutils literal notranslate"><span class="pre">sentence_transformers.losses</span></code> defines different loss functions that can be used to fine-tune embedding models on training data. The choice of loss function plays a critical role when fine-tuning the model. It determines how well our embedding model will work for the specific downstream task.</p>
<p>Sadly, there is no one size fits all loss function. Which loss function is suitable depends on the available training data and on the target task. Consider checking out the <a class="reference internal" href="../../sentence_transformer/loss_overview.html"><span class="std std-doc">Loss Overview</span></a> to help narrow down your choice of loss function(s).</p>
<section id="batchalltripletloss">
<h2>BatchAllTripletLoss<a class="headerlink" href="#batchalltripletloss" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="sentence_transformers.losses.BatchAllTripletLoss">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sentence_transformers.losses.</span></span><span class="sig-name descname"><span class="pre">BatchAllTripletLoss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model:</span> <span class="pre">~sentence_transformers.SentenceTransformer.SentenceTransformer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">distance_metric=&lt;function</span> <span class="pre">BatchHardTripletLossDistanceFunction.eucledian_distance&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">margin:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">5</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers\losses\BatchAllTripletLoss.py#L12-L150"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sentence_transformers.losses.BatchAllTripletLoss" title="Link to this definition"></a></dt>
<dd><p>BatchAllTripletLoss takes a batch with (sentence, label) pairs and computes the loss for all possible, valid
triplets, i.e., anchor and positive must have the same label, anchor and negative a different label. The labels
must be integers, with same label indicating sentences from the same class. Your train dataset
must contain at least 2 examples per label class.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong>  SentenceTransformer model</p></li>
<li><p><strong>distance_metric</strong>  Function that returns a distance between
two embeddings. The class SiameseDistanceMetric contains
pre-defined metrics that can be used.</p></li>
<li><p><strong>margin</strong>  Negative samples should be at least margin further
apart from the anchor than the positive.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">References</p>
<ul class="simple">
<li><p>Source: <a class="reference external" href="https://github.com/NegatioN/OnlineMiningTripletLoss/blob/master/online_triplet_loss/losses.py">https://github.com/NegatioN/OnlineMiningTripletLoss/blob/master/online_triplet_loss/losses.py</a></p></li>
<li><p>Paper: In Defense of the Triplet Loss for Person Re-Identification, <a class="reference external" href="https://arxiv.org/abs/1703.07737">https://arxiv.org/abs/1703.07737</a></p></li>
<li><p>Blog post: <a class="reference external" href="https://omoindrot.github.io/triplet-loss">https://omoindrot.github.io/triplet-loss</a></p></li>
</ul>
<dl>
<dt>Requirements:</dt><dd><ol class="arabic simple">
<li><p>Each sentence must be labeled with a class.</p></li>
<li><p>Your dataset must contain at least 2 examples per labels class.</p></li>
</ol>
</dd>
<dt>Inputs:</dt><dd><table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Texts</p></th>
<th class="head"><p>Labels</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>single sentences</p></td>
<td><p>class</p></td>
</tr>
</tbody>
</table>
</dd>
<dt>Recommendations:</dt><dd><ul class="simple">
<li><p>Use <code class="docutils literal notranslate"><span class="pre">BatchSamplers.GROUP_BY_LABEL</span></code> (<a class="reference internal" href="sampler.html#sentence_transformers.training_args.BatchSamplers" title="sentence_transformers.training_args.BatchSamplers"><code class="xref py py-class docutils literal notranslate"><span class="pre">docs</span></code></a>) to
ensure that each batch contains 2+ examples per label class.</p></li>
</ul>
</dd>
<dt>Relations:</dt><dd><ul class="simple">
<li><p><a class="reference internal" href="#sentence_transformers.losses.BatchHardTripletLoss" title="sentence_transformers.losses.BatchHardTripletLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">BatchHardTripletLoss</span></code></a> uses only the hardest positive and negative samples, rather than all possible, valid triplets.</p></li>
<li><p><a class="reference internal" href="#sentence_transformers.losses.BatchHardSoftMarginTripletLoss" title="sentence_transformers.losses.BatchHardSoftMarginTripletLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">BatchHardSoftMarginTripletLoss</span></code></a> uses only the hardest positive and negative samples, rather than all possible, valid triplets.
Also, it does not require setting a margin.</p></li>
<li><p><a class="reference internal" href="#sentence_transformers.losses.BatchSemiHardTripletLoss" title="sentence_transformers.losses.BatchSemiHardTripletLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">BatchSemiHardTripletLoss</span></code></a> uses only semi-hard triplets, valid triplets, rather than all possible, valid triplets.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span><span class="p">,</span> <span class="n">SentenceTransformerTrainer</span><span class="p">,</span> <span class="n">losses</span>
<span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">Dataset</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="s2">&quot;microsoft/mpnet-base&quot;</span><span class="p">)</span>
<span class="c1"># E.g. 0: sports, 1: economy, 2: politics</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">Dataset</span><span class="o">.</span><span class="n">from_dict</span><span class="p">({</span>
    <span class="s2">&quot;sentence&quot;</span><span class="p">:</span> <span class="p">[</span>
        <span class="s2">&quot;He played a great game.&quot;</span><span class="p">,</span>
        <span class="s2">&quot;The stock is up 20%&quot;</span><span class="p">,</span>
        <span class="s2">&quot;They won 2-1.&quot;</span><span class="p">,</span>
        <span class="s2">&quot;The last goal was amazing.&quot;</span><span class="p">,</span>
        <span class="s2">&quot;They all voted against the bill.&quot;</span><span class="p">,</span>
    <span class="p">],</span>
    <span class="s2">&quot;label&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
<span class="p">})</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">BatchAllTripletLoss</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">SentenceTransformerTrainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="batchhardsoftmargintripletloss">
<h2>BatchHardSoftMarginTripletLoss<a class="headerlink" href="#batchhardsoftmargintripletloss" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="sentence_transformers.losses.BatchHardSoftMarginTripletLoss">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sentence_transformers.losses.</span></span><span class="sig-name descname"><span class="pre">BatchHardSoftMarginTripletLoss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model:</span> <span class="pre">~sentence_transformers.SentenceTransformer.SentenceTransformer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">distance_metric=&lt;function</span> <span class="pre">BatchHardTripletLossDistanceFunction.eucledian_distance&gt;</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers\losses\BatchHardSoftMarginTripletLoss.py#L13-L152"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sentence_transformers.losses.BatchHardSoftMarginTripletLoss" title="Link to this definition"></a></dt>
<dd><p>BatchHardSoftMarginTripletLoss takes a batch with (sentence, label) pairs and computes the loss for all possible, valid
triplets, i.e., anchor and positive must have the same label, anchor and negative a different label. The labels
must be integers, with same label indicating sentences from the same class. Your train dataset
must contain at least 2 examples per label class. This soft-margin variant does not require setting a margin.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong>  SentenceTransformer model</p></li>
<li><p><strong>distance_metric</strong>  Function that returns a distance between
two embeddings. The class SiameseDistanceMetric contains
pre-defined metrics that can be used.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Definitions:</dt><dd><dl class="field-list simple">
<dt class="field-odd">Easy triplets<span class="colon">:</span></dt>
<dd class="field-odd"><p>Triplets which have a loss of 0 because
<code class="docutils literal notranslate"><span class="pre">distance(anchor,</span> <span class="pre">positive)</span> <span class="pre">+</span> <span class="pre">margin</span> <span class="pre">&lt;</span> <span class="pre">distance(anchor,</span> <span class="pre">negative)</span></code>.</p>
</dd>
<dt class="field-even">Hard triplets<span class="colon">:</span></dt>
<dd class="field-even"><p>Triplets where the negative is closer to the anchor than the positive, i.e.,
<code class="docutils literal notranslate"><span class="pre">distance(anchor,</span> <span class="pre">negative)</span> <span class="pre">&lt;</span> <span class="pre">distance(anchor,</span> <span class="pre">positive)</span></code>.</p>
</dd>
<dt class="field-odd">Semi-hard triplets<span class="colon">:</span></dt>
<dd class="field-odd"><p>Triplets where the negative is not closer to the anchor than the positive, but which
still have a positive loss, i.e., <code class="docutils literal notranslate"><span class="pre">distance(anchor,</span> <span class="pre">positive)</span> <span class="pre">&lt;</span> <span class="pre">distance(anchor,</span> <span class="pre">negative)</span> <span class="pre">+</span> <span class="pre">margin</span></code>.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">References</p>
<ul class="simple">
<li><p>Source: <a class="reference external" href="https://github.com/NegatioN/OnlineMiningTripletLoss/blob/master/online_triplet_loss/losses.py">https://github.com/NegatioN/OnlineMiningTripletLoss/blob/master/online_triplet_loss/losses.py</a></p></li>
<li><p>Paper: In Defense of the Triplet Loss for Person Re-Identification, <a class="reference external" href="https://arxiv.org/abs/1703.07737">https://arxiv.org/abs/1703.07737</a></p></li>
<li><p>Blog post: <a class="reference external" href="https://omoindrot.github.io/triplet-loss">https://omoindrot.github.io/triplet-loss</a></p></li>
</ul>
<dl>
<dt>Requirements:</dt><dd><ol class="arabic simple">
<li><p>Each sentence must be labeled with a class.</p></li>
<li><p>Your dataset must contain at least 2 examples per labels class.</p></li>
<li><p>Your dataset should contain hard positives and negatives.</p></li>
</ol>
</dd>
<dt>Inputs:</dt><dd><table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Texts</p></th>
<th class="head"><p>Labels</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>single sentences</p></td>
<td><p>class</p></td>
</tr>
</tbody>
</table>
</dd>
<dt>Recommendations:</dt><dd><ul class="simple">
<li><p>Use <code class="docutils literal notranslate"><span class="pre">BatchSamplers.GROUP_BY_LABEL</span></code> (<a class="reference internal" href="sampler.html#sentence_transformers.training_args.BatchSamplers" title="sentence_transformers.training_args.BatchSamplers"><code class="xref py py-class docutils literal notranslate"><span class="pre">docs</span></code></a>) to
ensure that each batch contains 2+ examples per label class.</p></li>
</ul>
</dd>
<dt>Relations:</dt><dd><ul class="simple">
<li><p><a class="reference internal" href="#sentence_transformers.losses.BatchHardTripletLoss" title="sentence_transformers.losses.BatchHardTripletLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">BatchHardTripletLoss</span></code></a> uses a user-specified margin, while this loss does not require setting a margin.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span><span class="p">,</span> <span class="n">SentenceTransformerTrainer</span><span class="p">,</span> <span class="n">losses</span>
<span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">Dataset</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="s2">&quot;microsoft/mpnet-base&quot;</span><span class="p">)</span>
<span class="c1"># E.g. 0: sports, 1: economy, 2: politics</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">Dataset</span><span class="o">.</span><span class="n">from_dict</span><span class="p">({</span>
    <span class="s2">&quot;sentence&quot;</span><span class="p">:</span> <span class="p">[</span>
        <span class="s2">&quot;He played a great game.&quot;</span><span class="p">,</span>
        <span class="s2">&quot;The stock is up 20%&quot;</span><span class="p">,</span>
        <span class="s2">&quot;They won 2-1.&quot;</span><span class="p">,</span>
        <span class="s2">&quot;The last goal was amazing.&quot;</span><span class="p">,</span>
        <span class="s2">&quot;They all voted against the bill.&quot;</span><span class="p">,</span>
    <span class="p">],</span>
    <span class="s2">&quot;label&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
<span class="p">})</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">BatchHardSoftMarginTripletLoss</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">SentenceTransformerTrainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="batchhardtripletloss">
<h2>BatchHardTripletLoss<a class="headerlink" href="#batchhardtripletloss" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="sentence_transformers.losses.BatchHardTripletLoss">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sentence_transformers.losses.</span></span><span class="sig-name descname"><span class="pre">BatchHardTripletLoss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model:</span> <span class="pre">~sentence_transformers.SentenceTransformer.SentenceTransformer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">distance_metric=&lt;function</span> <span class="pre">BatchHardTripletLossDistanceFunction.eucledian_distance&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">margin:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">5</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers\losses\BatchHardTripletLoss.py#L58-L266"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sentence_transformers.losses.BatchHardTripletLoss" title="Link to this definition"></a></dt>
<dd><p>BatchHardTripletLoss takes a batch with (sentence, label) pairs and computes the loss for all possible, valid
triplets, i.e., anchor and positive must have the same label, anchor and negative a different label. It then looks
for the hardest positive and the hardest negatives.
The labels must be integers, with same label indicating sentences from the same class. Your train dataset
must contain at least 2 examples per label class.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong>  SentenceTransformer model</p></li>
<li><p><strong>distance_metric</strong>  Function that returns a distance between
two embeddings. The class SiameseDistanceMetric contains
pre-defined metrics that can be used</p></li>
<li><p><strong>margin</strong>  Negative samples should be at least margin further
apart from the anchor than the positive.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Definitions:</dt><dd><dl class="field-list simple">
<dt class="field-odd">Easy triplets<span class="colon">:</span></dt>
<dd class="field-odd"><p>Triplets which have a loss of 0 because
<code class="docutils literal notranslate"><span class="pre">distance(anchor,</span> <span class="pre">positive)</span> <span class="pre">+</span> <span class="pre">margin</span> <span class="pre">&lt;</span> <span class="pre">distance(anchor,</span> <span class="pre">negative)</span></code>.</p>
</dd>
<dt class="field-even">Hard triplets<span class="colon">:</span></dt>
<dd class="field-even"><p>Triplets where the negative is closer to the anchor than the positive, i.e.,
<code class="docutils literal notranslate"><span class="pre">distance(anchor,</span> <span class="pre">negative)</span> <span class="pre">&lt;</span> <span class="pre">distance(anchor,</span> <span class="pre">positive)</span></code>.</p>
</dd>
<dt class="field-odd">Semi-hard triplets<span class="colon">:</span></dt>
<dd class="field-odd"><p>Triplets where the negative is not closer to the anchor than the positive, but which
still have a positive loss, i.e., <code class="docutils literal notranslate"><span class="pre">distance(anchor,</span> <span class="pre">positive)</span> <span class="pre">&lt;</span> <span class="pre">distance(anchor,</span> <span class="pre">negative)</span> <span class="pre">+</span> <span class="pre">margin</span></code>.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">References</p>
<ul class="simple">
<li><p>Source: <a class="reference external" href="https://github.com/NegatioN/OnlineMiningTripletLoss/blob/master/online_triplet_loss/losses.py">https://github.com/NegatioN/OnlineMiningTripletLoss/blob/master/online_triplet_loss/losses.py</a></p></li>
<li><p>Paper: In Defense of the Triplet Loss for Person Re-Identification, <a class="reference external" href="https://arxiv.org/abs/1703.07737">https://arxiv.org/abs/1703.07737</a></p></li>
<li><p>Blog post: <a class="reference external" href="https://omoindrot.github.io/triplet-loss">https://omoindrot.github.io/triplet-loss</a></p></li>
</ul>
<dl>
<dt>Requirements:</dt><dd><ol class="arabic simple">
<li><p>Each sentence must be labeled with a class.</p></li>
<li><p>Your dataset must contain at least 2 examples per labels class.</p></li>
<li><p>Your dataset should contain hard positives and negatives.</p></li>
</ol>
</dd>
<dt>Inputs:</dt><dd><table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Texts</p></th>
<th class="head"><p>Labels</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>single sentences</p></td>
<td><p>class</p></td>
</tr>
</tbody>
</table>
</dd>
<dt>Recommendations:</dt><dd><ul class="simple">
<li><p>Use <code class="docutils literal notranslate"><span class="pre">BatchSamplers.GROUP_BY_LABEL</span></code> (<a class="reference internal" href="sampler.html#sentence_transformers.training_args.BatchSamplers" title="sentence_transformers.training_args.BatchSamplers"><code class="xref py py-class docutils literal notranslate"><span class="pre">docs</span></code></a>) to
ensure that each batch contains 2+ examples per label class.</p></li>
</ul>
</dd>
<dt>Relations:</dt><dd><ul class="simple">
<li><p><a class="reference internal" href="#sentence_transformers.losses.BatchAllTripletLoss" title="sentence_transformers.losses.BatchAllTripletLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">BatchAllTripletLoss</span></code></a> uses all possible, valid triplets, rather than only the hardest positive and negative samples.</p></li>
<li><p><a class="reference internal" href="#sentence_transformers.losses.BatchSemiHardTripletLoss" title="sentence_transformers.losses.BatchSemiHardTripletLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">BatchSemiHardTripletLoss</span></code></a> uses only semi-hard triplets, valid triplets, rather than only the hardest positive and negative samples.</p></li>
<li><p><a class="reference internal" href="#sentence_transformers.losses.BatchHardSoftMarginTripletLoss" title="sentence_transformers.losses.BatchHardSoftMarginTripletLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">BatchHardSoftMarginTripletLoss</span></code></a> does not require setting a margin, while this loss does.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span><span class="p">,</span> <span class="n">SentenceTransformerTrainer</span><span class="p">,</span> <span class="n">losses</span>
<span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">Dataset</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="s2">&quot;microsoft/mpnet-base&quot;</span><span class="p">)</span>
<span class="c1"># E.g. 0: sports, 1: economy, 2: politics</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">Dataset</span><span class="o">.</span><span class="n">from_dict</span><span class="p">({</span>
    <span class="s2">&quot;sentence&quot;</span><span class="p">:</span> <span class="p">[</span>
        <span class="s2">&quot;He played a great game.&quot;</span><span class="p">,</span>
        <span class="s2">&quot;The stock is up 20%&quot;</span><span class="p">,</span>
        <span class="s2">&quot;They won 2-1.&quot;</span><span class="p">,</span>
        <span class="s2">&quot;The last goal was amazing.&quot;</span><span class="p">,</span>
        <span class="s2">&quot;They all voted against the bill.&quot;</span><span class="p">,</span>
    <span class="p">],</span>
    <span class="s2">&quot;label&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
<span class="p">})</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">BatchHardTripletLoss</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">SentenceTransformerTrainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="batchsemihardtripletloss">
<h2>BatchSemiHardTripletLoss<a class="headerlink" href="#batchsemihardtripletloss" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="sentence_transformers.losses.BatchSemiHardTripletLoss">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sentence_transformers.losses.</span></span><span class="sig-name descname"><span class="pre">BatchSemiHardTripletLoss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model:</span> <span class="pre">~sentence_transformers.SentenceTransformer.SentenceTransformer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">distance_metric=&lt;function</span> <span class="pre">BatchHardTripletLossDistanceFunction.eucledian_distance&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">margin:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">5</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers\losses\BatchSemiHardTripletLoss.py#L13-L187"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sentence_transformers.losses.BatchSemiHardTripletLoss" title="Link to this definition"></a></dt>
<dd><p>BatchSemiHardTripletLoss takes a batch with (label, sentence) pairs and computes the loss for all possible, valid
triplets, i.e., anchor and positive must have the same label, anchor and negative a different label. It then looks
for the semi hard positives and negatives.
The labels must be integers, with same label indicating sentences from the same class. Your train dataset
must contain at least 2 examples per label class.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong>  SentenceTransformer model</p></li>
<li><p><strong>distance_metric</strong>  Function that returns a distance between
two embeddings. The class SiameseDistanceMetric contains
pre-defined metrics that can be used</p></li>
<li><p><strong>margin</strong>  Negative samples should be at least margin further
apart from the anchor than the positive.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Definitions:</dt><dd><dl class="field-list simple">
<dt class="field-odd">Easy triplets<span class="colon">:</span></dt>
<dd class="field-odd"><p>Triplets which have a loss of 0 because
<code class="docutils literal notranslate"><span class="pre">distance(anchor,</span> <span class="pre">positive)</span> <span class="pre">+</span> <span class="pre">margin</span> <span class="pre">&lt;</span> <span class="pre">distance(anchor,</span> <span class="pre">negative)</span></code>.</p>
</dd>
<dt class="field-even">Hard triplets<span class="colon">:</span></dt>
<dd class="field-even"><p>Triplets where the negative is closer to the anchor than the positive, i.e.,
<code class="docutils literal notranslate"><span class="pre">distance(anchor,</span> <span class="pre">negative)</span> <span class="pre">&lt;</span> <span class="pre">distance(anchor,</span> <span class="pre">positive)</span></code>.</p>
</dd>
<dt class="field-odd">Semi-hard triplets<span class="colon">:</span></dt>
<dd class="field-odd"><p>Triplets where the negative is not closer to the anchor than the positive, but which
still have a positive loss, i.e., <code class="docutils literal notranslate"><span class="pre">distance(anchor,</span> <span class="pre">positive)</span> <span class="pre">&lt;</span> <span class="pre">distance(anchor,</span> <span class="pre">negative)</span> <span class="pre">+</span> <span class="pre">margin</span></code>.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">References</p>
<ul class="simple">
<li><p>Source: <a class="reference external" href="https://github.com/NegatioN/OnlineMiningTripletLoss/blob/master/online_triplet_loss/losses.py">https://github.com/NegatioN/OnlineMiningTripletLoss/blob/master/online_triplet_loss/losses.py</a></p></li>
<li><p>Paper: In Defense of the Triplet Loss for Person Re-Identification, <a class="reference external" href="https://arxiv.org/abs/1703.07737">https://arxiv.org/abs/1703.07737</a></p></li>
<li><p>Blog post: <a class="reference external" href="https://omoindrot.github.io/triplet-loss">https://omoindrot.github.io/triplet-loss</a></p></li>
</ul>
<dl>
<dt>Requirements:</dt><dd><ol class="arabic simple">
<li><p>Each sentence must be labeled with a class.</p></li>
<li><p>Your dataset must contain at least 2 examples per labels class.</p></li>
<li><p>Your dataset should contain semi hard positives and negatives.</p></li>
</ol>
</dd>
<dt>Inputs:</dt><dd><table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Texts</p></th>
<th class="head"><p>Labels</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>single sentences</p></td>
<td><p>class</p></td>
</tr>
</tbody>
</table>
</dd>
<dt>Recommendations:</dt><dd><ul class="simple">
<li><p>Use <code class="docutils literal notranslate"><span class="pre">BatchSamplers.GROUP_BY_LABEL</span></code> (<a class="reference internal" href="sampler.html#sentence_transformers.training_args.BatchSamplers" title="sentence_transformers.training_args.BatchSamplers"><code class="xref py py-class docutils literal notranslate"><span class="pre">docs</span></code></a>) to
ensure that each batch contains 2+ examples per label class.</p></li>
</ul>
</dd>
<dt>Relations:</dt><dd><ul class="simple">
<li><p><a class="reference internal" href="#sentence_transformers.losses.BatchHardTripletLoss" title="sentence_transformers.losses.BatchHardTripletLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">BatchHardTripletLoss</span></code></a> uses only the hardest positive and negative samples, rather than only semi hard positive and negatives.</p></li>
<li><p><a class="reference internal" href="#sentence_transformers.losses.BatchAllTripletLoss" title="sentence_transformers.losses.BatchAllTripletLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">BatchAllTripletLoss</span></code></a> uses all possible, valid triplets, rather than only semi hard positive and negatives.</p></li>
<li><p><a class="reference internal" href="#sentence_transformers.losses.BatchHardSoftMarginTripletLoss" title="sentence_transformers.losses.BatchHardSoftMarginTripletLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">BatchHardSoftMarginTripletLoss</span></code></a> uses only the hardest positive and negative samples, rather than only semi hard positive and negatives.
Also, it does not require setting a margin.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span><span class="p">,</span> <span class="n">SentenceTransformerTrainer</span><span class="p">,</span> <span class="n">losses</span>
<span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">Dataset</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="s2">&quot;microsoft/mpnet-base&quot;</span><span class="p">)</span>
<span class="c1"># E.g. 0: sports, 1: economy, 2: politics</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">Dataset</span><span class="o">.</span><span class="n">from_dict</span><span class="p">({</span>
    <span class="s2">&quot;sentence&quot;</span><span class="p">:</span> <span class="p">[</span>
        <span class="s2">&quot;He played a great game.&quot;</span><span class="p">,</span>
        <span class="s2">&quot;The stock is up 20%&quot;</span><span class="p">,</span>
        <span class="s2">&quot;They won 2-1.&quot;</span><span class="p">,</span>
        <span class="s2">&quot;The last goal was amazing.&quot;</span><span class="p">,</span>
        <span class="s2">&quot;They all voted against the bill.&quot;</span><span class="p">,</span>
    <span class="p">],</span>
    <span class="s2">&quot;label&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
<span class="p">})</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">BatchSemiHardTripletLoss</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">SentenceTransformerTrainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="contrastiveloss">
<h2>ContrastiveLoss<a class="headerlink" href="#contrastiveloss" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="sentence_transformers.losses.ContrastiveLoss">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sentence_transformers.losses.</span></span><span class="sig-name descname"><span class="pre">ContrastiveLoss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model:</span> <span class="pre">~sentence_transformers.SentenceTransformer.SentenceTransformer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">distance_metric=&lt;function</span> <span class="pre">SiameseDistanceMetric.&lt;lambda&gt;&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">margin:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">size_average:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers\losses\ContrastiveLoss.py#L21-L119"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sentence_transformers.losses.ContrastiveLoss" title="Link to this definition"></a></dt>
<dd><p>Contrastive loss. Expects as input two texts and a label of either 0 or 1. If the label == 1, then the distance between the
two embeddings is reduced. If the label == 0, then the distance between the embeddings is increased.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong>  SentenceTransformer model</p></li>
<li><p><strong>distance_metric</strong>  Function that returns a distance between
two embeddings. The class SiameseDistanceMetric contains
pre-defined metrices that can be used</p></li>
<li><p><strong>margin</strong>  Negative samples (label == 0) should have a distance
of at least the margin value.</p></li>
<li><p><strong>size_average</strong>  Average by the size of the mini-batch.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">References</p>
<ul class="simple">
<li><p>Further information: <a class="reference external" href="http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf">http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf</a></p></li>
<li><p><a class="reference external" href="../../../examples/sentence_transformer/training/quora_duplicate_questions/README.html">Training Examples &gt; Quora Duplicate Questions</a></p></li>
</ul>
<dl>
<dt>Requirements:</dt><dd><ol class="arabic simple">
<li><p>(anchor, positive/negative) pairs</p></li>
</ol>
</dd>
<dt>Inputs:</dt><dd><table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Texts</p></th>
<th class="head"><p>Labels</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>(anchor, positive/negative) pairs</p></td>
<td><p>1 if positive, 0 if negative</p></td>
</tr>
</tbody>
</table>
</dd>
<dt>Relations:</dt><dd><ul class="simple">
<li><p><a class="reference internal" href="#sentence_transformers.losses.OnlineContrastiveLoss" title="sentence_transformers.losses.OnlineContrastiveLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">OnlineContrastiveLoss</span></code></a> is similar, but uses hard positive and hard negative pairs.
It often yields better results.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span><span class="p">,</span> <span class="n">SentenceTransformerTrainer</span><span class="p">,</span> <span class="n">losses</span>
<span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">Dataset</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="s2">&quot;microsoft/mpnet-base&quot;</span><span class="p">)</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">Dataset</span><span class="o">.</span><span class="n">from_dict</span><span class="p">({</span>
    <span class="s2">&quot;sentence1&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;It&#39;s nice weather outside today.&quot;</span><span class="p">,</span> <span class="s2">&quot;He drove to work.&quot;</span><span class="p">],</span>
    <span class="s2">&quot;sentence2&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;It&#39;s so sunny.&quot;</span><span class="p">,</span> <span class="s2">&quot;She walked to the store.&quot;</span><span class="p">],</span>
    <span class="s2">&quot;label&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
<span class="p">})</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">ContrastiveLoss</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">SentenceTransformerTrainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="onlinecontrastiveloss">
<h2>OnlineContrastiveLoss<a class="headerlink" href="#onlinecontrastiveloss" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="sentence_transformers.losses.OnlineContrastiveLoss">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sentence_transformers.losses.</span></span><span class="sig-name descname"><span class="pre">OnlineContrastiveLoss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model:</span> <span class="pre">~sentence_transformers.SentenceTransformer.SentenceTransformer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">distance_metric=&lt;function</span> <span class="pre">SiameseDistanceMetric.&lt;lambda&gt;&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">margin:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">0.5</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers\losses\OnlineContrastiveLoss.py#L13-L88"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sentence_transformers.losses.OnlineContrastiveLoss" title="Link to this definition"></a></dt>
<dd><p>This Online Contrastive loss is similar to <code class="xref py py-class docutils literal notranslate"><span class="pre">ConstrativeLoss</span></code>, but it selects hard positive (positives that
are far apart) and hard negative pairs (negatives that are close) and computes the loss only for these pairs.
This loss often yields better performances than ContrastiveLoss.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong>  SentenceTransformer model</p></li>
<li><p><strong>distance_metric</strong>  Function that returns a distance between
two embeddings. The class SiameseDistanceMetric contains
pre-defined metrics that can be used</p></li>
<li><p><strong>margin</strong>  Negative samples (label == 0) should have a distance
of at least the margin value.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">References</p>
<ul class="simple">
<li><p><a class="reference external" href="../../../examples/sentence_transformer/training/quora_duplicate_questions/README.html">Training Examples &gt; Quora Duplicate Questions</a></p></li>
</ul>
<dl>
<dt>Requirements:</dt><dd><ol class="arabic simple">
<li><p>(anchor, positive/negative) pairs</p></li>
<li><p>Data should include hard positives and hard negatives</p></li>
</ol>
</dd>
<dt>Inputs:</dt><dd><table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Texts</p></th>
<th class="head"><p>Labels</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>(anchor, positive/negative) pairs</p></td>
<td><p>1 if positive, 0 if negative</p></td>
</tr>
</tbody>
</table>
</dd>
<dt>Relations:</dt><dd><ul class="simple">
<li><p><a class="reference internal" href="#sentence_transformers.losses.ContrastiveLoss" title="sentence_transformers.losses.ContrastiveLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">ContrastiveLoss</span></code></a> is similar, but does not use hard positive and hard negative pairs.
<a class="reference internal" href="#sentence_transformers.losses.OnlineContrastiveLoss" title="sentence_transformers.losses.OnlineContrastiveLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">OnlineContrastiveLoss</span></code></a> often yields better results.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span><span class="p">,</span> <span class="n">SentenceTransformerTrainer</span><span class="p">,</span> <span class="n">losses</span>
<span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">Dataset</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="s2">&quot;microsoft/mpnet-base&quot;</span><span class="p">)</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">Dataset</span><span class="o">.</span><span class="n">from_dict</span><span class="p">({</span>
    <span class="s2">&quot;sentence1&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;It&#39;s nice weather outside today.&quot;</span><span class="p">,</span> <span class="s2">&quot;He drove to work.&quot;</span><span class="p">],</span>
    <span class="s2">&quot;sentence2&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;It&#39;s so sunny.&quot;</span><span class="p">,</span> <span class="s2">&quot;She walked to the store.&quot;</span><span class="p">],</span>
    <span class="s2">&quot;label&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
<span class="p">})</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">OnlineContrastiveLoss</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">SentenceTransformerTrainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="contrastivetensionloss">
<h2>ContrastiveTensionLoss<a class="headerlink" href="#contrastivetensionloss" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="sentence_transformers.losses.ContrastiveTensionLoss">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sentence_transformers.losses.</span></span><span class="sig-name descname"><span class="pre">ContrastiveTensionLoss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="SentenceTransformer.html#sentence_transformers.SentenceTransformer" title="sentence_transformers.SentenceTransformer.SentenceTransformer"><span class="pre">SentenceTransformer</span></a></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers\losses\ContrastiveTensionLoss.py#L17-L104"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sentence_transformers.losses.ContrastiveTensionLoss" title="Link to this definition"></a></dt>
<dd><p>This loss expects only single sentences, without any labels. Positive and negative pairs are automatically created via random sampling,
such that a positive pair consists of two identical sentences and a negative pair consists of two different sentences. An independent
copy of the encoder model is created, which is used for encoding the first sentence of each pair. The original encoder model encodes the
second sentence. The embeddings are compared and scored using the generated labels (1 if positive, 0 if negative) using the binary cross
entropy objective.</p>
<p>Note that you must use the <cite>ContrastiveTensionDataLoader</cite> for this loss. The <cite>pos_neg_ratio</cite> of the ContrastiveTensionDataLoader can be
used to determine the number of negative pairs per positive pair.</p>
<p>Generally, <a class="reference internal" href="#sentence_transformers.losses.ContrastiveTensionLossInBatchNegatives" title="sentence_transformers.losses.ContrastiveTensionLossInBatchNegatives"><code class="xref py py-class docutils literal notranslate"><span class="pre">ContrastiveTensionLossInBatchNegatives</span></code></a> is recommended over this loss, as it gives a stronger training signal.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>model</strong>  SentenceTransformer model</p>
</dd>
</dl>
<p class="rubric">References</p>
<ul class="simple">
<li><p>Semantic Re-Tuning with Contrastive Tension: <a class="reference external" href="https://openreview.net/pdf?id=Ov_sMNau-PF">https://openreview.net/pdf?id=Ov_sMNau-PF</a></p></li>
<li><p><a class="reference external" href="../../../examples/sentence_transformer/unsupervised_learning/CT/README.html">Unsupervised Learning &gt; CT</a></p></li>
</ul>
<dl>
<dt>Inputs:</dt><dd><table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Texts</p></th>
<th class="head"><p>Labels</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>single sentences</p></td>
<td><p>none</p></td>
</tr>
</tbody>
</table>
</dd>
<dt>Relations:</dt><dd><ul class="simple">
<li><p><a class="reference internal" href="#sentence_transformers.losses.ContrastiveTensionLossInBatchNegatives" title="sentence_transformers.losses.ContrastiveTensionLossInBatchNegatives"><code class="xref py py-class docutils literal notranslate"><span class="pre">ContrastiveTensionLossInBatchNegatives</span></code></a> uses in-batch negative sampling, which gives a stronger training signal than this loss.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span><span class="p">,</span> <span class="n">losses</span>
<span class="kn">from</span> <span class="nn">sentence_transformers.losses</span> <span class="kn">import</span> <span class="n">ContrastiveTensionDataLoader</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="s1">&#39;all-MiniLM-L6-v2&#39;</span><span class="p">)</span>
<span class="n">train_examples</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s1">&#39;This is the 1st sentence&#39;</span><span class="p">,</span>
    <span class="s1">&#39;This is the 2nd sentence&#39;</span><span class="p">,</span>
    <span class="s1">&#39;This is the 3rd sentence&#39;</span><span class="p">,</span>
    <span class="s1">&#39;This is the 4th sentence&#39;</span><span class="p">,</span>
    <span class="s1">&#39;This is the 5th sentence&#39;</span><span class="p">,</span>
    <span class="s1">&#39;This is the 6th sentence&#39;</span><span class="p">,</span>
    <span class="s1">&#39;This is the 7th sentence&#39;</span><span class="p">,</span>
    <span class="s1">&#39;This is the 8th sentence&#39;</span><span class="p">,</span>
    <span class="s1">&#39;This is the 9th sentence&#39;</span><span class="p">,</span>
    <span class="s1">&#39;This is the final sentence&#39;</span><span class="p">,</span>
<span class="p">]</span>

<span class="n">train_dataloader</span> <span class="o">=</span> <span class="n">ContrastiveTensionDataLoader</span><span class="p">(</span><span class="n">train_examples</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">pos_neg_ratio</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">train_loss</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">ContrastiveTensionLoss</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="p">[(</span><span class="n">train_dataloader</span><span class="p">,</span> <span class="n">train_loss</span><span class="p">)],</span>
    <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p>
</dd></dl>

</section>
<section id="contrastivetensionlossinbatchnegatives">
<h2>ContrastiveTensionLossInBatchNegatives<a class="headerlink" href="#contrastivetensionlossinbatchnegatives" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="sentence_transformers.losses.ContrastiveTensionLossInBatchNegatives">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sentence_transformers.losses.</span></span><span class="sig-name descname"><span class="pre">ContrastiveTensionLossInBatchNegatives</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model:</span> <span class="pre">~sentence_transformers.SentenceTransformer.SentenceTransformer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">20.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">similarity_fct=&lt;function</span> <span class="pre">cos_sim&gt;</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers\losses\ContrastiveTensionLoss.py#L107-L195"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sentence_transformers.losses.ContrastiveTensionLossInBatchNegatives" title="Link to this definition"></a></dt>
<dd><p>This loss expects only single sentences, without any labels. Positive and negative pairs are automatically created via random sampling,
such that a positive pair consists of two identical sentences and a negative pair consists of two different sentences. An independent
copy of the encoder model is created, which is used for encoding the first sentence of each pair. The original encoder model encodes the
second sentence. Unlike <a class="reference internal" href="#sentence_transformers.losses.ContrastiveTensionLoss" title="sentence_transformers.losses.ContrastiveTensionLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">ContrastiveTensionLoss</span></code></a>, this loss uses the batch negative sampling strategy, i.e. the negative pairs
are sampled from the batch. Using in-batch negative sampling gives a stronger training signal than the original <a class="reference internal" href="#sentence_transformers.losses.ContrastiveTensionLoss" title="sentence_transformers.losses.ContrastiveTensionLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">ContrastiveTensionLoss</span></code></a>.
The performance usually increases with increasing batch sizes.</p>
<p>Note that you should not use the <cite>ContrastiveTensionDataLoader</cite> for this loss, but just a normal DataLoader with <cite>InputExample</cite> instances.
The two texts of each <cite>InputExample</cite> instance should be identical.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong>  SentenceTransformer model</p></li>
<li><p><strong>scale</strong>  Output of similarity function is multiplied by scale
value</p></li>
<li><p><strong>similarity_fct</strong>  similarity function between sentence
embeddings. By default, cos_sim. Can also be set to dot
product (and then set scale to 1)</p></li>
</ul>
</dd>
</dl>
<p class="rubric">References</p>
<ul class="simple">
<li><p>Semantic Re-Tuning with Contrastive Tension: <a class="reference external" href="https://openreview.net/pdf?id=Ov_sMNau-PF">https://openreview.net/pdf?id=Ov_sMNau-PF</a></p></li>
<li><p><a class="reference external" href="../../../examples/sentence_transformer/unsupervised_learning/CT_In-Batch_Negatives/README.html">Unsupervised Learning &gt; CT (In-Batch Negatives)</a></p></li>
</ul>
<dl>
<dt>Relations:</dt><dd><ul class="simple">
<li><p><a class="reference internal" href="#sentence_transformers.losses.ContrastiveTensionLoss" title="sentence_transformers.losses.ContrastiveTensionLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">ContrastiveTensionLoss</span></code></a> does not select negative pairs in-batch, resulting in a weaker training signal than this loss.</p></li>
</ul>
</dd>
<dt>Inputs:</dt><dd><table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Texts</p></th>
<th class="head"><p>Labels</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>(anchor, anchor) pairs</p></td>
<td><p>none</p></td>
</tr>
</tbody>
</table>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span><span class="p">,</span> <span class="n">losses</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="s1">&#39;all-MiniLM-L6-v2&#39;</span><span class="p">)</span>
<span class="n">train_examples</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">InputExample</span><span class="p">(</span><span class="n">texts</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;This is a positive pair&#39;</span><span class="p">,</span> <span class="s1">&#39;Where the distance will be minimized&#39;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">InputExample</span><span class="p">(</span><span class="n">texts</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;This is a negative pair&#39;</span><span class="p">,</span> <span class="s1">&#39;Their distance will be increased&#39;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>
<span class="p">]</span>
<span class="n">train_examples</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">InputExample</span><span class="p">(</span><span class="n">texts</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;This is the 1st sentence&#39;</span><span class="p">,</span> <span class="s1">&#39;This is the 1st sentence&#39;</span><span class="p">]),</span>
    <span class="n">InputExample</span><span class="p">(</span><span class="n">texts</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;This is the 2nd sentence&#39;</span><span class="p">,</span> <span class="s1">&#39;This is the 2nd sentence&#39;</span><span class="p">]),</span>
    <span class="n">InputExample</span><span class="p">(</span><span class="n">texts</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;This is the 3rd sentence&#39;</span><span class="p">,</span> <span class="s1">&#39;This is the 3rd sentence&#39;</span><span class="p">]),</span>
    <span class="n">InputExample</span><span class="p">(</span><span class="n">texts</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;This is the 4th sentence&#39;</span><span class="p">,</span> <span class="s1">&#39;This is the 4th sentence&#39;</span><span class="p">]),</span>
    <span class="n">InputExample</span><span class="p">(</span><span class="n">texts</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;This is the 5th sentence&#39;</span><span class="p">,</span> <span class="s1">&#39;This is the 5th sentence&#39;</span><span class="p">]),</span>
<span class="p">]</span>

<span class="n">train_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_examples</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>
<span class="n">train_loss</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">ContrastiveTensionLossInBatchNegatives</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="p">[(</span><span class="n">train_dataloader</span><span class="p">,</span> <span class="n">train_loss</span><span class="p">)],</span>
    <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="cosentloss">
<h2>CoSENTLoss<a class="headerlink" href="#cosentloss" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="sentence_transformers.losses.CoSENTLoss">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sentence_transformers.losses.</span></span><span class="sig-name descname"><span class="pre">CoSENTLoss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model:</span> <span class="pre">~sentence_transformers.SentenceTransformer.SentenceTransformer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">20.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">similarity_fct=&lt;function</span> <span class="pre">pairwise_cos_sim&gt;</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers\losses\CoSENTLoss.py#L13-L128"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sentence_transformers.losses.CoSENTLoss" title="Link to this definition"></a></dt>
<dd><p>This class implements CoSENT (Cosine Sentence) loss.
It expects that each of the InputExamples consists of a pair of texts and a float valued label, representing
the expected similarity score between the pair.</p>
<p>It computes the following loss function:</p>
<p><code class="docutils literal notranslate"><span class="pre">loss</span> <span class="pre">=</span> <span class="pre">logsum(1+exp(s(i,j)-s(k,l))+exp...)</span></code>, where <code class="docutils literal notranslate"><span class="pre">(i,j)</span></code> and <code class="docutils literal notranslate"><span class="pre">(k,l)</span></code> are any of the input pairs in the
batch such that the expected similarity of <code class="docutils literal notranslate"><span class="pre">(i,j)</span></code> is greater than <code class="docutils literal notranslate"><span class="pre">(k,l)</span></code>. The summation is over all possible
pairs of input pairs in the batch that match this condition.</p>
<p>Anecdotal experiments show that this loss function produces a more powerful training signal than <a class="reference internal" href="#sentence_transformers.losses.CosineSimilarityLoss" title="sentence_transformers.losses.CosineSimilarityLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">CosineSimilarityLoss</span></code></a>,
resulting in faster convergence and a final model with superior performance. Consequently, CoSENTLoss may be used
as a drop-in replacement for <a class="reference internal" href="#sentence_transformers.losses.CosineSimilarityLoss" title="sentence_transformers.losses.CosineSimilarityLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">CosineSimilarityLoss</span></code></a> in any training script.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong>  SentenceTransformerModel</p></li>
<li><p><strong>similarity_fct</strong>  Function to compute the PAIRWISE similarity
between embeddings. Default is
<code class="docutils literal notranslate"><span class="pre">util.pairwise_cos_sim</span></code>.</p></li>
<li><p><strong>scale</strong>  Output of similarity function is multiplied by scale
value. Represents the inverse temperature.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">References</p>
<ul class="simple">
<li><p>For further details, see: <a class="reference external" href="https://kexue.fm/archives/8847">https://kexue.fm/archives/8847</a></p></li>
</ul>
<dl>
<dt>Requirements:</dt><dd><ul class="simple">
<li><p>Sentence pairs with corresponding similarity scores in range of the similarity function. Default is [-1,1].</p></li>
</ul>
</dd>
<dt>Inputs:</dt><dd><table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Texts</p></th>
<th class="head"><p>Labels</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>(sentence_A, sentence_B) pairs</p></td>
<td><p>float similarity score</p></td>
</tr>
</tbody>
</table>
</dd>
<dt>Relations:</dt><dd><ul class="simple">
<li><p><a class="reference internal" href="#sentence_transformers.losses.AnglELoss" title="sentence_transformers.losses.AnglELoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">AnglELoss</span></code></a> is CoSENTLoss with <code class="docutils literal notranslate"><span class="pre">pairwise_angle_sim</span></code> as the metric, rather than <code class="docutils literal notranslate"><span class="pre">pairwise_cos_sim</span></code>.</p></li>
<li><p><a class="reference internal" href="#sentence_transformers.losses.CosineSimilarityLoss" title="sentence_transformers.losses.CosineSimilarityLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">CosineSimilarityLoss</span></code></a> seems to produce a weaker training signal than CoSENTLoss. In our experiments, CoSENTLoss is recommended.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span><span class="p">,</span> <span class="n">SentenceTransformerTrainer</span><span class="p">,</span> <span class="n">losses</span>
<span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">Dataset</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="s2">&quot;microsoft/mpnet-base&quot;</span><span class="p">)</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">Dataset</span><span class="o">.</span><span class="n">from_dict</span><span class="p">({</span>
    <span class="s2">&quot;sentence1&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;It&#39;s nice weather outside today.&quot;</span><span class="p">,</span> <span class="s2">&quot;He drove to work.&quot;</span><span class="p">],</span>
    <span class="s2">&quot;sentence2&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;It&#39;s so sunny.&quot;</span><span class="p">,</span> <span class="s2">&quot;She walked to the store.&quot;</span><span class="p">],</span>
    <span class="s2">&quot;score&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span>
<span class="p">})</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">CoSENTLoss</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">SentenceTransformerTrainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="angleloss">
<h2>AnglELoss<a class="headerlink" href="#angleloss" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="sentence_transformers.losses.AnglELoss">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sentence_transformers.losses.</span></span><span class="sig-name descname"><span class="pre">AnglELoss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="SentenceTransformer.html#sentence_transformers.SentenceTransformer" title="sentence_transformers.SentenceTransformer"><span class="pre">SentenceTransformer</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">20.0</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers\losses\AnglELoss.py#L6-L81"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sentence_transformers.losses.AnglELoss" title="Link to this definition"></a></dt>
<dd><p>This class implements AnglE (Angle Optimized) loss.
This is a modification of <a class="reference internal" href="#sentence_transformers.losses.CoSENTLoss" title="sentence_transformers.losses.CoSENTLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">CoSENTLoss</span></code></a>, designed to address the following issue:
The cosine functions gradient approaches 0 as the wave approaches the top or bottom of its form.
This can hinder the optimization process, so AnglE proposes to instead optimize the angle difference
in complex space in order to mitigate this effect.</p>
<p>It expects that each of the InputExamples consists of a pair of texts and a float valued label, representing
the expected similarity score between the pair.</p>
<p>It computes the following loss function:</p>
<p><code class="docutils literal notranslate"><span class="pre">loss</span> <span class="pre">=</span> <span class="pre">logsum(1+exp(s(k,l)-s(i,j))+exp...)</span></code>, where <code class="docutils literal notranslate"><span class="pre">(i,j)</span></code> and <code class="docutils literal notranslate"><span class="pre">(k,l)</span></code> are any of the input pairs in the
batch such that the expected similarity of <code class="docutils literal notranslate"><span class="pre">(i,j)</span></code> is greater than <code class="docutils literal notranslate"><span class="pre">(k,l)</span></code>. The summation is over all possible
pairs of input pairs in the batch that match this condition. This is the same as CoSENTLoss, with a different
similarity function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong>  SentenceTransformerModel</p></li>
<li><p><strong>scale</strong>  Output of similarity function is multiplied by scale
value. Represents the inverse temperature.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">References</p>
<ul class="simple">
<li><p>For further details, see: <a class="reference external" href="https://arxiv.org/abs/2309.12871v1">https://arxiv.org/abs/2309.12871v1</a></p></li>
</ul>
<dl>
<dt>Requirements:</dt><dd><ul class="simple">
<li><p>Sentence pairs with corresponding similarity scores in range of the similarity function. Default is [-1,1].</p></li>
</ul>
</dd>
<dt>Inputs:</dt><dd><table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Texts</p></th>
<th class="head"><p>Labels</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>(sentence_A, sentence_B) pairs</p></td>
<td><p>float similarity score</p></td>
</tr>
</tbody>
</table>
</dd>
<dt>Relations:</dt><dd><ul class="simple">
<li><p><a class="reference internal" href="#sentence_transformers.losses.CoSENTLoss" title="sentence_transformers.losses.CoSENTLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">CoSENTLoss</span></code></a> is AnglELoss with <code class="docutils literal notranslate"><span class="pre">pairwise_cos_sim</span></code> as the metric, rather than <code class="docutils literal notranslate"><span class="pre">pairwise_angle_sim</span></code>.</p></li>
<li><p><a class="reference internal" href="#sentence_transformers.losses.CosineSimilarityLoss" title="sentence_transformers.losses.CosineSimilarityLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">CosineSimilarityLoss</span></code></a> seems to produce a weaker training signal than <code class="docutils literal notranslate"><span class="pre">CoSENTLoss</span></code> or <code class="docutils literal notranslate"><span class="pre">AnglELoss</span></code>.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span><span class="p">,</span> <span class="n">SentenceTransformerTrainer</span><span class="p">,</span> <span class="n">losses</span>
<span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">Dataset</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="s2">&quot;microsoft/mpnet-base&quot;</span><span class="p">)</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">Dataset</span><span class="o">.</span><span class="n">from_dict</span><span class="p">({</span>
    <span class="s2">&quot;sentence1&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;It&#39;s nice weather outside today.&quot;</span><span class="p">,</span> <span class="s2">&quot;He drove to work.&quot;</span><span class="p">],</span>
    <span class="s2">&quot;sentence2&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;It&#39;s so sunny.&quot;</span><span class="p">,</span> <span class="s2">&quot;She walked to the store.&quot;</span><span class="p">],</span>
    <span class="s2">&quot;score&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span>
<span class="p">})</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">AnglELoss</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">SentenceTransformerTrainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="cosinesimilarityloss">
<h2>CosineSimilarityLoss<a class="headerlink" href="#cosinesimilarityloss" title="Link to this heading"></a></h2>
<img src="https://raw.githubusercontent.com/UKPLab/sentence-transformers/master/docs/img/SBERT_Siamese_Network.png" alt="SBERT Siamese Network Architecture" width="250"/>
<p>For each sentence pair, we pass sentence A and sentence B through our network which yields the embeddings <em>u</em> und <em>v</em>. The similarity of these embeddings is computed using cosine similarity and the result is compared to the gold similarity score.</p>
<p>This allows our network to be fine-tuned to recognize the similarity of sentences.</p>
<dl class="py class">
<dt class="sig sig-object py" id="sentence_transformers.losses.CosineSimilarityLoss">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sentence_transformers.losses.</span></span><span class="sig-name descname"><span class="pre">CosineSimilarityLoss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="SentenceTransformer.html#sentence_transformers.SentenceTransformer" title="sentence_transformers.SentenceTransformer.SentenceTransformer"><span class="pre">SentenceTransformer</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_fct</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.7)"><span class="pre">Module</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">MSELoss()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cos_score_transformation</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.7)"><span class="pre">Module</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">Identity()</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers\losses\CosineSimilarityLoss.py#L13-L98"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sentence_transformers.losses.CosineSimilarityLoss" title="Link to this definition"></a></dt>
<dd><p>CosineSimilarityLoss expects that the InputExamples consists of two texts and a float label. It computes the
vectors <code class="docutils literal notranslate"><span class="pre">u</span> <span class="pre">=</span> <span class="pre">model(sentence_A)</span></code> and <code class="docutils literal notranslate"><span class="pre">v</span> <span class="pre">=</span> <span class="pre">model(sentence_B)</span></code> and measures the cosine-similarity between the two.
By default, it minimizes the following loss: <code class="docutils literal notranslate"><span class="pre">||input_label</span> <span class="pre">-</span> <span class="pre">cos_score_transformation(cosine_sim(u,v))||_2</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong>  SentenceTransformer model</p></li>
<li><p><strong>loss_fct</strong>  Which pytorch loss function should be used to
compare the <code class="docutils literal notranslate"><span class="pre">cosine_similarity(u,</span> <span class="pre">v)</span></code> with the
input_label? By default, MSE is used: <code class="docutils literal notranslate"><span class="pre">||input_label</span> <span class="pre">-</span>
<span class="pre">cosine_sim(u,</span> <span class="pre">v)||_2</span></code></p></li>
<li><p><strong>cos_score_transformation</strong>  The cos_score_transformation
function is applied on top of cosine_similarity. By
default, the identify function is used (i.e. no change).</p></li>
</ul>
</dd>
</dl>
<p class="rubric">References</p>
<ul class="simple">
<li><p><a class="reference external" href="../../../examples/sentence_transformer/training/sts/README.html">Training Examples &gt; Semantic Textual Similarity</a></p></li>
</ul>
<dl>
<dt>Requirements:</dt><dd><ol class="arabic simple">
<li><p>Sentence pairs with corresponding similarity scores in range <cite>[0, 1]</cite></p></li>
</ol>
</dd>
<dt>Inputs:</dt><dd><table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Texts</p></th>
<th class="head"><p>Labels</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>(sentence_A, sentence_B) pairs</p></td>
<td><p>float similarity score</p></td>
</tr>
</tbody>
</table>
</dd>
<dt>Relations:</dt><dd><ul class="simple">
<li><p><a class="reference internal" href="#sentence_transformers.losses.CoSENTLoss" title="sentence_transformers.losses.CoSENTLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">CoSENTLoss</span></code></a> seems to produce a stronger training signal than CosineSimilarityLoss. In our experiments, CoSENTLoss is recommended.</p></li>
<li><p><a class="reference internal" href="#sentence_transformers.losses.AnglELoss" title="sentence_transformers.losses.AnglELoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">AnglELoss</span></code></a> is <a class="reference internal" href="#sentence_transformers.losses.CoSENTLoss" title="sentence_transformers.losses.CoSENTLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">CoSENTLoss</span></code></a> with <code class="docutils literal notranslate"><span class="pre">pairwise_angle_sim</span></code> as the metric, rather than <code class="docutils literal notranslate"><span class="pre">pairwise_cos_sim</span></code>. It also produces a stronger training signal than CosineSimilarityLoss.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span><span class="p">,</span> <span class="n">SentenceTransformerTrainer</span><span class="p">,</span> <span class="n">losses</span>
<span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">Dataset</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="s2">&quot;microsoft/mpnet-base&quot;</span><span class="p">)</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">Dataset</span><span class="o">.</span><span class="n">from_dict</span><span class="p">({</span>
    <span class="s2">&quot;sentence1&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;It&#39;s nice weather outside today.&quot;</span><span class="p">,</span> <span class="s2">&quot;He drove to work.&quot;</span><span class="p">],</span>
    <span class="s2">&quot;sentence2&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;It&#39;s so sunny.&quot;</span><span class="p">,</span> <span class="s2">&quot;She walked to the store.&quot;</span><span class="p">],</span>
    <span class="s2">&quot;score&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span>
<span class="p">})</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">CosineSimilarityLoss</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">SentenceTransformerTrainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="denoisingautoencoderloss">
<h2>DenoisingAutoEncoderLoss<a class="headerlink" href="#denoisingautoencoderloss" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="sentence_transformers.losses.DenoisingAutoEncoderLoss">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sentence_transformers.losses.</span></span><span class="sig-name descname"><span class="pre">DenoisingAutoEncoderLoss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="SentenceTransformer.html#sentence_transformers.SentenceTransformer" title="sentence_transformers.SentenceTransformer"><span class="pre">SentenceTransformer</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">decoder_name_or_path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tie_encoder_decoder</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers\losses\DenoisingAutoEncoderLoss.py#L15-L202"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sentence_transformers.losses.DenoisingAutoEncoderLoss" title="Link to this definition"></a></dt>
<dd><p>This loss expects as input a pairs of damaged sentences and the corresponding original ones.
During training, the decoder reconstructs the original sentences from the encoded sentence embeddings.
Here the argument decoder_name_or_path indicates the pretrained model (supported by Hugging Face) to be used as the decoder.
Since decoding process is included, here the decoder should have a class called XXXLMHead (in the context of Hugging Faces Transformers).
The tie_encoder_decoder flag indicates whether to tie the trainable parameters of encoder and decoder,
which is shown beneficial to model performance while limiting the amount of required memory.
Only when the encoder and decoder are from the same architecture, can the flag tie_encoder_decoder work.</p>
<p>The data generation process (i.e. the damaging process) has already been implemented in <code class="docutils literal notranslate"><span class="pre">DenoisingAutoEncoderDataset</span></code>,
allowing you to only provide regular sentences.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<a class="reference internal" href="SentenceTransformer.html#sentence_transformers.SentenceTransformer" title="sentence_transformers.SentenceTransformer"><em>SentenceTransformer</em></a>)  The SentenceTransformer model.</p></li>
<li><p><strong>decoder_name_or_path</strong> (<em>str</em><em>, </em><em>optional</em>)  Model name or path for initializing a decoder (compatible with Hugging Faces Transformers). Defaults to None.</p></li>
<li><p><strong>tie_encoder_decoder</strong> (<em>bool</em>)  Whether to tie the trainable parameters of encoder and decoder. Defaults to True.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">References</p>
<ul class="simple">
<li><p>TSDAE paper: <a class="reference external" href="https://arxiv.org/pdf/2104.06979.pdf">https://arxiv.org/pdf/2104.06979.pdf</a></p></li>
<li><p><a class="reference external" href="../../../examples/sentence_transformer/unsupervised_learning/TSDAE/README.html">Unsupervised Learning &gt; TSDAE</a></p></li>
</ul>
<dl>
<dt>Requirements:</dt><dd><ol class="arabic simple">
<li><p>The decoder should have a class called XXXLMHead (in the context of Hugging Faces Transformers)</p></li>
<li><p>Should use a large corpus</p></li>
</ol>
</dd>
<dt>Inputs:</dt><dd><table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Texts</p></th>
<th class="head"><p>Labels</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>(damaged_sentence, original_sentence) pairs</p></td>
<td><p>none</p></td>
</tr>
<tr class="row-odd"><td><p>sentence fed through <code class="docutils literal notranslate"><span class="pre">DenoisingAutoEncoderDataset</span></code></p></td>
<td><p>none</p></td>
</tr>
</tbody>
</table>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span><span class="p">,</span> <span class="n">losses</span>
<span class="kn">from</span> <span class="nn">sentence_transformers.datasets</span> <span class="kn">import</span> <span class="n">DenoisingAutoEncoderDataset</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;bert-base-cased&quot;</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
<span class="n">train_sentences</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;First training sentence&quot;</span><span class="p">,</span> <span class="s2">&quot;Second training sentence&quot;</span><span class="p">,</span> <span class="s2">&quot;Third training sentence&quot;</span><span class="p">,</span> <span class="s2">&quot;Fourth training sentence&quot;</span><span class="p">,</span>
<span class="p">]</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">DenoisingAutoEncoderDataset</span><span class="p">(</span><span class="n">train_sentences</span><span class="p">)</span>
<span class="n">train_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">drop_last</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">train_loss</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">DenoisingAutoEncoderLoss</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span> <span class="n">decoder_name_or_path</span><span class="o">=</span><span class="n">model_name</span><span class="p">,</span> <span class="n">tie_encoder_decoder</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">train_objectives</span><span class="o">=</span><span class="p">[(</span><span class="n">train_dataloader</span><span class="p">,</span> <span class="n">train_loss</span><span class="p">)],</span>
    <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="gistembedloss">
<h2>GISTEmbedLoss<a class="headerlink" href="#gistembedloss" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="sentence_transformers.losses.GISTEmbedLoss">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sentence_transformers.losses.</span></span><span class="sig-name descname"><span class="pre">GISTEmbedLoss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="SentenceTransformer.html#sentence_transformers.SentenceTransformer" title="sentence_transformers.SentenceTransformer.SentenceTransformer"><span class="pre">SentenceTransformer</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">guide</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="SentenceTransformer.html#sentence_transformers.SentenceTransformer" title="sentence_transformers.SentenceTransformer.SentenceTransformer"><span class="pre">SentenceTransformer</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">temperature</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.01</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">margin_strategy</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Literal</span><span class="p"><span class="pre">[</span></span><span class="s"><span class="pre">'absolute'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'relative'</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'absolute'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">margin</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.0</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers\losses\GISTEmbedLoss.py#L14-L225"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sentence_transformers.losses.GISTEmbedLoss" title="Link to this definition"></a></dt>
<dd><p>This loss is used to train a SentenceTransformer model using the GISTEmbed algorithm.
It takes a model and a guide model as input, and uses the guide model to guide the
in-batch negative sample selection. The cosine similarity is used to compute the loss
and the temperature parameter is used to scale the cosine similarities.</p>
<p>You can apply different false-negative filtering strategies to discard hard negatives that are too similar to
the positive. Two strategies are supported:</p>
<blockquote>
<div><ul class="simple">
<li><p>absolute: Discards negatives whose similarity score is greater than or equal to <code class="docutils literal notranslate"><span class="pre">positive_score</span> <span class="pre">-</span> <span class="pre">margin</span></code>.</p></li>
<li><p>relative: Discards negatives whose similarity score is greater than or equal to <code class="docutils literal notranslate"><span class="pre">positive_score</span> <span class="pre">*</span> <span class="pre">(1</span> <span class="pre">-</span> <span class="pre">margin)</span></code>.</p></li>
</ul>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong>  SentenceTransformer model based on a <cite>transformers</cite> model.</p></li>
<li><p><strong>guide</strong>  SentenceTransformer model to guide the in-batch negative sample selection.</p></li>
<li><p><strong>temperature</strong>  Temperature parameter to scale the cosine similarities.</p></li>
<li><p><strong>margin_strategy</strong>  Strategy used for false negative filtering. One of {absolute, relative}.</p></li>
<li><p><strong>margin</strong>  The margin value for filtering negatives. Defaults to 0.0, together with the absolute strategy,
this only removes negatives that are more similar to the query than the positive is to the query.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">References</p>
<ul class="simple">
<li><p>For further details, see: <a class="reference external" href="https://arxiv.org/abs/2402.16829">https://arxiv.org/abs/2402.16829</a></p></li>
</ul>
<dl>
<dt>Requirements:</dt><dd><ol class="arabic simple">
<li><p>(anchor, positive, negative) triplets</p></li>
<li><p>(anchor, positive) pairs</p></li>
</ol>
</dd>
<dt>Inputs:</dt><dd><table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Texts</p></th>
<th class="head"><p>Labels</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>(anchor, positive, negative) triplets</p></td>
<td><p>none</p></td>
</tr>
<tr class="row-odd"><td><p>(anchor, positive) pairs</p></td>
<td><p>none</p></td>
</tr>
</tbody>
</table>
</dd>
<dt>Recommendations:</dt><dd><ul class="simple">
<li><p>Use <code class="docutils literal notranslate"><span class="pre">BatchSamplers.NO_DUPLICATES</span></code> (<a class="reference internal" href="sampler.html#sentence_transformers.training_args.BatchSamplers" title="sentence_transformers.training_args.BatchSamplers"><code class="xref py py-class docutils literal notranslate"><span class="pre">docs</span></code></a>) to
ensure that no in-batch negatives are duplicates of the anchor or positive samples.</p></li>
</ul>
</dd>
<dt>Relations:</dt><dd><ul class="simple">
<li><p><a class="reference internal" href="#sentence_transformers.losses.MultipleNegativesRankingLoss" title="sentence_transformers.losses.MultipleNegativesRankingLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">MultipleNegativesRankingLoss</span></code></a> is similar to this loss, but it does not use
a guide model to guide the in-batch negative sample selection. <cite>GISTEmbedLoss</cite> yields
a stronger training signal at the cost of some training overhead.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span><span class="p">,</span> <span class="n">SentenceTransformerTrainer</span><span class="p">,</span> <span class="n">losses</span>
<span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">Dataset</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="s2">&quot;microsoft/mpnet-base&quot;</span><span class="p">)</span>
<span class="n">guide</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="s2">&quot;all-MiniLM-L6-v2&quot;</span><span class="p">)</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">Dataset</span><span class="o">.</span><span class="n">from_dict</span><span class="p">({</span>
    <span class="s2">&quot;anchor&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;It&#39;s nice weather outside today.&quot;</span><span class="p">,</span> <span class="s2">&quot;He drove to work.&quot;</span><span class="p">],</span>
    <span class="s2">&quot;positive&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;It&#39;s so sunny.&quot;</span><span class="p">,</span> <span class="s2">&quot;He took the car to the office.&quot;</span><span class="p">],</span>
<span class="p">})</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">GISTEmbedLoss</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">guide</span><span class="p">)</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">SentenceTransformerTrainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="cachedgistembedloss">
<h2>CachedGISTEmbedLoss<a class="headerlink" href="#cachedgistembedloss" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="sentence_transformers.losses.CachedGISTEmbedLoss">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sentence_transformers.losses.</span></span><span class="sig-name descname"><span class="pre">CachedGISTEmbedLoss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="SentenceTransformer.html#sentence_transformers.SentenceTransformer" title="sentence_transformers.SentenceTransformer"><span class="pre">SentenceTransformer</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">guide</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="SentenceTransformer.html#sentence_transformers.SentenceTransformer" title="sentence_transformers.SentenceTransformer"><span class="pre">SentenceTransformer</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">temperature</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.01</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mini_batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">show_progress_bar</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">margin_strategy</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Literal</span><span class="p"><span class="pre">[</span></span><span class="s"><span class="pre">'absolute'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'relative'</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'absolute'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">margin</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.0</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers\losses\CachedGISTEmbedLoss.py#L64-L387"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sentence_transformers.losses.CachedGISTEmbedLoss" title="Link to this definition"></a></dt>
<dd><p>This loss is a combination of <a class="reference internal" href="#sentence_transformers.losses.GISTEmbedLoss" title="sentence_transformers.losses.GISTEmbedLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">GISTEmbedLoss</span></code></a> and <a class="reference internal" href="#sentence_transformers.losses.CachedMultipleNegativesRankingLoss" title="sentence_transformers.losses.CachedMultipleNegativesRankingLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">CachedMultipleNegativesRankingLoss</span></code></a>.
Typically, <a class="reference internal" href="#sentence_transformers.losses.MultipleNegativesRankingLoss" title="sentence_transformers.losses.MultipleNegativesRankingLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">MultipleNegativesRankingLoss</span></code></a> requires a larger batch size for better performance.
<a class="reference internal" href="#sentence_transformers.losses.GISTEmbedLoss" title="sentence_transformers.losses.GISTEmbedLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">GISTEmbedLoss</span></code></a> yields stronger training signals than <a class="reference internal" href="#sentence_transformers.losses.MultipleNegativesRankingLoss" title="sentence_transformers.losses.MultipleNegativesRankingLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">MultipleNegativesRankingLoss</span></code></a> due to the
use of a guide model for in-batch negative sample selection. Meanwhile, <a class="reference internal" href="#sentence_transformers.losses.CachedMultipleNegativesRankingLoss" title="sentence_transformers.losses.CachedMultipleNegativesRankingLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">CachedMultipleNegativesRankingLoss</span></code></a>
allows for scaling of the batch size by dividing the computation into two stages of embedding and loss
calculation, which both can be scaled by mini-batches (<a class="reference external" href="https://arxiv.org/pdf/2101.06983.pdf">https://arxiv.org/pdf/2101.06983.pdf</a>).</p>
<p>By combining the guided selection from <a class="reference internal" href="#sentence_transformers.losses.GISTEmbedLoss" title="sentence_transformers.losses.GISTEmbedLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">GISTEmbedLoss</span></code></a> and Gradient Cache from
<a class="reference internal" href="#sentence_transformers.losses.CachedMultipleNegativesRankingLoss" title="sentence_transformers.losses.CachedMultipleNegativesRankingLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">CachedMultipleNegativesRankingLoss</span></code></a>, it is possible to reduce memory usage while maintaining performance
levels comparable to those of <a class="reference internal" href="#sentence_transformers.losses.GISTEmbedLoss" title="sentence_transformers.losses.GISTEmbedLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">GISTEmbedLoss</span></code></a>.</p>
<p>You can apply different false-negative filtering strategies to discard hard negatives that are too similar to
the positive. Two strategies are supported:</p>
<blockquote>
<div><ul class="simple">
<li><p>absolute: Discards negatives whose similarity score is greater than or equal to <code class="docutils literal notranslate"><span class="pre">positive_score</span> <span class="pre">-</span> <span class="pre">margin</span></code>.</p></li>
<li><p>relative: Discards negatives whose similarity score is greater than or equal to <code class="docutils literal notranslate"><span class="pre">positive_score</span> <span class="pre">*</span> <span class="pre">(1</span> <span class="pre">-</span> <span class="pre">margin)</span></code>.</p></li>
</ul>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong>  SentenceTransformer model</p></li>
<li><p><strong>guide</strong>  SentenceTransformer model to guide the in-batch negative sample selection.</p></li>
<li><p><strong>temperature</strong>  Temperature parameter to scale the cosine similarities.</p></li>
<li><p><strong>mini_batch_size</strong>  Mini-batch size for the forward pass, this denotes how much memory is actually used during
training and evaluation. The larger the mini-batch size, the more memory efficient the training is, but
the slower the training will be. Its recommended to set it as high as your GPU memory allows. The default
value is 32.</p></li>
<li><p><strong>show_progress_bar</strong>  If True, a progress bar for the mini-batches is shown during training. The default is False.</p></li>
<li><p><strong>margin_strategy</strong>  Strategy used for false negative filtering. One of {absolute, relative}.</p></li>
<li><p><strong>margin</strong>  The margin value for filtering negatives. Defaults to 0.0, together with the absolute strategy,
this only removes negatives that are more similar to the query than the positive is to the query.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">References</p>
<ul class="simple">
<li><p>Efficient Natural Language Response Suggestion for Smart Reply, Section 4.4: <a class="reference external" href="https://arxiv.org/pdf/1705.00652.pdf">https://arxiv.org/pdf/1705.00652.pdf</a></p></li>
<li><p>Scaling Deep Contrastive Learning Batch Size under Memory Limited Setup: <a class="reference external" href="https://arxiv.org/pdf/2101.06983.pdf">https://arxiv.org/pdf/2101.06983.pdf</a></p></li>
<li><p>GISTEmbed: Guided In-sample Selection of Training Negatives for Text Embedding Fine-tuning <a class="reference external" href="https://arxiv.org/abs/2402.16829">https://arxiv.org/abs/2402.16829</a></p></li>
</ul>
<dl>
<dt>Requirements:</dt><dd><ol class="arabic simple">
<li><p>(anchor, positive) pairs or (anchor, positive, negative pairs)</p></li>
<li><p>Should be used with large batch sizes for superior performance, but has slower training time than <a class="reference internal" href="#sentence_transformers.losses.MultipleNegativesRankingLoss" title="sentence_transformers.losses.MultipleNegativesRankingLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">MultipleNegativesRankingLoss</span></code></a></p></li>
</ol>
</dd>
<dt>Inputs:</dt><dd><table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Texts</p></th>
<th class="head"><p>Labels</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>(anchor, positive) pairs</p></td>
<td><p>none</p></td>
</tr>
<tr class="row-odd"><td><p>(anchor, positive, negative) triplets</p></td>
<td><p>none</p></td>
</tr>
<tr class="row-even"><td><p>(anchor, positive, negative_1, , negative_n)</p></td>
<td><p>none</p></td>
</tr>
</tbody>
</table>
</dd>
<dt>Recommendations:</dt><dd><ul class="simple">
<li><p>Use <code class="docutils literal notranslate"><span class="pre">BatchSamplers.NO_DUPLICATES</span></code> (<a class="reference internal" href="sampler.html#sentence_transformers.training_args.BatchSamplers" title="sentence_transformers.training_args.BatchSamplers"><code class="xref py py-class docutils literal notranslate"><span class="pre">docs</span></code></a>) to
ensure that no in-batch negatives are duplicates of the anchor or positive samples.</p></li>
</ul>
</dd>
<dt>Relations:</dt><dd><ul class="simple">
<li><p>Equivalent to <a class="reference internal" href="#sentence_transformers.losses.GISTEmbedLoss" title="sentence_transformers.losses.GISTEmbedLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">GISTEmbedLoss</span></code></a>, but with caching that allows for much higher batch sizes</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span><span class="p">,</span> <span class="n">SentenceTransformerTrainer</span><span class="p">,</span> <span class="n">losses</span>
<span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">Dataset</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="s2">&quot;microsoft/mpnet-base&quot;</span><span class="p">)</span>
<span class="n">guide</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="s2">&quot;all-MiniLM-L6-v2&quot;</span><span class="p">)</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">Dataset</span><span class="o">.</span><span class="n">from_dict</span><span class="p">({</span>
    <span class="s2">&quot;anchor&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;It&#39;s nice weather outside today.&quot;</span><span class="p">,</span> <span class="s2">&quot;He drove to work.&quot;</span><span class="p">],</span>
    <span class="s2">&quot;positive&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;It&#39;s so sunny.&quot;</span><span class="p">,</span> <span class="s2">&quot;He took the car to the office.&quot;</span><span class="p">],</span>
<span class="p">})</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">CachedGISTEmbedLoss</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">guide</span><span class="p">,</span>
    <span class="n">mini_batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
    <span class="n">margin_strategy</span><span class="o">=</span><span class="s2">&quot;absolute&quot;</span><span class="p">,</span>   <span class="c1"># or &quot;relative&quot; (e.g., margin=0.05 for max. 95% of positive similarity)</span>
    <span class="n">margin</span><span class="o">=</span><span class="mf">0.1</span>
<span class="p">)</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">SentenceTransformerTrainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="mseloss">
<h2>MSELoss<a class="headerlink" href="#mseloss" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="sentence_transformers.losses.MSELoss">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sentence_transformers.losses.</span></span><span class="sig-name descname"><span class="pre">MSELoss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="SentenceTransformer.html#sentence_transformers.SentenceTransformer" title="sentence_transformers.SentenceTransformer"><span class="pre">SentenceTransformer</span></a></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers\losses\MSELoss.py#L11-L97"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sentence_transformers.losses.MSELoss" title="Link to this definition"></a></dt>
<dd><p>Computes the MSE loss between the computed sentence embedding and a target sentence embedding. This loss
is used when extending sentence embeddings to new languages as described in our publication
Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation.</p>
<p>For an example, see <a class="reference external" href="../../../examples/sentence_transformer/training/distillation/README.html">the distillation documentation</a> on extending language models to new languages.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>model</strong>  SentenceTransformerModel</p>
</dd>
</dl>
<p class="rubric">References</p>
<ul class="simple">
<li><p>Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation: <a class="reference external" href="https://arxiv.org/abs/2004.09813">https://arxiv.org/abs/2004.09813</a></p></li>
<li><p><a class="reference external" href="../../../examples/sentence_transformer/training/distillation/README.html">Training &gt; Model Distillation</a></p></li>
<li><p><a class="reference external" href="../../../examples/sentence_transformer/training/multilingual/README.html">Training &gt; Multilingual Models</a></p></li>
</ul>
<dl>
<dt>Requirements:</dt><dd><ol class="arabic simple">
<li><p>Usually uses a finetuned teacher M in a knowledge distillation setup</p></li>
</ol>
</dd>
<dt>Inputs:</dt><dd><table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Texts</p></th>
<th class="head"><p>Labels</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>sentence</p></td>
<td><p>model sentence embeddings</p></td>
</tr>
<tr class="row-odd"><td><p>sentence_1, sentence_2, , sentence_N</p></td>
<td><p>model sentence embeddings</p></td>
</tr>
</tbody>
</table>
</dd>
<dt>Relations:</dt><dd><ul class="simple">
<li><p><a class="reference internal" href="#sentence_transformers.losses.MarginMSELoss" title="sentence_transformers.losses.MarginMSELoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">MarginMSELoss</span></code></a> is equivalent to this loss, but with a margin through a negative pair.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span><span class="p">,</span> <span class="n">SentenceTransformerTrainer</span><span class="p">,</span> <span class="n">losses</span>
<span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">Dataset</span>

<span class="n">student_model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="s2">&quot;microsoft/mpnet-base&quot;</span><span class="p">)</span>
<span class="n">teacher_model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="s2">&quot;all-mpnet-base-v2&quot;</span><span class="p">)</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">Dataset</span><span class="o">.</span><span class="n">from_dict</span><span class="p">({</span>
    <span class="s2">&quot;english&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;The first sentence&quot;</span><span class="p">,</span>  <span class="s2">&quot;The second sentence&quot;</span><span class="p">,</span> <span class="s2">&quot;The third sentence&quot;</span><span class="p">,</span>  <span class="s2">&quot;The fourth sentence&quot;</span><span class="p">],</span>
    <span class="s2">&quot;french&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;La premire phrase&quot;</span><span class="p">,</span>  <span class="s2">&quot;La deuxime phrase&quot;</span><span class="p">,</span> <span class="s2">&quot;La troisime phrase&quot;</span><span class="p">,</span>  <span class="s2">&quot;La quatrime phrase&quot;</span><span class="p">],</span>
<span class="p">})</span>

<span class="k">def</span> <span class="nf">compute_labels</span><span class="p">(</span><span class="n">batch</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;label&quot;</span><span class="p">:</span> <span class="n">teacher_model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s2">&quot;english&quot;</span><span class="p">])</span>
    <span class="p">}</span>

<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">train_dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">compute_labels</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">(</span><span class="n">student_model</span><span class="p">)</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">SentenceTransformerTrainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">student_model</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="marginmseloss">
<h2>MarginMSELoss<a class="headerlink" href="#marginmseloss" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="sentence_transformers.losses.MarginMSELoss">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sentence_transformers.losses.</span></span><span class="sig-name descname"><span class="pre">MarginMSELoss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model:</span> <span class="pre">SentenceTransformer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">similarity_fct=&lt;function</span> <span class="pre">pairwise_dot_score&gt;</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers\losses\MarginMSELoss.py#L11-L221"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sentence_transformers.losses.MarginMSELoss" title="Link to this definition"></a></dt>
<dd><p>Compute the MSE loss between the <code class="docutils literal notranslate"><span class="pre">|sim(Query,</span> <span class="pre">Pos)</span> <span class="pre">-</span> <span class="pre">sim(Query,</span> <span class="pre">Neg)|</span></code> and <code class="docutils literal notranslate"><span class="pre">|gold_sim(Query,</span> <span class="pre">Pos)</span> <span class="pre">-</span> <span class="pre">gold_sim(Query,</span> <span class="pre">Neg)|</span></code>.
By default, sim() is the dot-product. The gold_sim is often the similarity score from a teacher model.</p>
<p>In contrast to <a class="reference internal" href="#sentence_transformers.losses.MultipleNegativesRankingLoss" title="sentence_transformers.losses.MultipleNegativesRankingLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">MultipleNegativesRankingLoss</span></code></a>, the two passages do not
have to be strictly positive and negative, both can be relevant or not relevant for a given query. This can be
an advantage of MarginMSELoss over MultipleNegativesRankingLoss, but note that the MarginMSELoss is much slower
to train. With MultipleNegativesRankingLoss, with a batch size of 64, we compare one query against 128 passages.
With MarginMSELoss, we compare a query only against two passages. Its also possible to use multiple negatives
with MarginMSELoss, but the training would be even slower to train.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong>  SentenceTransformerModel</p></li>
<li><p><strong>similarity_fct</strong>  Which similarity function to use.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">References</p>
<ul class="simple">
<li><p>For more details, please refer to <a class="reference external" href="https://arxiv.org/abs/2010.02666">https://arxiv.org/abs/2010.02666</a>.</p></li>
<li><p><a class="reference external" href="../../../examples/sentence_transformer/training/ms_marco/README.html">Training Examples &gt; MS MARCO</a></p></li>
<li><p><a class="reference external" href="../../../examples/sentence_transformer/domain_adaptation/README.html">Unsupervised Learning &gt; Domain Adaptation</a></p></li>
</ul>
<dl>
<dt>Requirements:</dt><dd><ol class="arabic simple">
<li><p>(query, passage_one, passage_two) triplets or (query, positive, negative_1, , negative_n)</p></li>
<li><p>Usually used with a finetuned teacher M in a knowledge distillation setup</p></li>
</ol>
</dd>
<dt>Inputs:</dt><dd><table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Texts</p></th>
<th class="head"><p>Labels</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>(query, passage_one, passage_two) triplets</p></td>
<td><p>M(query, passage_one) - M(query, passage_two)</p></td>
</tr>
<tr class="row-odd"><td><p>(query, passage_one, passage_two) triplets</p></td>
<td><p>[M(query, passage_one), M(query, passage_two)]</p></td>
</tr>
<tr class="row-even"><td><p>(query, positive, negative_1, , negative_n)</p></td>
<td><p>[M(query, positive) - M(query, negative_i) for i in 1..n]</p></td>
</tr>
<tr class="row-odd"><td><p>(query, positive, negative_1, , negative_n)</p></td>
<td><p>[M(query, positive), M(query, negative_1), , M(query, negative_n)]</p></td>
</tr>
</tbody>
</table>
</dd>
<dt>Relations:</dt><dd><ul class="simple">
<li><p><a class="reference internal" href="#sentence_transformers.losses.MSELoss" title="sentence_transformers.losses.MSELoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">MSELoss</span></code></a> is similar to this loss, but without a margin through the negative pair.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Example</p>
<p>With gold labels, e.g. if you have hard scores for sentences. Imagine you want a model to embed sentences
with similar quality close to each other. If the text1 has quality 5 out of 5, text2 has quality
1 out of 5, and text3 has quality 3 out of 5, then the similarity of a pair can be defined as the
difference of the quality scores. So, the similarity between text1 and text2 is 4, and the
similarity between text1 and text3 is 2. If we use this as our Teacher Model, the label becomes
similraity(text1, text2) - similarity(text1, text3) = 4 - 2 = 2.</p>
<p>Positive values denote that the first passage is more similar to the query than the second passage,
while negative values denote the opposite.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span><span class="p">,</span> <span class="n">SentenceTransformerTrainer</span><span class="p">,</span> <span class="n">losses</span>
<span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">Dataset</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="s2">&quot;microsoft/mpnet-base&quot;</span><span class="p">)</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">Dataset</span><span class="o">.</span><span class="n">from_dict</span><span class="p">({</span>
    <span class="s2">&quot;text1&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;It&#39;s nice weather outside today.&quot;</span><span class="p">,</span> <span class="s2">&quot;He drove to work.&quot;</span><span class="p">],</span>
    <span class="s2">&quot;text2&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;It&#39;s so sunny.&quot;</span><span class="p">,</span> <span class="s2">&quot;He took the car to work.&quot;</span><span class="p">],</span>
    <span class="s2">&quot;text3&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;It&#39;s very sunny.&quot;</span><span class="p">,</span> <span class="s2">&quot;She walked to the store.&quot;</span><span class="p">],</span>
    <span class="s2">&quot;label&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">],</span>
<span class="p">})</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">MarginMSELoss</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">SentenceTransformerTrainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>
</div>
<p>We can also use a teacher model to compute the similarity scores. In this case, we can use the teacher model
to compute the similarity scores and use them as the silver labels. This is often used in knowledge distillation.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span><span class="p">,</span> <span class="n">SentenceTransformerTrainer</span><span class="p">,</span> <span class="n">losses</span>
<span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">Dataset</span>

<span class="n">student_model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="s2">&quot;microsoft/mpnet-base&quot;</span><span class="p">)</span>
<span class="n">teacher_model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="s2">&quot;all-mpnet-base-v2&quot;</span><span class="p">)</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">Dataset</span><span class="o">.</span><span class="n">from_dict</span><span class="p">({</span>
    <span class="s2">&quot;query&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;It&#39;s nice weather outside today.&quot;</span><span class="p">,</span> <span class="s2">&quot;He drove to work.&quot;</span><span class="p">],</span>
    <span class="s2">&quot;passage1&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;It&#39;s so sunny.&quot;</span><span class="p">,</span> <span class="s2">&quot;He took the car to work.&quot;</span><span class="p">],</span>
    <span class="s2">&quot;passage2&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;It&#39;s very sunny.&quot;</span><span class="p">,</span> <span class="s2">&quot;She walked to the store.&quot;</span><span class="p">],</span>
<span class="p">})</span>

<span class="k">def</span> <span class="nf">compute_labels</span><span class="p">(</span><span class="n">batch</span><span class="p">):</span>
    <span class="n">emb_queries</span> <span class="o">=</span> <span class="n">teacher_model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s2">&quot;query&quot;</span><span class="p">])</span>
    <span class="n">emb_passages1</span> <span class="o">=</span> <span class="n">teacher_model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s2">&quot;passage1&quot;</span><span class="p">])</span>
    <span class="n">emb_passages2</span> <span class="o">=</span> <span class="n">teacher_model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s2">&quot;passage2&quot;</span><span class="p">])</span>
    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;label&quot;</span><span class="p">:</span> <span class="n">teacher_model</span><span class="o">.</span><span class="n">similarity_pairwise</span><span class="p">(</span><span class="n">emb_queries</span><span class="p">,</span> <span class="n">emb_passages1</span><span class="p">)</span> <span class="o">-</span> <span class="n">teacher_model</span><span class="o">.</span><span class="n">similarity_pairwise</span><span class="p">(</span><span class="n">emb_queries</span><span class="p">,</span> <span class="n">emb_passages2</span><span class="p">)</span>
    <span class="p">}</span>

<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">train_dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">compute_labels</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">MarginMSELoss</span><span class="p">(</span><span class="n">student_model</span><span class="p">)</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">SentenceTransformerTrainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">student_model</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>
</div>
<p>We can also use multiple negatives during the knowledge distillation.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span><span class="p">,</span> <span class="n">SentenceTransformerTrainer</span><span class="p">,</span> <span class="n">losses</span>
<span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">Dataset</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="n">student_model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="s2">&quot;microsoft/mpnet-base&quot;</span><span class="p">)</span>
<span class="n">teacher_model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="s2">&quot;all-mpnet-base-v2&quot;</span><span class="p">)</span>

<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">Dataset</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span>
    <span class="p">{</span>
        <span class="s2">&quot;query&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;It&#39;s nice weather outside today.&quot;</span><span class="p">,</span> <span class="s2">&quot;He drove to work.&quot;</span><span class="p">],</span>
        <span class="s2">&quot;passage1&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;It&#39;s so sunny.&quot;</span><span class="p">,</span> <span class="s2">&quot;He took the car to work.&quot;</span><span class="p">],</span>
        <span class="s2">&quot;passage2&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;It&#39;s very cold.&quot;</span><span class="p">,</span> <span class="s2">&quot;She walked to the store.&quot;</span><span class="p">],</span>
        <span class="s2">&quot;passage3&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;Its rainy&quot;</span><span class="p">,</span> <span class="s2">&quot;She took the bus&quot;</span><span class="p">],</span>
    <span class="p">}</span>
<span class="p">)</span>


<span class="k">def</span> <span class="nf">compute_labels</span><span class="p">(</span><span class="n">batch</span><span class="p">):</span>
    <span class="n">emb_queries</span> <span class="o">=</span> <span class="n">teacher_model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s2">&quot;query&quot;</span><span class="p">])</span>
    <span class="n">emb_passages1</span> <span class="o">=</span> <span class="n">teacher_model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s2">&quot;passage1&quot;</span><span class="p">])</span>
    <span class="n">emb_passages2</span> <span class="o">=</span> <span class="n">teacher_model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s2">&quot;passage2&quot;</span><span class="p">])</span>
    <span class="n">emb_passages3</span> <span class="o">=</span> <span class="n">teacher_model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s2">&quot;passage3&quot;</span><span class="p">])</span>
    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;label&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="n">teacher_model</span><span class="o">.</span><span class="n">similarity_pairwise</span><span class="p">(</span><span class="n">emb_queries</span><span class="p">,</span> <span class="n">emb_passages1</span><span class="p">)</span>
                <span class="o">-</span> <span class="n">teacher_model</span><span class="o">.</span><span class="n">similarity_pairwise</span><span class="p">(</span><span class="n">emb_queries</span><span class="p">,</span> <span class="n">emb_passages2</span><span class="p">),</span>
                <span class="n">teacher_model</span><span class="o">.</span><span class="n">similarity_pairwise</span><span class="p">(</span><span class="n">emb_queries</span><span class="p">,</span> <span class="n">emb_passages1</span><span class="p">)</span>
                <span class="o">-</span> <span class="n">teacher_model</span><span class="o">.</span><span class="n">similarity_pairwise</span><span class="p">(</span><span class="n">emb_queries</span><span class="p">,</span> <span class="n">emb_passages3</span><span class="p">),</span>
            <span class="p">],</span>
            <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="p">}</span>


<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">train_dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">compute_labels</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">MarginMSELoss</span><span class="p">(</span><span class="n">student_model</span><span class="p">)</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">SentenceTransformerTrainer</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">student_model</span><span class="p">,</span> <span class="n">train_dataset</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="matryoshkaloss">
<h2>MatryoshkaLoss<a class="headerlink" href="#matryoshkaloss" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="sentence_transformers.losses.MatryoshkaLoss">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sentence_transformers.losses.</span></span><span class="sig-name descname"><span class="pre">MatryoshkaLoss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="SentenceTransformer.html#sentence_transformers.SentenceTransformer" title="sentence_transformers.SentenceTransformer"><span class="pre">SentenceTransformer</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">nn.Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">matryoshka_dims</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">matryoshka_weights</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_dims_per_step</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">-1</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers\losses\MatryoshkaLoss.py#L113-L252"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sentence_transformers.losses.MatryoshkaLoss" title="Link to this definition"></a></dt>
<dd><p>The MatryoshkaLoss can be seen as a loss <em>modifier</em> that allows you to use other loss functions at various
different embedding dimensions. This is useful for when you want to train a model where users have the option
to lower the embedding dimension to improve their embedding comparison speed and costs.</p>
<p>This loss is also compatible with the Cached losses, which are in-batch negative losses that allow for
higher batch sizes. The higher batch sizes allow for more negatives, and often result in a stronger model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong>  SentenceTransformer model</p></li>
<li><p><strong>loss</strong>  The loss function to be used, e.g.
<a class="reference internal" href="#sentence_transformers.losses.MultipleNegativesRankingLoss" title="sentence_transformers.losses.MultipleNegativesRankingLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">MultipleNegativesRankingLoss</span></code></a>,
<a class="reference internal" href="#sentence_transformers.losses.CoSENTLoss" title="sentence_transformers.losses.CoSENTLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">CoSENTLoss</span></code></a>, etc.</p></li>
<li><p><strong>matryoshka_dims</strong>  A list of embedding dimensions to be used
for the loss function, e.g. [768, 512, 256, 128, 64].</p></li>
<li><p><strong>matryoshka_weights</strong>  A list of weights to be used for the
loss function, e.g. [1, 1, 1, 1, 1]. If None, then the
weights will be set to 1 for all dimensions.</p></li>
<li><p><strong>n_dims_per_step</strong>  The number of dimensions to use per step.
If -1, then all dimensions are used. If &gt; 0, then a
random sample of n_dims_per_step dimensions are used per
step. The default value is -1.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">References</p>
<ul class="simple">
<li><p>The concept was introduced in this paper: <a class="reference external" href="https://arxiv.org/abs/2205.13147">https://arxiv.org/abs/2205.13147</a></p></li>
<li><p><a class="reference external" href="../../../examples/sentence_transformer/training/matryoshka/README.html">Matryoshka Embeddings</a></p></li>
</ul>
<dl>
<dt>Inputs:</dt><dd><table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Texts</p></th>
<th class="head"><p>Labels</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>any</p></td>
<td><p>any</p></td>
</tr>
</tbody>
</table>
</dd>
<dt>Relations:</dt><dd><ul class="simple">
<li><dl class="simple">
<dt><a class="reference internal" href="#sentence_transformers.losses.Matryoshka2dLoss" title="sentence_transformers.losses.Matryoshka2dLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">Matryoshka2dLoss</span></code></a> uses this loss in combination with <a class="reference internal" href="#sentence_transformers.losses.AdaptiveLayerLoss" title="sentence_transformers.losses.AdaptiveLayerLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">AdaptiveLayerLoss</span></code></a> which allows for</dt><dd><p>layer reduction for faster inference.</p>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span><span class="p">,</span> <span class="n">SentenceTransformerTrainer</span><span class="p">,</span> <span class="n">losses</span>
<span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">Dataset</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="s2">&quot;microsoft/mpnet-base&quot;</span><span class="p">)</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">Dataset</span><span class="o">.</span><span class="n">from_dict</span><span class="p">({</span>
    <span class="s2">&quot;anchor&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;It&#39;s nice weather outside today.&quot;</span><span class="p">,</span> <span class="s2">&quot;He drove to work.&quot;</span><span class="p">],</span>
    <span class="s2">&quot;positive&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;It&#39;s so sunny.&quot;</span><span class="p">,</span> <span class="s2">&quot;He took the car to the office.&quot;</span><span class="p">],</span>
<span class="p">})</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">MultipleNegativesRankingLoss</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">MatryoshkaLoss</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="p">[</span><span class="mi">768</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">64</span><span class="p">])</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">SentenceTransformerTrainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="matryoshka2dloss">
<h2>Matryoshka2dLoss<a class="headerlink" href="#matryoshka2dloss" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="sentence_transformers.losses.Matryoshka2dLoss">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sentence_transformers.losses.</span></span><span class="sig-name descname"><span class="pre">Matryoshka2dLoss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="SentenceTransformer.html#sentence_transformers.SentenceTransformer" title="sentence_transformers.SentenceTransformer"><span class="pre">SentenceTransformer</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="models.html#sentence_transformers.models.Module" title="sentence_transformers.models.Module"><span class="pre">Module</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">matryoshka_dims</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">matryoshka_weights</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_layers_per_step</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_dims_per_step</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">last_layer_weight</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prior_layers_weight</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kl_div_weight</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kl_temperature</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.3</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers\losses\Matryoshka2dLoss.py#L13-L151"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sentence_transformers.losses.Matryoshka2dLoss" title="Link to this definition"></a></dt>
<dd><p>The Matryoshka2dLoss can be seen as a loss <em>modifier</em> that combines the <a class="reference internal" href="#sentence_transformers.losses.AdaptiveLayerLoss" title="sentence_transformers.losses.AdaptiveLayerLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">AdaptiveLayerLoss</span></code></a> and the
<a class="reference internal" href="#sentence_transformers.losses.MatryoshkaLoss" title="sentence_transformers.losses.MatryoshkaLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">MatryoshkaLoss</span></code></a>. This allows you to train an embedding model that 1) allows users to specify the number
of model layers to use, and 2) allows users to specify the output dimensions to use.</p>
<p>The former is useful for when you want users to have the option to lower the number of layers used to improve
their inference speed and memory usage, and the latter is useful for when you want users to have the option to
lower the output dimensions to improve the efficiency of their downstream tasks (e.g. retrieval) or to lower
their storage costs.</p>
<p>Note, this uses <cite>n_layers_per_step=1</cite> and <cite>n_dims_per_step=1</cite> as default, following the original 2DMSE
implementation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong>  SentenceTransformer model</p></li>
<li><p><strong>loss</strong>  The loss function to be used, e.g.
<a class="reference internal" href="#sentence_transformers.losses.MultipleNegativesRankingLoss" title="sentence_transformers.losses.MultipleNegativesRankingLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">MultipleNegativesRankingLoss</span></code></a>,
<a class="reference internal" href="#sentence_transformers.losses.CoSENTLoss" title="sentence_transformers.losses.CoSENTLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">CoSENTLoss</span></code></a>, etc.</p></li>
<li><p><strong>matryoshka_dims</strong>  A list of embedding dimensions to be used
for the loss function, e.g. [768, 512, 256, 128, 64].</p></li>
<li><p><strong>matryoshka_weights</strong>  A list of weights to be used for the
loss function, e.g. [1, 1, 1, 1, 1]. If None, then the
weights will be set to 1 for all dimensions.</p></li>
<li><p><strong>n_layers_per_step</strong>  The number of layers to use per step. If
-1, then all layers are used. If &gt; 0, then a random
sample of n_layers_per_step layers are used per step.
The 2DMSE paper uses <cite>n_layers_per_step=1</cite>. The default
value is -1.</p></li>
<li><p><strong>n_dims_per_step</strong>  The number of dimensions to use per step.
If -1, then all dimensions are used. If &gt; 0, then a
random sample of n_dims_per_step dimensions are used per
step. The default value is -1.</p></li>
<li><p><strong>last_layer_weight</strong>  The weight to use for the loss of the
final layer. Increase this to focus more on the
performance when using all layers. The default value is
1.0.</p></li>
<li><p><strong>prior_layers_weight</strong>  The weight to use for the loss of the
prior layers. Increase this to focus more on the
performance when using fewer layers. The default value
is 1.0.</p></li>
<li><p><strong>kl_div_weight</strong>  The weight to use for the KL-divergence loss
that is used to make the prior layers match that of the
last layer. Increase this to focus more on the
performance when using fewer layers. The default value
is 1.0.</p></li>
<li><p><strong>kl_temperature</strong>  The temperature to use for the KL-divergence
loss. If 0, then the KL-divergence loss is not used. The
default value is 1.0.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">References</p>
<ul class="simple">
<li><p>See the 2D Matryoshka Sentence Embeddings (2DMSE) paper: <a class="reference external" href="https://arxiv.org/abs/2402.14776">https://arxiv.org/abs/2402.14776</a></p></li>
<li><p><a class="reference external" href="../../../examples/sentence_transformer/training/matryoshka/README.html">Matryoshka Embeddings</a></p></li>
<li><p><a class="reference external" href="../../../examples/sentence_transformer/training/adaptive_layer/README.html">Adaptive Layers</a></p></li>
</ul>
<dl>
<dt>Requirements:</dt><dd><ol class="arabic simple">
<li><p>The base loss cannot be <a class="reference internal" href="#sentence_transformers.losses.CachedMultipleNegativesRankingLoss" title="sentence_transformers.losses.CachedMultipleNegativesRankingLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">CachedMultipleNegativesRankingLoss</span></code></a>,
<a class="reference internal" href="#sentence_transformers.losses.CachedMultipleNegativesSymmetricRankingLoss" title="sentence_transformers.losses.CachedMultipleNegativesSymmetricRankingLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">CachedMultipleNegativesSymmetricRankingLoss</span></code></a>, or <a class="reference internal" href="#sentence_transformers.losses.CachedGISTEmbedLoss" title="sentence_transformers.losses.CachedGISTEmbedLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">CachedGISTEmbedLoss</span></code></a>.</p></li>
</ol>
</dd>
<dt>Inputs:</dt><dd><table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Texts</p></th>
<th class="head"><p>Labels</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>any</p></td>
<td><p>any</p></td>
</tr>
</tbody>
</table>
</dd>
<dt>Relations:</dt><dd><ul class="simple">
<li><p><a class="reference internal" href="#sentence_transformers.losses.MatryoshkaLoss" title="sentence_transformers.losses.MatryoshkaLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">MatryoshkaLoss</span></code></a> is used in this loss, and it is responsible for the dimensionality reduction.</p></li>
<li><p><a class="reference internal" href="#sentence_transformers.losses.AdaptiveLayerLoss" title="sentence_transformers.losses.AdaptiveLayerLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">AdaptiveLayerLoss</span></code></a> is used in this loss, and it is responsible for the layer reduction.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span><span class="p">,</span> <span class="n">SentenceTransformerTrainer</span><span class="p">,</span> <span class="n">losses</span>
<span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">Dataset</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="s2">&quot;microsoft/mpnet-base&quot;</span><span class="p">)</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">Dataset</span><span class="o">.</span><span class="n">from_dict</span><span class="p">({</span>
    <span class="s2">&quot;anchor&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;It&#39;s nice weather outside today.&quot;</span><span class="p">,</span> <span class="s2">&quot;He drove to work.&quot;</span><span class="p">],</span>
    <span class="s2">&quot;positive&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;It&#39;s so sunny.&quot;</span><span class="p">,</span> <span class="s2">&quot;He took the car to the office.&quot;</span><span class="p">],</span>
<span class="p">})</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">MultipleNegativesRankingLoss</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">Matryoshka2dLoss</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="p">[</span><span class="mi">768</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">64</span><span class="p">])</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">SentenceTransformerTrainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="adaptivelayerloss">
<h2>AdaptiveLayerLoss<a class="headerlink" href="#adaptivelayerloss" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="sentence_transformers.losses.AdaptiveLayerLoss">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sentence_transformers.losses.</span></span><span class="sig-name descname"><span class="pre">AdaptiveLayerLoss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="SentenceTransformer.html#sentence_transformers.SentenceTransformer" title="sentence_transformers.SentenceTransformer"><span class="pre">SentenceTransformer</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">nn.Module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_layers_per_step</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">last_layer_weight</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prior_layers_weight</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kl_div_weight</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kl_temperature</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.3</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers\losses\AdaptiveLayerLoss.py#L106-L273"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sentence_transformers.losses.AdaptiveLayerLoss" title="Link to this definition"></a></dt>
<dd><p>The AdaptiveLayerLoss can be seen as a loss <em>modifier</em> that allows you to use other loss functions at non-final
layers of the Sentence Transformer model. This is useful for when you want to train a model where users have
the option to lower the number of layers used to improve their inference speed and memory usage.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong>  SentenceTransformer model</p></li>
<li><p><strong>loss</strong>  The loss function to be used, e.g.
<a class="reference internal" href="#sentence_transformers.losses.MultipleNegativesRankingLoss" title="sentence_transformers.losses.MultipleNegativesRankingLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">MultipleNegativesRankingLoss</span></code></a>,
<a class="reference internal" href="#sentence_transformers.losses.CoSENTLoss" title="sentence_transformers.losses.CoSENTLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">CoSENTLoss</span></code></a>, etc.</p></li>
<li><p><strong>n_layers_per_step</strong>  The number of layers to use per step. If
-1, then all layers are used. If &gt; 0, then a random
sample of <cite>n_layers_per_step</cite> layers are used per step,
separate from the final layer, which is always used. The
2DMSE paper uses <cite>n_layers_per_step=1</cite>. The default
value is 1.</p></li>
<li><p><strong>last_layer_weight</strong>  The weight to use for the loss of the
final layer. Increase this to focus more on the
performance when using all layers. The default value is
1.0.</p></li>
<li><p><strong>prior_layers_weight</strong>  The weight to use for the loss of the
prior layers. Increase this to focus more on the
performance when using fewer layers. The default value
is 1.0.</p></li>
<li><p><strong>kl_div_weight</strong>  The weight to use for the KL-divergence loss
that is used to make the prior layers match that of the
last layer. Increase this to focus more on the
performance when using fewer layers. The default value
is 1.0.</p></li>
<li><p><strong>kl_temperature</strong>  The temperature to use for the KL-divergence
loss. If 0, then the KL-divergence loss is not used. The
default value is 1.0.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">References</p>
<ul class="simple">
<li><p>The concept was inspired by the 2DMSE paper: <a class="reference external" href="https://arxiv.org/abs/2402.14776">https://arxiv.org/abs/2402.14776</a></p></li>
<li><p><a class="reference external" href="../../../examples/sentence_transformer/training/adaptive_layer/README.html">Adaptive Layers</a></p></li>
</ul>
<dl>
<dt>Requirements:</dt><dd><ol class="arabic simple">
<li><p>The base loss cannot be <a class="reference internal" href="#sentence_transformers.losses.CachedMultipleNegativesRankingLoss" title="sentence_transformers.losses.CachedMultipleNegativesRankingLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">CachedMultipleNegativesRankingLoss</span></code></a>,
<a class="reference internal" href="#sentence_transformers.losses.CachedMultipleNegativesSymmetricRankingLoss" title="sentence_transformers.losses.CachedMultipleNegativesSymmetricRankingLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">CachedMultipleNegativesSymmetricRankingLoss</span></code></a>, or <a class="reference internal" href="#sentence_transformers.losses.CachedGISTEmbedLoss" title="sentence_transformers.losses.CachedGISTEmbedLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">CachedGISTEmbedLoss</span></code></a>.</p></li>
</ol>
</dd>
<dt>Inputs:</dt><dd><table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Texts</p></th>
<th class="head"><p>Labels</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>any</p></td>
<td><p>any</p></td>
</tr>
</tbody>
</table>
</dd>
<dt>Relations:</dt><dd><ul class="simple">
<li><dl class="simple">
<dt><a class="reference internal" href="#sentence_transformers.losses.Matryoshka2dLoss" title="sentence_transformers.losses.Matryoshka2dLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">Matryoshka2dLoss</span></code></a> uses this loss in combination with <a class="reference internal" href="#sentence_transformers.losses.MatryoshkaLoss" title="sentence_transformers.losses.MatryoshkaLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">MatryoshkaLoss</span></code></a> which allows for</dt><dd><p>output dimensionality reduction for faster downstream tasks (e.g. retrieval).</p>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span><span class="p">,</span> <span class="n">SentenceTransformerTrainer</span><span class="p">,</span> <span class="n">losses</span>
<span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">Dataset</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="s2">&quot;microsoft/mpnet-base&quot;</span><span class="p">)</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">Dataset</span><span class="o">.</span><span class="n">from_dict</span><span class="p">({</span>
    <span class="s2">&quot;anchor&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;It&#39;s nice weather outside today.&quot;</span><span class="p">,</span> <span class="s2">&quot;He drove to work.&quot;</span><span class="p">],</span>
    <span class="s2">&quot;positive&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;It&#39;s so sunny.&quot;</span><span class="p">,</span> <span class="s2">&quot;He took the car to the office.&quot;</span><span class="p">],</span>
<span class="p">})</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">MultipleNegativesRankingLoss</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">AdaptiveLayerLoss</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">SentenceTransformerTrainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="megabatchmarginloss">
<h2>MegaBatchMarginLoss<a class="headerlink" href="#megabatchmarginloss" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="sentence_transformers.losses.MegaBatchMarginLoss">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sentence_transformers.losses.</span></span><span class="sig-name descname"><span class="pre">MegaBatchMarginLoss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="SentenceTransformer.html#sentence_transformers.SentenceTransformer" title="sentence_transformers.SentenceTransformer"><span class="pre">SentenceTransformer</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">positive_margin</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">negative_margin</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_mini_batched_version</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mini_batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">50</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers\losses\MegaBatchMarginLoss.py#L12-L180"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sentence_transformers.losses.MegaBatchMarginLoss" title="Link to this definition"></a></dt>
<dd><p>Given a large batch (like 500 or more examples) of (anchor_i, positive_i) pairs, find for each pair in the batch
the hardest negative, i.e. find j != i such that cos_sim(anchor_i, positive_j) is maximal. Then create from this a
triplet (anchor_i, positive_i, positive_j) where positive_j serves as the negative for this triplet.</p>
<p>Then train as with the triplet loss.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong>  SentenceTransformerModel</p></li>
<li><p><strong>positive_margin</strong>  Positive margin, cos(anchor, positive)
should be &gt; positive_margin</p></li>
<li><p><strong>negative_margin</strong>  Negative margin, cos(anchor, negative)
should be &lt; negative_margin</p></li>
<li><p><strong>use_mini_batched_version</strong>  As large batch sizes require a lot
of memory, we can use a mini-batched version. We break
down the large batch into smaller batches with fewer
examples.</p></li>
<li><p><strong>mini_batch_size</strong>  Size for the mini-batches. Should be a
divisor for the batch size in your data loader.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">References</p>
<ul class="simple">
<li><p>This loss function was inspired by the ParaNMT paper: <a class="reference external" href="https://www.aclweb.org/anthology/P18-1042/">https://www.aclweb.org/anthology/P18-1042/</a></p></li>
</ul>
<dl>
<dt>Requirements:</dt><dd><ol class="arabic simple">
<li><p>(anchor, positive) pairs</p></li>
<li><p>Large batches (500 or more examples)</p></li>
</ol>
</dd>
<dt>Inputs:</dt><dd><table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Texts</p></th>
<th class="head"><p>Labels</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>(anchor, positive) pairs</p></td>
<td><p>none</p></td>
</tr>
</tbody>
</table>
</dd>
<dt>Recommendations:</dt><dd><ul class="simple">
<li><p>Use <code class="docutils literal notranslate"><span class="pre">BatchSamplers.NO_DUPLICATES</span></code> (<a class="reference internal" href="sampler.html#sentence_transformers.training_args.BatchSamplers" title="sentence_transformers.training_args.BatchSamplers"><code class="xref py py-class docutils literal notranslate"><span class="pre">docs</span></code></a>) to
ensure that no in-batch negatives are duplicates of the anchor or positive samples.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span><span class="p">,</span> <span class="n">SentenceTransformerTrainingArguments</span><span class="p">,</span> <span class="n">SentenceTransformerTrainer</span><span class="p">,</span> <span class="n">losses</span>
<span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">Dataset</span>

<span class="n">train_batch_size</span> <span class="o">=</span> <span class="mi">250</span>
<span class="n">train_mini_batch_size</span> <span class="o">=</span> <span class="mi">32</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="s1">&#39;all-MiniLM-L6-v2&#39;</span><span class="p">)</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">Dataset</span><span class="o">.</span><span class="n">from_dict</span><span class="p">({</span>
    <span class="s2">&quot;anchor&quot;</span><span class="p">:</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;This is sentence number </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">)],</span>
    <span class="s2">&quot;positive&quot;</span><span class="p">:</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;This is sentence number </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">501</span><span class="p">)],</span>
<span class="p">})</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">MegaBatchMarginLoss</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">mini_batch_size</span><span class="o">=</span><span class="n">train_mini_batch_size</span><span class="p">)</span>

<span class="n">args</span> <span class="o">=</span> <span class="n">SentenceTransformerTrainingArguments</span><span class="p">(</span>
    <span class="n">output_dir</span><span class="o">=</span><span class="s2">&quot;output&quot;</span><span class="p">,</span>
    <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="n">train_batch_size</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">SentenceTransformerTrainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">args</span><span class="o">=</span><span class="n">args</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="multiplenegativesrankingloss">
<h2>MultipleNegativesRankingLoss<a class="headerlink" href="#multiplenegativesrankingloss" title="Link to this heading"></a></h2>
<p><em>MultipleNegativesRankingLoss</em> is a great loss function if you only have positive pairs, for example, only pairs of similar texts like pairs of paraphrases, pairs of duplicate questions, pairs of (query, response), or pairs of (source_language, target_language).</p>
<dl class="py class">
<dt class="sig sig-object py" id="sentence_transformers.losses.MultipleNegativesRankingLoss">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sentence_transformers.losses.</span></span><span class="sig-name descname"><span class="pre">MultipleNegativesRankingLoss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model:</span> <span class="pre">~sentence_transformers.SentenceTransformer.SentenceTransformer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">20.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">similarity_fct=&lt;function</span> <span class="pre">cos_sim&gt;</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers\losses\MultipleNegativesRankingLoss.py#L13-L145"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sentence_transformers.losses.MultipleNegativesRankingLoss" title="Link to this definition"></a></dt>
<dd><p>Given a list of (anchor, positive) pairs or (anchor, positive, negative) triplets, this loss optimizes the following:</p>
<ol class="arabic simple">
<li><p>Given an anchor (e.g. a question), assign the highest similarity to the corresponding positive (i.e. answer)
out of every single positive and negative (e.g. all answers) in the batch.</p></li>
</ol>
<p>If you provide the optional negatives, they will all be used as extra options from which the model must pick the
correct positive. Within reason, the harder this picking is, the stronger the model will become. Because of
this, a higher batch size results in more in-batch negatives, which then increases performance (to a point).</p>
<p>This loss function works great to train embeddings for retrieval setups where you have positive pairs
(e.g. (query, answer)) as it will sample in each batch <code class="docutils literal notranslate"><span class="pre">n-1</span></code> negative docs randomly.</p>
<p>This loss is also known as InfoNCE loss, SimCSE loss, Cross-Entropy Loss with in-batch negatives, or simply
in-batch negatives loss.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong>  SentenceTransformer model</p></li>
<li><p><strong>scale</strong>  Output of similarity function is multiplied by scale
value</p></li>
<li><p><strong>similarity_fct</strong>  similarity function between sentence
embeddings. By default, cos_sim. Can also be set to dot
product (and then set scale to 1)</p></li>
</ul>
</dd>
</dl>
<p class="rubric">References</p>
<ul class="simple">
<li><p>Efficient Natural Language Response Suggestion for Smart Reply, Section 4.4: <a class="reference external" href="https://arxiv.org/pdf/1705.00652.pdf">https://arxiv.org/pdf/1705.00652.pdf</a></p></li>
<li><p><a class="reference external" href="../../../examples/sentence_transformer/training/nli/README.html">Training Examples &gt; Natural Language Inference</a></p></li>
<li><p><a class="reference external" href="../../../examples/sentence_transformer/training/paraphrases/README.html">Training Examples &gt; Paraphrase Data</a></p></li>
<li><p><a class="reference external" href="../../../examples/sentence_transformer/training/quora_duplicate_questions/README.html">Training Examples &gt; Quora Duplicate Questions</a></p></li>
<li><p><a class="reference external" href="../../../examples/sentence_transformer/training/ms_marco/README.html">Training Examples &gt; MS MARCO</a></p></li>
<li><p><a class="reference external" href="../../../examples/sentence_transformer/unsupervised_learning/SimCSE/README.html">Unsupervised Learning &gt; SimCSE</a></p></li>
<li><p><a class="reference external" href="../../../examples/sentence_transformer/unsupervised_learning/query_generation/README.html">Unsupervised Learning &gt; GenQ</a></p></li>
</ul>
<dl>
<dt>Requirements:</dt><dd><ol class="arabic simple">
<li><p>(anchor, positive) pairs or (anchor, positive, negative) triplets</p></li>
</ol>
</dd>
<dt>Inputs:</dt><dd><table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Texts</p></th>
<th class="head"><p>Labels</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>(anchor, positive) pairs</p></td>
<td><p>none</p></td>
</tr>
<tr class="row-odd"><td><p>(anchor, positive, negative) triplets</p></td>
<td><p>none</p></td>
</tr>
<tr class="row-even"><td><p>(anchor, positive, negative_1, , negative_n)</p></td>
<td><p>none</p></td>
</tr>
</tbody>
</table>
</dd>
<dt>Recommendations:</dt><dd><ul class="simple">
<li><p>Use <code class="docutils literal notranslate"><span class="pre">BatchSamplers.NO_DUPLICATES</span></code> (<a class="reference internal" href="sampler.html#sentence_transformers.training_args.BatchSamplers" title="sentence_transformers.training_args.BatchSamplers"><code class="xref py py-class docutils literal notranslate"><span class="pre">docs</span></code></a>) to
ensure that no in-batch negatives are duplicates of the anchor or positive samples.</p></li>
</ul>
</dd>
<dt>Relations:</dt><dd><ul class="simple">
<li><p><a class="reference internal" href="#sentence_transformers.losses.CachedMultipleNegativesRankingLoss" title="sentence_transformers.losses.CachedMultipleNegativesRankingLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">CachedMultipleNegativesRankingLoss</span></code></a> is equivalent to this loss, but it uses caching that allows for
much higher batch sizes (and thus better performance) without extra memory usage. However, it is slightly
slower.</p></li>
<li><p><a class="reference internal" href="#sentence_transformers.losses.MultipleNegativesSymmetricRankingLoss" title="sentence_transformers.losses.MultipleNegativesSymmetricRankingLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">MultipleNegativesSymmetricRankingLoss</span></code></a> is equivalent to this loss, but with an additional loss term.</p></li>
<li><p><a class="reference internal" href="#sentence_transformers.losses.GISTEmbedLoss" title="sentence_transformers.losses.GISTEmbedLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">GISTEmbedLoss</span></code></a> is equivalent to this loss, but uses a guide model to guide the in-batch negative
sample selection. <cite>GISTEmbedLoss</cite> yields a stronger training signal at the cost of some training overhead.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span><span class="p">,</span> <span class="n">SentenceTransformerTrainer</span><span class="p">,</span> <span class="n">losses</span>
<span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">Dataset</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="s2">&quot;microsoft/mpnet-base&quot;</span><span class="p">)</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">Dataset</span><span class="o">.</span><span class="n">from_dict</span><span class="p">({</span>
    <span class="s2">&quot;anchor&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;It&#39;s nice weather outside today.&quot;</span><span class="p">,</span> <span class="s2">&quot;He drove to work.&quot;</span><span class="p">],</span>
    <span class="s2">&quot;positive&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;It&#39;s so sunny.&quot;</span><span class="p">,</span> <span class="s2">&quot;He took the car to the office.&quot;</span><span class="p">],</span>
<span class="p">})</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">MultipleNegativesRankingLoss</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">SentenceTransformerTrainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="cachedmultiplenegativesrankingloss">
<h2>CachedMultipleNegativesRankingLoss<a class="headerlink" href="#cachedmultiplenegativesrankingloss" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="sentence_transformers.losses.CachedMultipleNegativesRankingLoss">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sentence_transformers.losses.</span></span><span class="sig-name descname"><span class="pre">CachedMultipleNegativesRankingLoss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="pre">model:</span> <span class="pre">SentenceTransformer,</span> <span class="pre">scale:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">20.0,</span> <span class="pre">similarity_fct:</span> <span class="pre">callable[[Tensor,</span> <span class="pre">Tensor],</span> <span class="pre">Tensor]</span> <span class="pre">=</span> <span class="pre">&lt;function</span> <span class="pre">cos_sim&gt;,</span> <span class="pre">mini_batch_size:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">32,</span> <span class="pre">show_progress_bar:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">False</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers\losses\CachedMultipleNegativesRankingLoss.py#L63-L299"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sentence_transformers.losses.CachedMultipleNegativesRankingLoss" title="Link to this definition"></a></dt>
<dd><p>Boosted version of MultipleNegativesRankingLoss (<a class="reference external" href="https://arxiv.org/pdf/1705.00652.pdf">https://arxiv.org/pdf/1705.00652.pdf</a>) by GradCache (<a class="reference external" href="https://arxiv.org/pdf/2101.06983.pdf">https://arxiv.org/pdf/2101.06983.pdf</a>).</p>
<p>Constrastive learning (here our MNRL loss) with in-batch negatives is usually hard to work with large batch sizes due to (GPU) memory limitation.
Even with batch-scaling methods like gradient-scaling, it cannot work either. This is because the in-batch negatives make the data points within
the same batch non-independent and thus the batch cannot be broke down into mini-batches. GradCache is a smart way to solve this problem.
It achieves the goal by dividing the computation into two stages of embedding and loss calculation, which both can be scaled by mini-batches.
As a result, memory of constant size (e.g. that works with batch size = 32) can now process much larger batches (e.g. 65536).</p>
<p>In detail:</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>It first does a quick embedding step without gradients/computation graphs to get all the embeddings;</p></li>
<li><p>Calculate the loss, backward up to the embeddings and cache the gradients wrt. to the embeddings;</p></li>
<li><p>A 2nd embedding step with gradients/computation graphs and connect the cached gradients into the backward chain.</p></li>
</ol>
</div></blockquote>
<p>Notes: All steps are done with mini-batches. In the original implementation of GradCache, (2) is not done in mini-batches and
requires a lot memory when the batch size is large. One drawback is about the speed. Gradient caching will sacrifice
around 20% computation time according to the paper.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong>  SentenceTransformer model</p></li>
<li><p><strong>scale</strong>  Output of similarity function is multiplied by scale value</p></li>
<li><p><strong>similarity_fct</strong>  similarity function between sentence embeddings. By default, cos_sim. Can also be set to dot
product (and then set scale to 1)</p></li>
<li><p><strong>mini_batch_size</strong>  Mini-batch size for the forward pass, this denotes how much memory is actually used during
training and evaluation. The larger the mini-batch size, the more memory efficient the training is, but
the slower the training will be. Its recommended to set it as high as your GPU memory allows. The default
value is 32.</p></li>
<li><p><strong>show_progress_bar</strong>  If True, a progress bar for the mini-batches is shown during training. The default is False.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">References</p>
<ul class="simple">
<li><p>Efficient Natural Language Response Suggestion for Smart Reply, Section 4.4: <a class="reference external" href="https://arxiv.org/pdf/1705.00652.pdf">https://arxiv.org/pdf/1705.00652.pdf</a></p></li>
<li><p>Scaling Deep Contrastive Learning Batch Size under Memory Limited Setup: <a class="reference external" href="https://arxiv.org/pdf/2101.06983.pdf">https://arxiv.org/pdf/2101.06983.pdf</a></p></li>
</ul>
<dl>
<dt>Requirements:</dt><dd><ol class="arabic simple">
<li><p>(anchor, positive) pairs or (anchor, positive, negative pairs)</p></li>
<li><p>Should be used with large <cite>per_device_train_batch_size</cite> and low <cite>mini_batch_size</cite> for superior performance, but slower training time than <a class="reference internal" href="#sentence_transformers.losses.MultipleNegativesRankingLoss" title="sentence_transformers.losses.MultipleNegativesRankingLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">MultipleNegativesRankingLoss</span></code></a>.</p></li>
</ol>
</dd>
<dt>Inputs:</dt><dd><table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Texts</p></th>
<th class="head"><p>Labels</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>(anchor, positive) pairs</p></td>
<td><p>none</p></td>
</tr>
<tr class="row-odd"><td><p>(anchor, positive, negative) triplets</p></td>
<td><p>none</p></td>
</tr>
<tr class="row-even"><td><p>(anchor, positive, negative_1, , negative_n)</p></td>
<td><p>none</p></td>
</tr>
</tbody>
</table>
</dd>
<dt>Recommendations:</dt><dd><ul class="simple">
<li><p>Use <code class="docutils literal notranslate"><span class="pre">BatchSamplers.NO_DUPLICATES</span></code> (<a class="reference internal" href="sampler.html#sentence_transformers.training_args.BatchSamplers" title="sentence_transformers.training_args.BatchSamplers"><code class="xref py py-class docutils literal notranslate"><span class="pre">docs</span></code></a>) to
ensure that no in-batch negatives are duplicates of the anchor or positive samples.</p></li>
</ul>
</dd>
<dt>Relations:</dt><dd><ul class="simple">
<li><p>Equivalent to <a class="reference internal" href="#sentence_transformers.losses.MultipleNegativesRankingLoss" title="sentence_transformers.losses.MultipleNegativesRankingLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">MultipleNegativesRankingLoss</span></code></a>, but with caching that allows for much higher batch sizes
(and thus better performance) without extra memory usage. This loss also trains roughly 2x to 2.4x slower than
<a class="reference internal" href="#sentence_transformers.losses.MultipleNegativesRankingLoss" title="sentence_transformers.losses.MultipleNegativesRankingLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">MultipleNegativesRankingLoss</span></code></a>.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span><span class="p">,</span> <span class="n">SentenceTransformerTrainer</span><span class="p">,</span> <span class="n">losses</span>
<span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">Dataset</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="s2">&quot;microsoft/mpnet-base&quot;</span><span class="p">)</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">Dataset</span><span class="o">.</span><span class="n">from_dict</span><span class="p">({</span>
    <span class="s2">&quot;anchor&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;It&#39;s nice weather outside today.&quot;</span><span class="p">,</span> <span class="s2">&quot;He drove to work.&quot;</span><span class="p">],</span>
    <span class="s2">&quot;positive&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;It&#39;s so sunny.&quot;</span><span class="p">,</span> <span class="s2">&quot;He took the car to the office.&quot;</span><span class="p">],</span>
<span class="p">})</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">CachedMultipleNegativesRankingLoss</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">mini_batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">)</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">SentenceTransformerTrainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="multiplenegativessymmetricrankingloss">
<h2>MultipleNegativesSymmetricRankingLoss<a class="headerlink" href="#multiplenegativessymmetricrankingloss" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="sentence_transformers.losses.MultipleNegativesSymmetricRankingLoss">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sentence_transformers.losses.</span></span><span class="sig-name descname"><span class="pre">MultipleNegativesSymmetricRankingLoss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model:</span> <span class="pre">~sentence_transformers.SentenceTransformer.SentenceTransformer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">20.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">similarity_fct=&lt;function</span> <span class="pre">cos_sim&gt;</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers\losses\MultipleNegativesSymmetricRankingLoss.py#L13-L98"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sentence_transformers.losses.MultipleNegativesSymmetricRankingLoss" title="Link to this definition"></a></dt>
<dd><p>Given a list of (anchor, positive) pairs, this loss sums the following two losses:</p>
<ol class="arabic simple">
<li><p>Forward loss: Given an anchor, find the sample with the highest similarity out of all positives in the batch.
This is equivalent to <a class="reference internal" href="#sentence_transformers.losses.MultipleNegativesRankingLoss" title="sentence_transformers.losses.MultipleNegativesRankingLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">MultipleNegativesRankingLoss</span></code></a>.</p></li>
<li><p>Backward loss: Given a positive, find the sample with the highest similarity out of all anchors in the batch.</p></li>
</ol>
<p>For example with question-answer pairs, <a class="reference internal" href="#sentence_transformers.losses.MultipleNegativesRankingLoss" title="sentence_transformers.losses.MultipleNegativesRankingLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">MultipleNegativesRankingLoss</span></code></a> just computes the loss to find
the answer given a question, but <a class="reference internal" href="#sentence_transformers.losses.MultipleNegativesSymmetricRankingLoss" title="sentence_transformers.losses.MultipleNegativesSymmetricRankingLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">MultipleNegativesSymmetricRankingLoss</span></code></a> additionally computes the
loss to find the question given an answer.</p>
<p>Note: If you pass triplets, the negative entry will be ignored. A anchor is just searched for the positive.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong>  SentenceTransformer model</p></li>
<li><p><strong>scale</strong>  Output of similarity function is multiplied by scale
value</p></li>
<li><p><strong>similarity_fct</strong>  similarity function between sentence
embeddings. By default, cos_sim. Can also be set to dot
product (and then set scale to 1)</p></li>
</ul>
</dd>
</dl>
<dl>
<dt>Requirements:</dt><dd><ol class="arabic simple">
<li><p>(anchor, positive) pairs</p></li>
</ol>
</dd>
<dt>Inputs:</dt><dd><table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Texts</p></th>
<th class="head"><p>Labels</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>(anchor, positive) pairs</p></td>
<td><p>none</p></td>
</tr>
</tbody>
</table>
</dd>
<dt>Recommendations:</dt><dd><ul class="simple">
<li><p>Use <code class="docutils literal notranslate"><span class="pre">BatchSamplers.NO_DUPLICATES</span></code> (<a class="reference internal" href="sampler.html#sentence_transformers.training_args.BatchSamplers" title="sentence_transformers.training_args.BatchSamplers"><code class="xref py py-class docutils literal notranslate"><span class="pre">docs</span></code></a>) to
ensure that no in-batch negatives are duplicates of the anchor or positive samples.</p></li>
</ul>
</dd>
<dt>Relations:</dt><dd><ul class="simple">
<li><p>Like <a class="reference internal" href="#sentence_transformers.losses.MultipleNegativesRankingLoss" title="sentence_transformers.losses.MultipleNegativesRankingLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">MultipleNegativesRankingLoss</span></code></a>, but with an additional loss term.</p></li>
<li><p><a class="reference internal" href="#sentence_transformers.losses.CachedMultipleNegativesSymmetricRankingLoss" title="sentence_transformers.losses.CachedMultipleNegativesSymmetricRankingLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">CachedMultipleNegativesSymmetricRankingLoss</span></code></a> is equivalent to this loss, but it uses caching that
allows for much higher batch sizes (and thus better performance) without extra memory usage. However, it
is slightly slower.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span><span class="p">,</span> <span class="n">SentenceTransformerTrainer</span><span class="p">,</span> <span class="n">losses</span>
<span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">Dataset</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="s2">&quot;microsoft/mpnet-base&quot;</span><span class="p">)</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">Dataset</span><span class="o">.</span><span class="n">from_dict</span><span class="p">({</span>
    <span class="s2">&quot;anchor&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;It&#39;s nice weather outside today.&quot;</span><span class="p">,</span> <span class="s2">&quot;He drove to work.&quot;</span><span class="p">],</span>
    <span class="s2">&quot;positive&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;It&#39;s so sunny.&quot;</span><span class="p">,</span> <span class="s2">&quot;He took the car to the office.&quot;</span><span class="p">],</span>
<span class="p">})</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">MultipleNegativesSymmetricRankingLoss</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">SentenceTransformerTrainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="cachedmultiplenegativessymmetricrankingloss">
<h2>CachedMultipleNegativesSymmetricRankingLoss<a class="headerlink" href="#cachedmultiplenegativessymmetricrankingloss" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="sentence_transformers.losses.CachedMultipleNegativesSymmetricRankingLoss">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sentence_transformers.losses.</span></span><span class="sig-name descname"><span class="pre">CachedMultipleNegativesSymmetricRankingLoss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="pre">model:</span> <span class="pre">SentenceTransformer,</span> <span class="pre">scale:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">20.0,</span> <span class="pre">similarity_fct:</span> <span class="pre">callable[[Tensor,</span> <span class="pre">Tensor],</span> <span class="pre">Tensor]</span> <span class="pre">=</span> <span class="pre">&lt;function</span> <span class="pre">cos_sim&gt;,</span> <span class="pre">mini_batch_size:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">32,</span> <span class="pre">show_progress_bar:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">False</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers\losses\CachedMultipleNegativesSymmetricRankingLoss.py#L40-L255"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sentence_transformers.losses.CachedMultipleNegativesSymmetricRankingLoss" title="Link to this definition"></a></dt>
<dd><p>Boosted version of <a class="reference internal" href="#sentence_transformers.losses.MultipleNegativesSymmetricRankingLoss" title="sentence_transformers.losses.MultipleNegativesSymmetricRankingLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">MultipleNegativesSymmetricRankingLoss</span></code></a> (MNSRL) by GradCache (<a class="reference external" href="https://arxiv.org/pdf/2101.06983.pdf">https://arxiv.org/pdf/2101.06983.pdf</a>).</p>
<p>Given a list of (anchor, positive) pairs, MNSRL sums the following two losses:</p>
<ol class="arabic simple">
<li><p>Forward loss: Given an anchor, find the sample with the highest similarity out of all positives in the batch.</p></li>
<li><p>Backward loss: Given a positive, find the sample with the highest similarity out of all anchors in the batch.</p></li>
</ol>
<p>For example with question-answer pairs, the forward loss finds the answer for a given question and the backward loss
finds the question for a given answer. This loss is common in symmetric tasks, such as semantic textual similarity.</p>
<p>The caching modification allows for large batch sizes (which give a better training signal) with constant memory usage,
allowing you to reach optimal training signal with regular hardware.</p>
<p>Note: If you pass triplets, the negative entry will be ignored. An anchor is just searched for the positive.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong>  SentenceTransformer model</p></li>
<li><p><strong>scale</strong>  Output of similarity function is multiplied by scale value</p></li>
<li><p><strong>similarity_fct</strong>  similarity function between sentence embeddings. By default, cos_sim.
Can also be set to dot product (and then set scale to 1)</p></li>
<li><p><strong>mini_batch_size</strong>  Mini-batch size for the forward pass, this denotes how much memory is actually used during
training and evaluation. The larger the mini-batch size, the more memory efficient the training is, but
the slower the training will be.</p></li>
<li><p><strong>show_progress_bar</strong>  If True, shows progress bar during processing</p></li>
</ul>
</dd>
</dl>
<dl>
<dt>Requirements:</dt><dd><ol class="arabic simple">
<li><p>(anchor, positive) pairs</p></li>
<li><p>Should be used with large batch sizes for superior performance, but has slower training time than non-cached versions</p></li>
</ol>
</dd>
<dt>Inputs:</dt><dd><table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Texts</p></th>
<th class="head"><p>Labels</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>(anchor, positive) pairs</p></td>
<td><p>none</p></td>
</tr>
</tbody>
</table>
</dd>
<dt>Recommendations:</dt><dd><ul class="simple">
<li><p>Use <code class="docutils literal notranslate"><span class="pre">BatchSamplers.NO_DUPLICATES</span></code> (<a class="reference internal" href="sampler.html#sentence_transformers.training_args.BatchSamplers" title="sentence_transformers.training_args.BatchSamplers"><code class="xref py py-class docutils literal notranslate"><span class="pre">docs</span></code></a>) to
ensure that no in-batch negatives are duplicates of the anchor or positive samples.</p></li>
</ul>
</dd>
<dt>Relations:</dt><dd><ul class="simple">
<li><p>Like <a class="reference internal" href="#sentence_transformers.losses.MultipleNegativesRankingLoss" title="sentence_transformers.losses.MultipleNegativesRankingLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">MultipleNegativesRankingLoss</span></code></a>, but with an additional symmetric loss term and caching mechanism.</p></li>
<li><p>Inspired by <a class="reference internal" href="#sentence_transformers.losses.CachedMultipleNegativesRankingLoss" title="sentence_transformers.losses.CachedMultipleNegativesRankingLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">CachedMultipleNegativesRankingLoss</span></code></a>, adapted for symmetric loss calculation.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span><span class="p">,</span> <span class="n">SentenceTransformerTrainer</span><span class="p">,</span> <span class="n">losses</span>
<span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">Dataset</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="s2">&quot;microsoft/mpnet-base&quot;</span><span class="p">)</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">Dataset</span><span class="o">.</span><span class="n">from_dict</span><span class="p">({</span>
    <span class="s2">&quot;anchor&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;It&#39;s nice weather outside today.&quot;</span><span class="p">,</span> <span class="s2">&quot;He drove to work.&quot;</span><span class="p">],</span>
    <span class="s2">&quot;positive&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;It&#39;s so sunny.&quot;</span><span class="p">,</span> <span class="s2">&quot;He took the car to the office.&quot;</span><span class="p">],</span>
<span class="p">})</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">CachedMultipleNegativesSymmetricRankingLoss</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">mini_batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">SentenceTransformerTrainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>
</div>
<p class="rubric">References</p>
<ul class="simple">
<li><p>Efficient Natural Language Response Suggestion for Smart Reply, Section 4.4: <a class="reference external" href="https://arxiv.org/pdf/1705.00652.pdf">https://arxiv.org/pdf/1705.00652.pdf</a></p></li>
<li><p>Scaling Deep Contrastive Learning Batch Size under Memory Limited Setup: <a class="reference external" href="https://arxiv.org/pdf/2101.06983.pdf">https://arxiv.org/pdf/2101.06983.pdf</a></p></li>
</ul>
</dd></dl>

</section>
<section id="softmaxloss">
<h2>SoftmaxLoss<a class="headerlink" href="#softmaxloss" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="sentence_transformers.losses.SoftmaxLoss">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sentence_transformers.losses.</span></span><span class="sig-name descname"><span class="pre">SoftmaxLoss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="SentenceTransformer.html#sentence_transformers.SentenceTransformer" title="sentence_transformers.SentenceTransformer.SentenceTransformer"><span class="pre">SentenceTransformer</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">sentence_embedding_dimension</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_labels</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">concatenation_sent_rep</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">concatenation_sent_difference</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">concatenation_sent_multiplication</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_fct</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Callable</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">CrossEntropyLoss()</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers\losses\SoftmaxLoss.py#L17-L155"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sentence_transformers.losses.SoftmaxLoss" title="Link to this definition"></a></dt>
<dd><p>This loss was used in our SBERT publication (<a class="reference external" href="https://arxiv.org/abs/1908.10084">https://arxiv.org/abs/1908.10084</a>) to train the SentenceTransformer
model on NLI data. It adds a softmax classifier on top of the output of two transformer networks.</p>
<p><a class="reference internal" href="#sentence_transformers.losses.MultipleNegativesRankingLoss" title="sentence_transformers.losses.MultipleNegativesRankingLoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">MultipleNegativesRankingLoss</span></code></a> is an alternative loss function that often yields better results,
as per <a class="reference external" href="https://arxiv.org/abs/2004.09813">https://arxiv.org/abs/2004.09813</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<a class="reference internal" href="SentenceTransformer.html#sentence_transformers.SentenceTransformer" title="sentence_transformers.SentenceTransformer"><em>SentenceTransformer</em></a>)  The SentenceTransformer model.</p></li>
<li><p><strong>sentence_embedding_dimension</strong> (<em>int</em>)  The dimension of the sentence embeddings.</p></li>
<li><p><strong>num_labels</strong> (<em>int</em>)  The number of different labels.</p></li>
<li><p><strong>concatenation_sent_rep</strong> (<em>bool</em>)  Whether to concatenate vectors u,v for the softmax classifier. Defaults to True.</p></li>
<li><p><strong>concatenation_sent_difference</strong> (<em>bool</em>)  Whether to add abs(u-v) for the softmax classifier. Defaults to True.</p></li>
<li><p><strong>concatenation_sent_multiplication</strong> (<em>bool</em>)  Whether to add u*v for the softmax classifier. Defaults to False.</p></li>
<li><p><strong>loss_fct</strong> (<em>Callable</em>)  Custom pytorch loss function. If not set, uses nn.CrossEntropyLoss(). Defaults to nn.CrossEntropyLoss().</p></li>
</ul>
</dd>
</dl>
<p class="rubric">References</p>
<ul class="simple">
<li><p>Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks: <a class="reference external" href="https://arxiv.org/abs/1908.10084">https://arxiv.org/abs/1908.10084</a></p></li>
<li><p><a class="reference external" href="../../../examples/sentence_transformer/training/nli/README.html">Training Examples &gt; Natural Language Inference</a></p></li>
</ul>
<dl>
<dt>Requirements:</dt><dd><ol class="arabic simple">
<li><p>sentence pairs with a class label</p></li>
</ol>
</dd>
<dt>Inputs:</dt><dd><table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Texts</p></th>
<th class="head"><p>Labels</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>(sentence_A, sentence_B) pairs</p></td>
<td><p>class</p></td>
</tr>
</tbody>
</table>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span><span class="p">,</span> <span class="n">SentenceTransformerTrainer</span><span class="p">,</span> <span class="n">losses</span>
<span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">Dataset</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="s2">&quot;microsoft/mpnet-base&quot;</span><span class="p">)</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">Dataset</span><span class="o">.</span><span class="n">from_dict</span><span class="p">({</span>
    <span class="s2">&quot;sentence1&quot;</span><span class="p">:</span> <span class="p">[</span>
        <span class="s2">&quot;A person on a horse jumps over a broken down airplane.&quot;</span><span class="p">,</span>
        <span class="s2">&quot;A person on a horse jumps over a broken down airplane.&quot;</span><span class="p">,</span>
        <span class="s2">&quot;A person on a horse jumps over a broken down airplane.&quot;</span><span class="p">,</span>
        <span class="s2">&quot;Children smiling and waving at camera&quot;</span><span class="p">,</span>
    <span class="p">],</span>
    <span class="s2">&quot;sentence2&quot;</span><span class="p">:</span> <span class="p">[</span>
        <span class="s2">&quot;A person is training his horse for a competition.&quot;</span><span class="p">,</span>
        <span class="s2">&quot;A person is at a diner, ordering an omelette.&quot;</span><span class="p">,</span>
        <span class="s2">&quot;A person is outdoors, on a horse.&quot;</span><span class="p">,</span>
        <span class="s2">&quot;There are children present.&quot;</span><span class="p">,</span>
    <span class="p">],</span>
    <span class="s2">&quot;label&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
<span class="p">})</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">SoftmaxLoss</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">get_sentence_embedding_dimension</span><span class="p">(),</span> <span class="n">num_labels</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">SentenceTransformerTrainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="tripletloss">
<h2>TripletLoss<a class="headerlink" href="#tripletloss" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="sentence_transformers.losses.TripletLoss">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sentence_transformers.losses.</span></span><span class="sig-name descname"><span class="pre">TripletLoss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model:</span> <span class="pre">~sentence_transformers.SentenceTransformer.SentenceTransformer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">distance_metric=&lt;function</span> <span class="pre">TripletDistanceMetric.&lt;lambda&gt;&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">triplet_margin:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">5</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers\losses\TripletLoss.py#L22-L124"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sentence_transformers.losses.TripletLoss" title="Link to this definition"></a></dt>
<dd><p>This class implements triplet loss. Given a triplet of (anchor, positive, negative),
the loss minimizes the distance between anchor and positive while it maximizes the distance
between anchor and negative. It compute the following loss function:</p>
<p><code class="docutils literal notranslate"><span class="pre">loss</span> <span class="pre">=</span> <span class="pre">max(||anchor</span> <span class="pre">-</span> <span class="pre">positive||</span> <span class="pre">-</span> <span class="pre">||anchor</span> <span class="pre">-</span> <span class="pre">negative||</span> <span class="pre">+</span> <span class="pre">margin,</span> <span class="pre">0)</span></code>.</p>
<p>Margin is an important hyperparameter and needs to be tuned respectively.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong>  SentenceTransformerModel</p></li>
<li><p><strong>distance_metric</strong>  Function to compute distance between two
embeddings. The class TripletDistanceMetric contains
common distance metrices that can be used.</p></li>
<li><p><strong>triplet_margin</strong>  The negative should be at least this much
further away from the anchor than the positive.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">References</p>
<ul class="simple">
<li><p>For further details, see: <a class="reference external" href="https://en.wikipedia.org/wiki/Triplet_loss">https://en.wikipedia.org/wiki/Triplet_loss</a></p></li>
</ul>
<dl>
<dt>Requirements:</dt><dd><ol class="arabic simple">
<li><p>(anchor, positive, negative) triplets</p></li>
</ol>
</dd>
<dt>Inputs:</dt><dd><table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Texts</p></th>
<th class="head"><p>Labels</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>(anchor, positive, negative) triplets</p></td>
<td><p>none</p></td>
</tr>
</tbody>
</table>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span><span class="p">,</span> <span class="n">SentenceTransformerTrainer</span><span class="p">,</span> <span class="n">losses</span>
<span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">Dataset</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="s2">&quot;microsoft/mpnet-base&quot;</span><span class="p">)</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">Dataset</span><span class="o">.</span><span class="n">from_dict</span><span class="p">({</span>
    <span class="s2">&quot;anchor&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;It&#39;s nice weather outside today.&quot;</span><span class="p">,</span> <span class="s2">&quot;He drove to work.&quot;</span><span class="p">],</span>
    <span class="s2">&quot;positive&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;It&#39;s so sunny.&quot;</span><span class="p">,</span> <span class="s2">&quot;He took the car to the office.&quot;</span><span class="p">],</span>
    <span class="s2">&quot;negative&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;It&#39;s quite rainy, sadly.&quot;</span><span class="p">,</span> <span class="s2">&quot;She walked to the store.&quot;</span><span class="p">],</span>
<span class="p">})</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">TripletLoss</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">)</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">SentenceTransformerTrainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</section>
<section id="distillkldivloss">
<h2>DistillKLDivLoss<a class="headerlink" href="#distillkldivloss" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="sentence_transformers.losses.DistillKLDivLoss">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sentence_transformers.losses.</span></span><span class="sig-name descname"><span class="pre">DistillKLDivLoss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model:</span> <span class="pre">SentenceTransformer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">similarity_fct=&lt;function</span> <span class="pre">pairwise_dot_score&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">temperature:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">2.0</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers\losses\DistillKLDivLoss.py#L11-L173"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sentence_transformers.losses.DistillKLDivLoss" title="Link to this definition"></a></dt>
<dd><p>Compute the KL divergence loss between probability distributions derived from student and teacher models similarity scores.
By default, similarity is calculated using the dot-product. This loss is designed for knowledge distillation
where a smaller student model learns from a more powerful teacher model.</p>
<p>The loss computes softmax probabilities from the teacher similarity scores and log-softmax probabilities
from the student model, then calculates the KL divergence between these distributions.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong>  SentenceTransformer model (student model)</p></li>
<li><p><strong>similarity_fct</strong>  Which similarity function to use for the student model</p></li>
<li><p><strong>temperature</strong>  Temperature parameter to soften probability distributions (higher temperature = softer distributions)
When combined with other losses, a temperature of 1.0 is also viable. Defaults to 2.0.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">References</p>
<ul class="simple">
<li><p>For more details, please refer to <a class="reference external" href="https://arxiv.org/abs/2010.11386">https://arxiv.org/abs/2010.11386</a></p></li>
</ul>
<dl>
<dt>Requirements:</dt><dd><ol class="arabic simple">
<li><p>(query, positive, negative_1, , negative_n) examples</p></li>
<li><p>Labels containing teacher models scores between query-positive and query-negative pairs</p></li>
</ol>
</dd>
<dt>Inputs:</dt><dd><table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Texts</p></th>
<th class="head"><p>Labels</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>(query, positive, negative)</p></td>
<td><p>[Teacher(query, positive), Teacher(query, negative)]</p></td>
</tr>
<tr class="row-odd"><td><p>(query, positive, negative_1, , negative_n)</p></td>
<td><p>[Teacher(query, positive), Teacher(query, negative_i)]</p></td>
</tr>
</tbody>
</table>
</dd>
<dt>Relations:</dt><dd><ul class="simple">
<li><p>Similar to <a class="reference internal" href="#sentence_transformers.losses.MarginMSELoss" title="sentence_transformers.losses.MarginMSELoss"><code class="xref py py-class docutils literal notranslate"><span class="pre">MarginMSELoss</span></code></a> but uses KL divergence instead of MSE</p></li>
<li><p>More suited for distillation tasks where preserving ranking is important</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Example</p>
<p>Using a teacher model to compute similarity scores for distillation:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span><span class="p">,</span> <span class="n">SentenceTransformerTrainer</span><span class="p">,</span> <span class="n">losses</span>
<span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">Dataset</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="n">student_model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="s2">&quot;microsoft/mpnet-base&quot;</span><span class="p">)</span>
<span class="n">teacher_model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="s2">&quot;all-mpnet-base-v2&quot;</span><span class="p">)</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">Dataset</span><span class="o">.</span><span class="n">from_dict</span><span class="p">({</span>
    <span class="s2">&quot;query&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;It&#39;s nice weather outside today.&quot;</span><span class="p">,</span> <span class="s2">&quot;He drove to work.&quot;</span><span class="p">],</span>
    <span class="s2">&quot;positive&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;It&#39;s so sunny.&quot;</span><span class="p">,</span> <span class="s2">&quot;He took the car to work.&quot;</span><span class="p">],</span>
    <span class="s2">&quot;negative&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;It&#39;s very cold.&quot;</span><span class="p">,</span> <span class="s2">&quot;She walked to the store.&quot;</span><span class="p">],</span>
<span class="p">})</span>

<span class="k">def</span> <span class="nf">compute_labels</span><span class="p">(</span><span class="n">batch</span><span class="p">):</span>
    <span class="n">emb_queries</span> <span class="o">=</span> <span class="n">teacher_model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s2">&quot;query&quot;</span><span class="p">])</span>
    <span class="n">emb_positives</span> <span class="o">=</span> <span class="n">teacher_model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s2">&quot;positive&quot;</span><span class="p">])</span>
    <span class="n">emb_negatives</span> <span class="o">=</span> <span class="n">teacher_model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s2">&quot;negative&quot;</span><span class="p">])</span>

    <span class="n">pos_scores</span> <span class="o">=</span> <span class="n">teacher_model</span><span class="o">.</span><span class="n">similarity_pairwise</span><span class="p">(</span><span class="n">emb_queries</span><span class="p">,</span> <span class="n">emb_positives</span><span class="p">)</span>
    <span class="n">neg_scores</span> <span class="o">=</span> <span class="n">teacher_model</span><span class="o">.</span><span class="n">similarity_pairwise</span><span class="p">(</span><span class="n">emb_queries</span><span class="p">,</span> <span class="n">emb_negatives</span><span class="p">)</span>

    <span class="c1"># Stack the scores for positive and negative pairs</span>
    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;label&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">pos_scores</span><span class="p">,</span> <span class="n">neg_scores</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="p">}</span>

<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">train_dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">compute_labels</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">DistillKLDivLoss</span><span class="p">(</span><span class="n">student_model</span><span class="p">)</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">SentenceTransformerTrainer</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">student_model</span><span class="p">,</span> <span class="n">train_dataset</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>
</div>
<p>With multiple negatives:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span><span class="p">,</span> <span class="n">SentenceTransformerTrainer</span><span class="p">,</span> <span class="n">losses</span>
<span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">Dataset</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="n">student_model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="s2">&quot;microsoft/mpnet-base&quot;</span><span class="p">)</span>
<span class="n">teacher_model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="s2">&quot;all-mpnet-base-v2&quot;</span><span class="p">)</span>

<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">Dataset</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span>
    <span class="p">{</span>
        <span class="s2">&quot;query&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;It&#39;s nice weather outside today.&quot;</span><span class="p">,</span> <span class="s2">&quot;He drove to work.&quot;</span><span class="p">],</span>
        <span class="s2">&quot;positive&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;It&#39;s so sunny.&quot;</span><span class="p">,</span> <span class="s2">&quot;He took the car to work.&quot;</span><span class="p">],</span>
        <span class="s2">&quot;negative1&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;It&#39;s very cold.&quot;</span><span class="p">,</span> <span class="s2">&quot;She walked to the store.&quot;</span><span class="p">],</span>
        <span class="s2">&quot;negative2&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;Its rainy&quot;</span><span class="p">,</span> <span class="s2">&quot;She took the bus&quot;</span><span class="p">],</span>
    <span class="p">}</span>
<span class="p">)</span>


<span class="k">def</span> <span class="nf">compute_labels</span><span class="p">(</span><span class="n">batch</span><span class="p">):</span>
    <span class="n">emb_queries</span> <span class="o">=</span> <span class="n">teacher_model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s2">&quot;query&quot;</span><span class="p">])</span>
    <span class="n">emb_positives</span> <span class="o">=</span> <span class="n">teacher_model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s2">&quot;positive&quot;</span><span class="p">])</span>
    <span class="n">emb_negatives1</span> <span class="o">=</span> <span class="n">teacher_model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s2">&quot;negative1&quot;</span><span class="p">])</span>
    <span class="n">emb_negatives2</span> <span class="o">=</span> <span class="n">teacher_model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s2">&quot;negative2&quot;</span><span class="p">])</span>

    <span class="n">pos_scores</span> <span class="o">=</span> <span class="n">teacher_model</span><span class="o">.</span><span class="n">similarity_pairwise</span><span class="p">(</span><span class="n">emb_queries</span><span class="p">,</span> <span class="n">emb_positives</span><span class="p">)</span>
    <span class="n">neg_scores1</span> <span class="o">=</span> <span class="n">teacher_model</span><span class="o">.</span><span class="n">similarity_pairwise</span><span class="p">(</span><span class="n">emb_queries</span><span class="p">,</span> <span class="n">emb_negatives1</span><span class="p">)</span>
    <span class="n">neg_scores2</span> <span class="o">=</span> <span class="n">teacher_model</span><span class="o">.</span><span class="n">similarity_pairwise</span><span class="p">(</span><span class="n">emb_queries</span><span class="p">,</span> <span class="n">emb_negatives2</span><span class="p">)</span>

    <span class="c1"># Stack the scores for positive and multiple negative pairs</span>
    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;label&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">pos_scores</span><span class="p">,</span> <span class="n">neg_scores1</span><span class="p">,</span> <span class="n">neg_scores2</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="p">}</span>

<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">train_dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">compute_labels</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">DistillKLDivLoss</span><span class="p">(</span><span class="n">student_model</span><span class="p">)</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">SentenceTransformerTrainer</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">student_model</span><span class="p">,</span> <span class="n">train_dataset</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="training_args.html" class="btn btn-neutral float-left" title="Training Arguments" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="sampler.html" class="btn btn-neutral float-right" title="Samplers" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.


</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

</body>
</html>
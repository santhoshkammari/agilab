{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cb9acafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm.sampling_params import GuidedDecodingParams, SamplingParams\n",
    "from pydantic import BaseModel,Field\n",
    "from typing import Literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b13217f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Grammar created: <class 'xgrammar.grammar.Grammar'>\n",
      "   Class: Grammar\n",
      "   Module: xgrammar.grammar\n",
      "--------\n",
      "basic_escape ::= (([\\\"\\\\/bfnrt]) | (\"u\" [A-Fa-f0-9] [A-Fa-f0-9] [A-Fa-f0-9] [A-Fa-f0-9])) (=(basic_string_sub))\n",
      "basic_string_sub ::= ((\"\\\"\") | ([^\\0-\\x1f\\\"\\\\\\r\\n] basic_string_sub) | (\"\\\\\" basic_escape basic_string_sub)) (=([ \\n\\t]* [,}\\]:]))\n",
      "basic_string ::= ((\"\\\"\" basic_string_sub)) (=(root_part_0 [ \\n\\t]* \"}\"))\n",
      "root_prop_1 ::= ((\"\\\"celsius\\\"\") | (\"\\\"fahrenheit\\\"\"))\n",
      "root_part_0 ::= (([ \\n\\t]* \",\" [ \\n\\t]* \"\\\"unit\\\"\" [ \\n\\t]* \":\" [ \\n\\t]* root_prop_1)) (=([ \\n\\t]* \"}\"))\n",
      "root ::= ((\"{\" [ \\n\\t]* \"\\\"location\\\"\" [ \\n\\t]* \":\" [ \\n\\t]* basic_string root_part_0 [ \\n\\t]* \"}\"))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import xgrammar as xgr\n",
    "import json\n",
    "\n",
    "class GetWeatherInput(BaseModel):\n",
    "    \"\"\"Input schema for get_weather function.\"\"\"\n",
    "    location: str \n",
    "    unit: Literal[\"celsius\", \"fahrenheit\"]\n",
    "    \n",
    "grammar = xgr.Grammar.from_json_schema(json.dumps(GetWeatherInput.model_json_schema()))\n",
    "\n",
    "print(f\"\\n✅ Grammar created: {type(grammar)}\")\n",
    "print(f\"   Class: {grammar.__class__.__name__}\")\n",
    "print(f\"   Module: {grammar.__class__.__module__}\")\n",
    "\n",
    "print('--------')\n",
    "print(grammar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f1530e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "integer ::= (\"-\"? integral-part) space\n",
      "integral-part ::= [0-9] | [1-9] ([0-9] ([0-9] ([0-9] ([0-9] ([0-9] ([0-9] ([0-9] ([0-9] ([0-9] ([0-9] ([0-9] ([0-9] ([0-9] ([0-9] ([0-9])?)?)?)?)?)?)?)?)?)?)?)?)?)?)?\n",
      "rating-kv ::= \"\\\"rating\\\"\" space \":\" space integer\n",
      "schema ::= \"{\" space rating-kv \"}\" space\n",
      "space ::= \" \"?\n",
      "thinking ::= ([^<] | \"<\" [^/] | \"</\" [^t] | \"</t\" [^h] | \"</th\" [^i] | \"</thi\" [^n] | \"</thin\" [^k] | \"</think\" [^>])+ \"</think>\"\n",
      "space ::= \" \"?\n",
      "root ::= thinking space schema\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import json\n",
    "\n",
    "import litellm\n",
    "from llama_cpp import LlamaGrammar\n",
    "from pydantic import BaseModel\n",
    "from torch.cuda import temperature\n",
    "\n",
    "\n",
    "class Rating(BaseModel):\n",
    "    rating: int\n",
    "\n",
    "def get_combined_grammar(response_format, eot_token: str = r\"</think>\"):\n",
    "    schema_grammar = LlamaGrammar.from_json_schema(json.dumps(response_format.model_json_schema()))\n",
    "\n",
    "    anything_but_eot_parts = [\n",
    "        (f'\"{eot_token[:i]}\" ' if i > 0 else \"\") + f\"[^{eot_token[i]}]\"\n",
    "        for i in range(len(eot_token))\n",
    "    ]\n",
    "    anything_but_eot = \" | \".join(anything_but_eot_parts)\n",
    "    grammar_string = f'thinking ::= ({anything_but_eot})+ \"{eot_token}\"\\n' \\\n",
    "                     f'space ::= \" \"?\\n' \\\n",
    "                     f'root ::= thinking space schema'\n",
    "    thinking_grammar_str = LlamaGrammar.from_string(grammar_string)._grammar\n",
    "\n",
    "    schema_grammar_str = schema_grammar._grammar.replace(\"root\", \"schema\")\n",
    "    combined_grammar_str = f\"{schema_grammar_str}\\n{thinking_grammar_str}\"\n",
    "    print(combined_grammar_str)\n",
    "    return LlamaGrammar.from_string(combined_grammar_str)._grammar\n",
    "\n",
    "\n",
    "def rate_ai_assistant(example):\n",
    "    prompt = \"This is a description of an AI assistant, rate the quality of the description from 0 (poor) to 5 (clear):\\n\\n```\\n{prompt}\\n```\"\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt.format(prompt=example[\"prompt\"])}]\n",
    "\n",
    "    grammar = get_combined_grammar(Rating)\n",
    "    return grammar\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    rate_ai_assistant({\"prompt\": \"you are an etherium developer\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bebe77",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_grammer = \"\"\"basic_escape ::= (([\\\"\\\\/bfnrt]) | (\"u\" [A-Fa-f0-9] [A-Fa-f0-9] [A-Fa-f0-9] [A-Fa-f0-9])) (=(basic_string_sub))\n",
    "basic_string_sub ::= ((\"\\\"\") | ([^\\0-\\x1f\\\"\\\\\\r\\n] basic_string_sub) | (\"\\\\\" basic_escape basic_string_sub)) (=([ \\n\\t]* [,}\\]:]))\n",
    "basic_string ::= ((\"\\\"\" basic_string_sub)) (=(root_part_0 [ \\n\\t]* \"}\"))\n",
    "root_prop_1 ::= ((\"\\\"celsius\\\"\") | (\"\\\"fahrenheit\\\"\"))\n",
    "root_part_0 ::= (([ \\n\\t]* \",\" [ \\n\\t]* \"\\\"unit\\\"\" [ \\n\\t]* \":\" [ \\n\\t]* root_prop_1)) (=([ \\n\\t]* \"}\"))\n",
    "root ::= ((\"{\" [ \\n\\t]* \"\\\"location\\\"\" [ \\n\\t]* \":\" [ \\n\\t]* basic_string root_part_0 [ \\n\\t]* \"}\"))\"\"\"\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c96093b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntlpt59/main/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer, TextStreamer\n",
    "import time\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77b0c988",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"LiquidAI/LFM2-350M\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"bfloat16\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "9ba01154",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"Gets the current weather for a given city.\n",
    "    \n",
    "    Args:\n",
    "        city: The name of the city to get weather for.\n",
    "        \n",
    "    Returns:\n",
    "        A string describing the current weather conditions.\n",
    "\n",
    "    \"\"\"\n",
    "    return f\"It's 22Â°C and sunny in {city} with 55% humidity and 8 km/h winds.\"\n",
    "\n",
    "def read(path: str) -> str:\n",
    "    \"\"\"Reads a file from the local filesystem. You can access any file directly by using this tool.\n",
    "  Assume this tool is able to read all files on the machine. If the User provides a path to a file assume that path is valid. It is okay to read\n",
    "  a file that does not exist; an error will be returned.\n",
    "\n",
    "    Args:\n",
    "        path: The file path to read from.\n",
    "\n",
    "    Returns:\n",
    "        The file contents as a string.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "    except FileNotFoundError:\n",
    "        return f\"Error: File not found: {path}\"\n",
    "    except IOError as e:\n",
    "        return f\"Error reading file {path}: {e}\"\n",
    "\n",
    "\n",
    "def write_file(path: str, content: str) -> str:\n",
    "    \"\"\"Writes content to a file.\n",
    "\n",
    "    Args:\n",
    "        path: The file path to write to.\n",
    "        content: The content to write to the file.\n",
    "\n",
    "    Returns:\n",
    "        A success or error message.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(path, 'w', encoding='utf-8') as file:\n",
    "            file.write(content)\n",
    "        return f\"Successfully wrote to {path}\"\n",
    "    except IOError as e:\n",
    "        return f\"Error writing to file {path}: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "d87177ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import re\n",
    "\n",
    "def parse_tool_calls(assistant):\n",
    "    \"\"\"Parse tool calls from string format to structured dict.\"\"\"\n",
    "    \n",
    "    if '<|tool_call_start|>' in assistant:\n",
    "        tool_message = assistant.split(\"<|tool_call_end|>\")[0].replace(\"<|tool_call_start|>\",\"\")\n",
    "    else:\n",
    "        tool_message = \"\"\n",
    "        \n",
    "    # Remove the outer brackets if present\n",
    "    tool_message = tool_message.strip()\n",
    "    if tool_message.startswith('[') and tool_message.endswith(']'):\n",
    "        tool_message = tool_message[1:-1]\n",
    "    \n",
    "    # Split multiple function calls\n",
    "    # Simple approach: split by ), and add ) back except for last one\n",
    "    calls = []\n",
    "    \n",
    "    # Find all function calls using regex\n",
    "    pattern = r'(\\w+)\\((.*?)\\)(?=,\\s*\\w+\\(|$)'\n",
    "    matches = re.findall(pattern, tool_message)\n",
    "    \n",
    "    for func_name, args_str in matches:\n",
    "        expr = f\"{func_name}({args_str})\"\n",
    "        tree = ast.parse(expr, mode='eval')\n",
    "        call_node = tree.body\n",
    "        parsed_call = {\n",
    "            'function': call_node.func.id,\n",
    "            'arguments': {}\n",
    "        }\n",
    "        \n",
    "        for keyword in call_node.keywords:\n",
    "            parsed_call['arguments'][keyword.arg] = ast.literal_eval(keyword.value)\n",
    "        \n",
    "        calls.append(parsed_call)\n",
    "    return calls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "4a66c94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(messages,tools=None,max_new_tokens=1024,skip_special_tokens=False):\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tools = tools,\n",
    "        return_tensors=\"pt\",\n",
    "        tokenize=True,\n",
    "    ).to(model.device)\n",
    "\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        do_sample=True,\n",
    "        temperature=0.3,\n",
    "        min_p=0.15,\n",
    "        repetition_penalty=1.05,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "    )\n",
    "\n",
    "    text = tokenizer.decode(output[0], skip_special_tokens=skip_special_tokens)\n",
    "    print(text)\n",
    "    return text.split(\"<|im_start|>assistant\")[-1].strip().replace(\"<|im_end|>\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "381ca37a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|startoftext|><|im_start|>system\n",
      "List of tools: <|tool_list_start|>[{\"type\": \"function\", \"function\": {\"name\": \"read\", \"description\": \"Reads a file from the local filesystem. You can access any file directly by using this tool.\\nAssume this tool is able to read all files on the machine. If the User provides a path to a file assume that path is valid. It is okay to read\\na file that does not exist; an error will be returned.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"path\": {\"type\": \"string\", \"description\": \"The file path to read from.\"}}, \"required\": [\"path\"]}, \"return\": {\"type\": \"string\", \"description\": \"The file contents as a string.\"}}}]<|tool_list_end|><|im_end|>\n",
      "<|im_start|>user\n",
      "read  /home/ntlpt59/master/own/agilab/exp/lfm2_claude/main.py, use read tool<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<|tool_call_start|>[read(path=\"/home/ntlpt59/master/own/agilab/exp/lfm2_claude/main.py\")]<|tool_call_end|><|im_end|>\n",
      "<|tool_call_start|>[read(path=\"/home/ntlpt59/master/own/agilab/exp/lfm2_claude/main.py\")]<|tool_call_end|>\n",
      "<|startoftext|><|im_start|>system\n",
      "List of tools: <|tool_list_start|>[{\"type\": \"function\", \"function\": {\"name\": \"read\", \"description\": \"Reads a file from the local filesystem. You can access any file directly by using this tool.\\nAssume this tool is able to read all files on the machine. If the User provides a path to a file assume that path is valid. It is okay to read\\na file that does not exist; an error will be returned.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"path\": {\"type\": \"string\", \"description\": \"The file path to read from.\"}}, \"required\": [\"path\"]}, \"return\": {\"type\": \"string\", \"description\": \"The file contents as a string.\"}}}]<|tool_list_end|><|im_end|>\n",
      "<|im_start|>user\n",
      "read  /home/ntlpt59/master/own/agilab/exp/lfm2_claude/main.py, use read tool<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<|tool_call_start|>[read(path=\"/home/ntlpt59/master/own/agilab/exp/lfm2_claude/main.py\")]<|tool_call_end|><|im_end|>\n",
      "<|im_start|>tool\n",
      "<|tool_response_start|>import json\n",
      "import re\n",
      "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
      "\n",
      "\n",
      "def read_file(path: str) -> str:\n",
      "    \"\"\"Reads the contents of a file and returns it as a string.\n",
      "\n",
      "    Args:\n",
      "        path: The file path to read from.\n",
      "\n",
      "    Returns:\n",
      "        The file contents as a string.\n",
      "    \"\"\"\n",
      "    try:\n",
      "        with open(path, 'r', encoding='utf-8') as file:\n",
      "            return file.read()\n",
      "    except FileNotFoundError:\n",
      "        return f\"Error: File not found: {path}\"\n",
      "    except IOError as e:\n",
      "        return f\"Error reading file {path}: {e}\"\n",
      "\n",
      "\n",
      "def write_file(path: str, content: str) -> str:\n",
      "    \"\"\"Writes content to a file.\n",
      "\n",
      "    Args:\n",
      "        path: The file path to write to.\n",
      "        content: The content to write to the file.\n",
      "\n",
      "    Returns:\n",
      "        A success or error message.\n",
      "    \"\"\"\n",
      "    try:\n",
      "        with open(path, 'w', encoding='utf-8') as file:\n",
      "            file.write(content)\n",
      "        return f\"Successfully wrote to {path}\"\n",
      "    except IOError as e:\n",
      "        return f\"Error writing to file {path}: {e}\"\n",
      "\n",
      "\n",
      "def execute_tool_call(tool_call_text):\n",
      "    \"\"\"Execute a tool call and return the result.\"\"\"\n",
      "    match = re.search(r'(\\w+)\\((.*?)\\)', tool_call_text)\n",
      "    if not match:\n",
      "        return \"Error: Invalid tool call format\"\n",
      "\n",
      "    func_name, args_str = match.groups()\n",
      "\n",
      "    try:\n",
      "        if '=' in args_str:\n",
      "            kwargs = {}\n",
      "            for arg in args_str.split(','):\n",
      "                if '=' in arg:\n",
      "                    key, value = arg.split('=', 1)\n",
      "                    key = key.strip()\n",
      "                    value = value.strip().strip('\"\\'')\n",
      "                    kwargs[key] = value\n",
      "\n",
      "            if func_name == \"read_file\":\n",
      "                return read_file(**kwargs)\n",
      "            elif func_name == \"write_file\":\n",
      "                return write_file(**kwargs)\n",
      "        else:\n",
      "            args = [arg.strip().strip('\"\\'') for arg in args_str.split(',') if arg.strip()]\n",
      "            if func_name == \"read_file\":\n",
      "                return read_file(*args)\n",
      "            elif func_name == \"write_file\":\n",
      "                return write_file(*args)\n",
      "    except Exception as e:\n",
      "        return f\"Error executing {func_name}: {e}\"\n",
      "\n",
      "    return f\"Unknown function: {func_name}\"\n",
      "\n",
      "\n",
      "def print_clean(icon, action, details=\"\"):\n",
      "    \"\"\"Print with clean formatting.\"\"\"\n",
      "    print(f\"{icon} {action}\" + (f\" â€¢ {details}\" if details else \"\"))\n",
      "\n",
      "\n",
      "# Load model and tokenizer\n",
      "print_clean(\"ðŸš€\", \"Loading LFM2-350M...\")\n",
      "model_id = \"LiquidAI/LFM2-350M\"\n",
      "model = AutoModelForCausalLM.from_pretrained(\n",
      "    model_id,\n",
      "    device_map=\"auto\",\n",
      "    torch_dtype=\"bfloat16\",\n",
      ")\n",
      "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
      "\n",
      "# Main execution loop\n",
      "prompt = \"read main.py and summarize\"\n",
      "tools = [read_file, write_file]\n",
      "\n",
      "print_clean(\"ðŸ’¬\", \"User\", prompt)\n",
      "\n",
      "for turn in range(10):  # Max 10 turns to prevent infinite loops\n",
      "    print_clean(\"ðŸ¤”\", \"Thinking...\")\n",
      "\n",
      "    input_ids = tokenizer.apply_chat_template(\n",
      "        [{\"role\": \"user\", \"content\": prompt}],\n",
      "        tools=tools,\n",
      "        add_generation_prompt=True,\n",
      "        return_tensors=\"pt\",\n",
      "        tokenize=True,\n",
      "    ).to(model.device)\n",
      "\n",
      "    output = model.generate(\n",
      "        input_ids,\n",
      "        do_sample=True,\n",
      "        temperature=0.3,\n",
      "        min_p=0.15,\n",
      "        repetition_penalty=1.05,\n",
      "        max_new_tokens=512,\n",
      "    )\n",
      "\n",
      "    response = tokenizer.decode(output[0][input_ids.shape[1]:], skip_special_tokens=False)\n",
      "    print('-----')\n",
      "    print(response)\n",
      "    print('-----')\n",
      "\n",
      "    # Check for tool calls\n",
      "    tool_call_pattern = r'<\\|tool_call_start\\|\\>\\[(.*?)\\]<\\|tool_call_end\\|\\>'\n",
      "    tool_calls = re.findall(tool_call_pattern, response)\n",
      "\n",
      "    if tool_calls:\n",
      "        for tool_call in tool_calls:\n",
      "            print_clean(\"ðŸ”§\", \"Tool call\", tool_call)\n",
      "            result = execute_tool_call(tool_call)\n",
      "            print_clean(\"ðŸ“„\", \"Result\", result[:100] + \"...\" if len(result) > 100 else result)\n",
      "\n",
      "            # Add tool result to conversation context\n",
      "            prompt += f\"\\n\\nTool result: {result}\"\n",
      "    else:\n",
      "        # No more tool calls, show final response\n",
      "        clean_response = response.split('<|tool_call_start|>')[0].strip()\n",
      "        if clean_response:\n",
      "            print_clean(\"âœ¨\", \"Response\", clean_response)\n",
      "        break\n",
      "\n",
      "print_clean(\"âœ…\", \"Done\")\n",
      "\n",
      "\n",
      "<|tool_response_end|><|im_end|>\n",
      "<|im_start|>assistant\n",
      "This script reads the contents of `/home/ntlpt59/master/own/agilab/exp/lfm2_claude/main.py` using the `read_file` tool, then attempts to execute various tools like `read_file`, `write_file`, and `execute_tool_call`. It prints each step, including potential errors encountered during execution.\n",
      "\n",
      "Here's a breakdown of what's happening:\n",
      "\n",
      "*   **File Reading:** The `read_file` function attempts to open and read the specified file. If the file doesn't exist or there's an I/O error, it returns an appropriate error message.\n",
      "*   **Tool Execution:** The `execute_tool_call` function searches for a pattern matching `<tool_call_start>` followed by `<tool_call_end>` to identify a tool call. It then processes the generated output based on the tool's signature.\n",
      "*   **Prompt Construction:** The `print_clean` function formats the output to display a clean, human-readable prompt.\n",
      "*   **Tool Call Handling:** The script iterates through a list of tool calls, applying them to the model's input and generating responses. It prints each tool call's description, the generated output, and a summary of the response.\n",
      "*   **Output Formatting:** The script handles cases where no tool calls are found, displaying a final response summarizing the results.\n",
      "\n",
      "**Example Output (hypothetical):**\n",
      "\n",
      "```\n",
      "ðŸ’¬ Reading main.py and summarizing...\n",
      "ðŸ’¬ User: Read main.py and summarize\n",
      "ðŸ’¬ Tool call: <\\|tool_call_start\\| (12345) <\\|tool_call_end\\|>\n",
      "âœ… Tool result: This model generated a summary of the input text. The generated output is truncated to 100 characters.\n",
      "```\n",
      "\n",
      "**Important Considerations:**\n",
      "\n",
      "*   **Error Handling:** The script includes basic error handling, but you might want to expand it to handle specific exceptions more gracefully (e.g., file not found errors, tokenization issues).\n",
      "*   **Tool Support:** The current implementation supports `read_file` and `write_file`. You'll need to add support for other tools by modifying the `execute_tool_call` function to recognize their patterns.\n",
      "*   **Prompt Customization:** The prompt formatting is very basic. You could customize it further to make the conversation more engaging.\n",
      "*   **Input Validation:** The script assumes the input file is in the correct format (`main.py`). You might want to add checks to ensure the file exists before attempting to read it.\n",
      "\n",
      "This script provides a solid foundation for interacting with the LFM2-350M model using the NLTK library and its Transformers components.<|im_end|>\n",
      "This script reads the contents of `/home/ntlpt59/master/own/agilab/exp/lfm2_claude/main.py` using the `read_file` tool, then attempts to execute various tools like `read_file`, `write_file`, and `execute_tool_call`. It prints each step, including potential errors encountered during execution.\n",
      "\n",
      "Here's a breakdown of what's happening:\n",
      "\n",
      "*   **File Reading:** The `read_file` function attempts to open and read the specified file. If the file doesn't exist or there's an I/O error, it returns an appropriate error message.\n",
      "*   **Tool Execution:** The `execute_tool_call` function searches for a pattern matching `<tool_call_start>` followed by `<tool_call_end>` to identify a tool call. It then processes the generated output based on the tool's signature.\n",
      "*   **Prompt Construction:** The `print_clean` function formats the output to display a clean, human-readable prompt.\n",
      "*   **Tool Call Handling:** The script iterates through a list of tool calls, applying them to the model's input and generating responses. It prints each tool call's description, the generated output, and a summary of the response.\n",
      "*   **Output Formatting:** The script handles cases where no tool calls are found, displaying a final response summarizing the results.\n",
      "\n",
      "**Example Output (hypothetical):**\n",
      "\n",
      "```\n",
      "ðŸ’¬ Reading main.py and summarizing...\n",
      "ðŸ’¬ User: Read main.py and summarize\n",
      "ðŸ’¬ Tool call: <\\|tool_call_start\\| (12345) <\\|tool_call_end\\|>\n",
      "âœ… Tool result: This model generated a summary of the input text. The generated output is truncated to 100 characters.\n",
      "```\n",
      "\n",
      "**Important Considerations:**\n",
      "\n",
      "*   **Error Handling:** The script includes basic error handling, but you might want to expand it to handle specific exceptions more gracefully (e.g., file not found errors, tokenization issues).\n",
      "*   **Tool Support:** The current implementation supports `read_file` and `write_file`. You'll need to add support for other tools by modifying the `execute_tool_call` function to recognize their patterns.\n",
      "*   **Prompt Customization:** The prompt formatting is very basic. You could customize it further to make the conversation more engaging.\n",
      "*   **Input Validation:** The script assumes the input file is in the correct format (`main.py`). You might want to add checks to ensure the file exists before attempting to read it.\n",
      "\n",
      "This script provides a solid foundation for interacting with the LFM2-350M model using the NLTK library and its Transformers components.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tr = {\"read\":read}\n",
    "# messages = [{\"role\": \"user\", \"content\": \"What is weather in london and gurgaon?\"}]\n",
    "messages = [\n",
    "    # {\"role\":\"system\",\"content\":\"Try to use the available tools and answer user questions.\"},\n",
    "    # {\"role\": \"user\", \"content\": \"what imports we have in /home/ntlpt59/master/own/agilab/exp/lfm2_claude/main.py file, read and tell me from that file\"}\n",
    "    {\"role\": \"user\", \"content\": \"read  /home/ntlpt59/master/own/agilab/exp/lfm2_claude/main.py , use read tool\"}\n",
    "    ]\n",
    "\n",
    "\n",
    "while True:\n",
    "    response = infer(messages,tools=list(tr.values()))\n",
    "    print(response)\n",
    "\n",
    "    tc = parse_tool_calls(response)\n",
    "    if not tc: break\n",
    "\n",
    "    messages.append({'role':'assistant','content':response})\n",
    "\n",
    "    for etc in tc:\n",
    "        result = tr[etc['function']](**etc['arguments'])\n",
    "        messages.append({\"role\":\"tool\",\"content\":result})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13b8c30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
